{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34faab65",
   "metadata": {},
   "source": [
    "# In this notebook, we attempt in creating an attention based NN to predict the target on the timeseries alone. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1623fe",
   "metadata": {},
   "source": [
    "We first create an example, then we will generalize it and include it into training.py. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3110fa3d",
   "metadata": {},
   "source": [
    "# Import and preparations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ec0051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, importlib\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from proj_mod import training, data_processing, visualization\n",
    "importlib.reload(training);\n",
    "importlib.reload(data_processing);\n",
    "importlib.reload(visualization);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25e0968f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.3.0\n"
     ]
    }
   ],
   "source": [
    "#Only run this cell if needed. AMD gpus might need this. \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../dotenv_env/deep_learning.env\")\n",
    "\n",
    "print(os.environ.get(\"HSA_OVERRIDE_GFX_VERSION\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9776742b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "device=(torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a37b30",
   "metadata": {},
   "source": [
    "# Data preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ffb2f",
   "metadata": {},
   "source": [
    "### Load time id order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f295403",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_time=np.load(\"../processed_data/recovered_time_id_order.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24968e95",
   "metadata": {},
   "source": [
    "### Load timeseries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ce645bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RV_ts=pd.read_parquet(\"../processed_data/book_RV_ts_60_si.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c789a9",
   "metadata": {},
   "source": [
    "### Load target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfdfdf49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>126</td>\n",
       "      <td>32751</td>\n",
       "      <td>0.003461</td>\n",
       "      <td>126-32751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>126</td>\n",
       "      <td>32753</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>126-32753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>126</td>\n",
       "      <td>32758</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>126-32758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>126</td>\n",
       "      <td>32763</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>126-32763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>126</td>\n",
       "      <td>32767</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>126-32767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        stock_id  time_id    target     row_id\n",
       "0              0        5  0.004136        0-5\n",
       "1              0       11  0.001445       0-11\n",
       "2              0       16  0.002168       0-16\n",
       "3              0       31  0.002195       0-31\n",
       "4              0       62  0.001747       0-62\n",
       "...          ...      ...       ...        ...\n",
       "428927       126    32751  0.003461  126-32751\n",
       "428928       126    32753  0.003113  126-32753\n",
       "428929       126    32758  0.004070  126-32758\n",
       "428930       126    32763  0.003357  126-32763\n",
       "428931       126    32767  0.002090  126-32767\n",
       "\n",
       "[428932 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target=pd.read_csv(\"../raw_data/kaggle_ORVP/train.csv\")\n",
    "df_target[\"row_id\"]=df_target[\"stock_id\"].astype(int).astype(str)+\"-\"+df_target[\"time_id\"].astype(int).astype(str)\n",
    "df_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c116a84",
   "metadata": {},
   "source": [
    "### Create datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a7a56d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In fold 0 :\n",
      "\n",
      "Train set end at 8117 .\n",
      "\n",
      "Test set start at 15516 end at 10890 .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ycoeusz/git/Realized_Volatility_Prediction_-erdos_deeplearning-/NNetwork/../proj_mod/training.py:351: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_tab_copy[\"sub_int_num\"]=np.nan\n",
      "/home/ycoeusz/git/Realized_Volatility_Prediction_-erdos_deeplearning-/NNetwork/../proj_mod/training.py:351: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_tab_copy[\"sub_int_num\"]=np.nan\n"
     ]
    }
   ],
   "source": [
    "time_split_list=data_processing.time_cross_val_split(list_time=list_time,n_split=1,percent_val_size=10,list_output=True)\n",
    "train_time_id,test_time_id=time_split_list[0][0],time_split_list[0][1]\n",
    "\n",
    "train_dataset=training.RVdataset(time_id_list=train_time_id,ts_features=[\"sub_int_RV\"],tab_features=[\"emb_id\"],df_ts_feat=df_RV_ts,df_target=df_target)\n",
    "test_dataset=training.RVdataset(time_id_list=test_time_id,ts_features=[\"sub_int_RV\"],tab_features=[\"emb_id\"],df_ts_feat=df_RV_ts,df_target=df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2e36e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed=nn.Embedding(num_embeddings=4,embedding_dim=2)\n",
    "out=embed(torch.tensor([[0,1,2,3],[0,1,2,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7c02435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1853, -1.0454],\n",
       "         [-0.4169, -0.8853],\n",
       "         [ 2.0273, -0.7891],\n",
       "         [ 0.1283,  0.6013]],\n",
       "\n",
       "        [[ 0.1853, -1.0454],\n",
       "         [-0.4169, -0.8853],\n",
       "         [ 2.0273, -0.7891],\n",
       "         [ 0.1283,  0.6013]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3782818a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4235233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3],\n",
       "        [0, 1, 2, 3]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0,1,2,3]).expand(2,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f2c8f0",
   "metadata": {},
   "source": [
    "# Create example (encoder based only) transformer model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e557279",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ts_encoder_example(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder_attn=nn.MultiheadAttention(embed_dim=32,num_heads=4,dropout=0.1,batch_first=True)\n",
    "        self.encoder_norm1=nn.LayerNorm(32)\n",
    "        self.encoder_feedforward=nn.Sequential(\n",
    "            nn.Linear(in_features=32,out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64,out_features=32)\n",
    "        )\n",
    "        self.encoder_norm2=nn.LayerNorm(32)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        attn,_=self.encoder_attn(x,x,x)\n",
    "        x=self.encoder_norm1(x+attn)\n",
    "        attn=self.encoder_feedforward(x)\n",
    "        return self.encoder_norm2(x+attn)\n",
    "        \n",
    "\n",
    "class ts_trans_example(nn.Module): \n",
    "    # An example where only encoder is used, the logic behind this is that we only want one value output, it might not be needed to add in decoders (which )\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #Frozen conv \n",
    "        self.frozen_conv=training.frozen_diff_conv(n_diff=2) \n",
    "        #Position embedding \n",
    "        self.pos_emb=nn.Embedding(num_embeddings=60,embedding_dim=32) # 60 is the length of our (default) timeseries. \n",
    "        self.ts_proj=nn.Linear(in_features=3,out_features=32)\n",
    "        self.pos_attn=nn.MultiheadAttention(embed_dim=32,batch_first=True,dropout=0.1,num_heads=4)\n",
    "        self.pos_norm=nn.LayerNorm(32) \n",
    "        #Encoder stacking \n",
    "        self.encoder_layers=nn.ModuleList([\n",
    "            ts_encoder_example()\n",
    "            for _ in range(4)\n",
    "        ])\n",
    "        #Final feedforward \n",
    "        self.final_linear=nn.Linear(in_features=32,out_features=1)\n",
    "        \n",
    "        #scaler\n",
    "        self.input_scaler=10000\n",
    "        \n",
    "    def forward(self,x): \n",
    "        #Create and reshape the timeseries tensor \n",
    "        x*=self.input_scaler\n",
    "        x=torch.unsqueeze(x,dim=1)\n",
    "        x=self.frozen_conv(x)\n",
    "        x=x.permute(0,2,1) \n",
    "        # print(x.shape)\n",
    "        x=self.ts_proj(x) # (N, 60, 32) 60 is the timeseries length \n",
    "        #Adding in position for positional impact \n",
    "        pos_id=torch.arange(60, device=x.device).expand(x.shape[0],60)\n",
    "        pos_emb=self.pos_emb(pos_id)\n",
    "        pos,_=self.pos_attn(x,pos_emb,pos_emb)\n",
    "        x=x+pos\n",
    "        x=self.pos_norm(x)\n",
    "        #Run though the encoder layers \n",
    "        for layer in self.encoder_layers: \n",
    "            x=layer(x)\n",
    "        return torch.sum(x,dim=1)/self.input_scaler # (N,1)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad642fb",
   "metadata": {},
   "source": [
    "### Create loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90e5e086",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=512,shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=512,shuffle=True, num_workers =4, pin_memory=True)\n",
    "\n",
    "# Loss tracking\n",
    "train_loss = []\n",
    "val_loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa7c045",
   "metadata": {},
   "source": [
    "### Init model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f07f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_example_mod=ts_trans_example().to(device=device)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(trans_example_mod.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c3d1939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "ts_trans_example                                             --\n",
       "â”œâ”€frozen_diff_conv: 1-1                                      --\n",
       "â”‚    â””â”€Conv1d: 2-1                                           (2)\n",
       "â”œâ”€Embedding: 1-2                                             1,920\n",
       "â”œâ”€Linear: 1-3                                                128\n",
       "â”œâ”€MultiheadAttention: 1-4                                    3,168\n",
       "â”‚    â””â”€NonDynamicallyQuantizableLinear: 2-2                  1,056\n",
       "â”œâ”€LayerNorm: 1-5                                             64\n",
       "â”œâ”€ModuleList: 1-6                                            --\n",
       "â”‚    â””â”€ts_encoder_example: 2-3                               --\n",
       "â”‚    â”‚    â””â”€MultiheadAttention: 3-1                          4,224\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-2                                   64\n",
       "â”‚    â”‚    â””â”€Sequential: 3-3                                  4,192\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-4                                   64\n",
       "â”‚    â””â”€ts_encoder_example: 2-4                               --\n",
       "â”‚    â”‚    â””â”€MultiheadAttention: 3-5                          4,224\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-6                                   64\n",
       "â”‚    â”‚    â””â”€Sequential: 3-7                                  4,192\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-8                                   64\n",
       "â”‚    â””â”€ts_encoder_example: 2-5                               --\n",
       "â”‚    â”‚    â””â”€MultiheadAttention: 3-9                          4,224\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-10                                  64\n",
       "â”‚    â”‚    â””â”€Sequential: 3-11                                 4,192\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-12                                  64\n",
       "â”‚    â””â”€ts_encoder_example: 2-6                               --\n",
       "â”‚    â”‚    â””â”€MultiheadAttention: 3-13                         4,224\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-14                                  64\n",
       "â”‚    â”‚    â””â”€Sequential: 3-15                                 4,192\n",
       "â”‚    â”‚    â””â”€LayerNorm: 3-16                                  64\n",
       "â”œâ”€Linear: 1-7                                                33\n",
       "=====================================================================================\n",
       "Total params: 40,547\n",
       "Trainable params: 40,545\n",
       "Non-trainable params: 2\n",
       "====================================================================================="
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(trans_example_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579fea31",
   "metadata": {},
   "source": [
    "Oh fucks that is a shit ton of tranable parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa1a9fd",
   "metadata": {},
   "source": [
    "### Training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f46acbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new best validation loss at epoch  1  with validation loss of  tensor(1.8792, device='cuda:0')\n",
      "At  18.355358362197876  epoch  1 has training loss  tensor(3.6192, device='cuda:0')  and validation loss  tensor(1.8792, device='cuda:0') .\n",
      "\n",
      "A new best validation loss at epoch  2  with validation loss of  tensor(1.5531, device='cuda:0')\n",
      "A new best validation loss at epoch  3  with validation loss of  tensor(1.4722, device='cuda:0')\n",
      "A new best validation loss at epoch  4  with validation loss of  tensor(1.4472, device='cuda:0')\n",
      "A new best validation loss at epoch  5  with validation loss of  tensor(1.4031, device='cuda:0')\n",
      "At  95.56986117362976  epoch  5 has training loss  tensor(1.5026, device='cuda:0')  and validation loss  tensor(1.4031, device='cuda:0') .\n",
      "\n",
      "A new best validation loss at epoch  7  with validation loss of  tensor(1.3791, device='cuda:0')\n",
      "A new best validation loss at epoch  8  with validation loss of  tensor(1.3670, device='cuda:0')\n",
      "A new best validation loss at epoch  10  with validation loss of  tensor(1.3588, device='cuda:0')\n",
      "At  191.7888479232788  epoch  10 has training loss  tensor(1.4540, device='cuda:0')  and validation loss  tensor(1.3588, device='cuda:0') .\n",
      "\n",
      "At  288.1280252933502  epoch  15 has training loss  tensor(1.4366, device='cuda:0')  and validation loss  tensor(1.3621, device='cuda:0') .\n",
      "\n",
      "A new best validation loss at epoch  17  with validation loss of  tensor(1.3507, device='cuda:0')\n",
      "A new best validation loss at epoch  19  with validation loss of  tensor(1.3384, device='cuda:0')\n",
      "At  384.3098750114441  epoch  20 has training loss  tensor(1.4321, device='cuda:0')  and validation loss  tensor(1.3402, device='cuda:0') .\n",
      "\n",
      "A new best validation loss at epoch  21  with validation loss of  tensor(1.3366, device='cuda:0')\n",
      "At  480.63302063941956  epoch  25 has training loss  tensor(1.4278, device='cuda:0')  and validation loss  tensor(1.3535, device='cuda:0') .\n",
      "\n",
      "A new best validation loss at epoch  27  with validation loss of  tensor(1.3363, device='cuda:0')\n",
      "A new best validation loss at epoch  29  with validation loss of  tensor(1.3350, device='cuda:0')\n",
      "At  577.5174448490143  epoch  30 has training loss  tensor(1.4246, device='cuda:0')  and validation loss  tensor(1.3380, device='cuda:0') .\n",
      "\n",
      "A new best validation loss at epoch  32  with validation loss of  tensor(1.3327, device='cuda:0')\n",
      "At  674.4512631893158  epoch  35 has training loss  tensor(1.4224, device='cuda:0')  and validation loss  tensor(1.3381, device='cuda:0') .\n",
      "\n",
      "A new best validation loss at epoch  37  with validation loss of  tensor(1.3318, device='cuda:0')\n",
      "A new best validation loss at epoch  40  with validation loss of  tensor(1.3301, device='cuda:0')\n",
      "At  770.5515491962433  epoch  40 has training loss  tensor(1.4178, device='cuda:0')  and validation loss  tensor(1.3301, device='cuda:0') .\n",
      "\n",
      "A new best validation loss at epoch  41  with validation loss of  tensor(1.3270, device='cuda:0')\n",
      "At  867.0632445812225  epoch  45 has training loss  tensor(1.4186, device='cuda:0')  and validation loss  tensor(1.3335, device='cuda:0') .\n",
      "\n",
      "A new best validation loss at epoch  47  with validation loss of  tensor(1.3253, device='cuda:0')\n",
      "At  963.3282749652863  epoch  50 has training loss  tensor(1.4159, device='cuda:0')  and validation loss  tensor(1.3317, device='cuda:0') .\n",
      "\n",
      "A new best validation loss at epoch  54  with validation loss of  tensor(1.3233, device='cuda:0')\n",
      "At  1059.9351952075958  epoch  55 has training loss  tensor(1.4136, device='cuda:0')  and validation loss  tensor(1.3314, device='cuda:0') .\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreg_training_loop_rmspe\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrans_example_mod\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mot_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mreport_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlist_train_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlist_val_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-6\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Realized_Volatility_Prediction_-erdos_deeplearning-/NNetwork/../proj_mod/training.py:154\u001b[39m, in \u001b[36mreg_training_loop_rmspe\u001b[39m\u001b[34m(optimizer, model, train_loader, val_loader, device, ot_steps, recall_best, eps, list_train_loss, list_val_loss, report_interval, n_epochs, scaler, norm_train_target, train_target)\u001b[39m\n\u001b[32m    150\u001b[39m     train_target_mean=train_loader.dataset.feat_norm_dict[train_target][\u001b[32m0\u001b[39m]\n\u001b[32m    151\u001b[39m     train_target_std=train_loader.dataset.feat_norm_dict[train_target][\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m pred=\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m#Moved here\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m norm_train_target:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/module.py:1767\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1765\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1776\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1777\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1780\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1781\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mts_trans_example.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m#Run though the encoder layers \u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encoder_layers: \n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     x=\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.sum(x,dim=\u001b[32m1\u001b[39m)/\u001b[38;5;28mself\u001b[39m.input_scaler\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/module.py:1767\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1765\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1776\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1777\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1780\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1781\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mts_encoder_example.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     attn,_=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     x=\u001b[38;5;28mself\u001b[39m.encoder_norm1(x+attn)\n\u001b[32m     16\u001b[39m     attn=\u001b[38;5;28mself\u001b[39m.encoder_feedforward(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/module.py:1767\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1765\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1776\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1777\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1780\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1781\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/modules/activation.py:1377\u001b[39m, in \u001b[36mMultiheadAttention.forward\u001b[39m\u001b[34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   1351\u001b[39m     attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[32m   1352\u001b[39m         query,\n\u001b[32m   1353\u001b[39m         key,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1374\u001b[39m         is_causal=is_causal,\n\u001b[32m   1375\u001b[39m     )\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1377\u001b[39m     attn_output, attn_output_weights = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[32m   1399\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m), attn_output_weights\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/deep_learning_3_11_8/lib/python3.11/site-packages/torch/nn/functional.py:6391\u001b[39m, in \u001b[36mmulti_head_attention_forward\u001b[39m\u001b[34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   6386\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m bias_v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   6388\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   6389\u001b[39m \u001b[38;5;66;03m# reshape q, k, v for multihead attention and make them batch first\u001b[39;00m\n\u001b[32m   6390\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6391\u001b[39m q = \u001b[43mq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m   6392\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m static_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   6393\u001b[39m     k = k.view(k.shape[\u001b[32m0\u001b[39m], bsz * num_heads, head_dim).transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "training.reg_training_loop_rmspe(optimizer=optimizer,model=trans_example_mod,train_loader=train_loader,val_loader=test_loader,ot_steps=20,report_interval=5,n_epochs=100,list_train_loss=train_loss,list_val_loss=val_loss,device=device,eps=1e-6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_3_11_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
