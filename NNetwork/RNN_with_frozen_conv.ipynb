{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77e9ebe",
   "metadata": {},
   "source": [
    "## RNN with frozen convolution layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b854e",
   "metadata": {},
   "source": [
    "The idea is inspired by an application of RNN in predicting if an online review (say, for a product) is positive or negative (1 or 0). Each word in the review sentence is first projected to a large dimension vector space and then sent into an RNN network sequentially. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f6b66b",
   "metadata": {},
   "source": [
    "We will make use of frozen convolution layer to create derivative like features for our time series features. Then, we use the derivative values of a certain time as if a \"word\" in a \"review\", the target RV as if the \"score\" of the \"review\" to train an RNN network that predicts the target RV with the time series features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd88f7",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "907e47ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, importlib\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from proj_mod import training, data_processing\n",
    "importlib.reload(training);\n",
    "importlib.reload(data_processing);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12efccd",
   "metadata": {},
   "source": [
    "Below is what Yuan needed to get his gpu working, do not run if you do not need it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd83d5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../dotenv_env/deep_learning.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebd9ea17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.3.0\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get(\"HSA_OVERRIDE_GFX_VERSION\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb26cd",
   "metadata": {},
   "source": [
    "Let's say, we have following timeseries feature (created randomly), we will create the derivative features first. As a reminder, the zero dimension is the batch size, the dimension one is channel (needed for the first convolution layer, not really a \"thing\" for time series like features). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb3d0064",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_feature=torch.randn(1,1,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b28d5ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8983,  0.5294,  1.4815,  0.3880,  0.3053, -1.5859,  0.0210,\n",
       "          -0.6448, -1.3457, -1.2188, -0.0443, -1.0198, -0.6600, -0.2874,\n",
       "          -0.0385, -1.2630, -0.1093, -2.1785, -1.1292, -0.4585,  1.7239,\n",
       "           0.9651,  0.1646, -0.7025,  0.6864,  1.0771, -0.9391,  0.5244,\n",
       "           0.2303, -0.5098,  0.1424, -1.3571, -0.7195, -0.5237, -0.0643,\n",
       "           0.4805, -0.4966, -0.7618,  0.3211,  0.0881, -1.3531, -1.1748,\n",
       "          -2.0773, -0.4347,  0.3090,  0.9424,  0.8958,  1.6446,  1.7279,\n",
       "           0.7207, -0.0487, -0.8066, -0.2446,  0.1778, -0.6278, -2.7156,\n",
       "           0.2626,  1.2195, -0.9886, -1.0032]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "739b2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_create_diff=training.frozen_diff_conv(n_diff=4)\n",
    "ts_diff_feature=conv_create_diff(ts_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48f9ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-8.9833e-01,  5.2935e-01,  1.4815e+00,  3.8805e-01,  3.0529e-01,\n",
       "          -1.5859e+00,  2.1027e-02, -6.4484e-01, -1.3457e+00, -1.2188e+00,\n",
       "          -4.4305e-02, -1.0198e+00, -6.5998e-01, -2.8743e-01, -3.8456e-02,\n",
       "          -1.2630e+00, -1.0929e-01, -2.1785e+00, -1.1292e+00, -4.5855e-01,\n",
       "           1.7239e+00,  9.6507e-01,  1.6458e-01, -7.0247e-01,  6.8645e-01,\n",
       "           1.0771e+00, -9.3911e-01,  5.2441e-01,  2.3027e-01, -5.0984e-01,\n",
       "           1.4243e-01, -1.3571e+00, -7.1949e-01, -5.2366e-01, -6.4339e-02,\n",
       "           4.8050e-01, -4.9660e-01, -7.6183e-01,  3.2113e-01,  8.8130e-02,\n",
       "          -1.3531e+00, -1.1748e+00, -2.0773e+00, -4.3466e-01,  3.0903e-01,\n",
       "           9.4244e-01,  8.9581e-01,  1.6446e+00,  1.7279e+00,  7.2071e-01,\n",
       "          -4.8721e-02, -8.0657e-01, -2.4458e-01,  1.7781e-01, -6.2775e-01,\n",
       "          -2.7156e+00,  2.6262e-01,  1.2195e+00, -9.8861e-01, -1.0032e+00],\n",
       "         [ 1.4277e+00,  9.5216e-01, -1.0935e+00, -8.2756e-02, -1.8912e+00,\n",
       "           1.6069e+00, -6.6587e-01, -7.0087e-01,  1.2694e-01,  1.1745e+00,\n",
       "          -9.7550e-01,  3.5983e-01,  3.7255e-01,  2.4898e-01, -1.2246e+00,\n",
       "           1.1537e+00, -2.0692e+00,  1.0493e+00,  6.7064e-01,  2.1825e+00,\n",
       "          -7.5884e-01, -8.0049e-01, -8.6705e-01,  1.3889e+00,  3.9068e-01,\n",
       "          -2.0162e+00,  1.4635e+00, -2.9414e-01, -7.4011e-01,  6.5227e-01,\n",
       "          -1.4996e+00,  6.3764e-01,  1.9583e-01,  4.5932e-01,  5.4484e-01,\n",
       "          -9.7711e-01, -2.6522e-01,  1.0830e+00, -2.3300e-01, -1.4412e+00,\n",
       "           1.7825e-01, -9.0244e-01,  1.6426e+00,  7.4369e-01,  6.3342e-01,\n",
       "          -4.6635e-02,  7.4877e-01,  8.3366e-02, -1.0072e+00, -7.6943e-01,\n",
       "          -7.5785e-01,  5.6199e-01,  4.2239e-01, -8.0557e-01, -2.0878e+00,\n",
       "           2.9782e+00,  9.5686e-01, -2.2081e+00, -1.4631e-02,  0.0000e+00],\n",
       "         [-4.7552e-01, -2.0456e+00,  1.0107e+00, -1.8085e+00,  3.4982e+00,\n",
       "          -2.2728e+00, -3.5007e-02,  8.2781e-01,  1.0475e+00, -2.1500e+00,\n",
       "           1.3353e+00,  1.2722e-02, -1.2357e-01, -1.4736e+00,  2.3783e+00,\n",
       "          -3.2230e+00,  3.1185e+00, -3.7867e-01,  1.5118e+00, -2.9413e+00,\n",
       "          -4.1642e-02, -6.6564e-02,  2.2560e+00, -9.9824e-01, -2.4069e+00,\n",
       "           3.4797e+00, -1.7577e+00, -4.4597e-01,  1.3924e+00, -2.1518e+00,\n",
       "           2.1372e+00, -4.4180e-01,  2.6349e-01,  8.5520e-02, -1.5220e+00,\n",
       "           7.1189e-01,  1.3482e+00, -1.3160e+00, -1.2082e+00,  1.6195e+00,\n",
       "          -1.0807e+00,  2.5451e+00, -8.9893e-01, -1.1027e-01, -6.8005e-01,\n",
       "           7.9541e-01, -6.6541e-01, -1.0906e+00,  2.3782e-01,  1.1577e-02,\n",
       "           1.3198e+00, -1.3960e-01, -1.2280e+00, -1.2823e+00,  5.0661e+00,\n",
       "          -2.0214e+00, -3.1649e+00,  2.1935e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [-1.5701e+00,  3.0563e+00, -2.8192e+00,  5.3066e+00, -5.7710e+00,\n",
       "           2.2378e+00,  8.6282e-01,  2.1971e-01, -3.1975e+00,  3.4853e+00,\n",
       "          -1.3226e+00, -1.3629e-01, -1.3500e+00,  3.8519e+00, -5.6013e+00,\n",
       "           6.3415e+00, -3.4972e+00,  1.8905e+00, -4.4531e+00,  2.8997e+00,\n",
       "          -2.4921e-02,  2.3225e+00, -3.2542e+00, -1.4087e+00,  5.8867e+00,\n",
       "          -5.2374e+00,  1.3117e+00,  1.8383e+00, -3.5442e+00,  4.2890e+00,\n",
       "          -2.5790e+00,  7.0530e-01, -1.7797e-01, -1.6075e+00,  2.2338e+00,\n",
       "           6.3629e-01, -2.6641e+00,  1.0774e-01,  2.8277e+00, -2.7002e+00,\n",
       "           3.6258e+00, -3.4440e+00,  7.8866e-01, -5.6978e-01,  1.4755e+00,\n",
       "          -1.4608e+00, -4.2520e-01,  1.3284e+00, -2.2624e-01,  1.3083e+00,\n",
       "          -1.4594e+00, -1.0884e+00, -5.4327e-02,  6.3484e+00, -7.0874e+00,\n",
       "          -1.1436e+00,  5.3584e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 4.6265e+00, -5.8755e+00,  8.1258e+00, -1.1078e+01,  8.0088e+00,\n",
       "          -1.3750e+00, -6.4310e-01, -3.4172e+00,  6.6828e+00, -4.8079e+00,\n",
       "           1.1863e+00, -1.2137e+00,  5.2019e+00, -9.4532e+00,  1.1943e+01,\n",
       "          -9.8387e+00,  5.3877e+00, -6.3436e+00,  7.3528e+00, -2.9246e+00,\n",
       "           2.3475e+00, -5.5767e+00,  1.8455e+00,  7.2953e+00, -1.1124e+01,\n",
       "           6.5491e+00,  5.2667e-01, -5.3825e+00,  7.8332e+00, -6.8680e+00,\n",
       "           3.2843e+00, -8.8327e-01, -1.4295e+00,  3.8413e+00, -1.5976e+00,\n",
       "          -3.3004e+00,  2.7719e+00,  2.7199e+00, -5.5278e+00,  6.3259e+00,\n",
       "          -7.0698e+00,  4.2327e+00, -1.3584e+00,  2.0452e+00, -2.9363e+00,\n",
       "           1.0356e+00,  1.7536e+00, -1.5547e+00,  1.5345e+00, -2.7677e+00,\n",
       "           3.7109e-01,  1.0340e+00,  6.4027e+00, -1.3436e+01,  5.9439e+00,\n",
       "           6.5020e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_diff_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79075913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 60])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_diff_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3354b4",
   "metadata": {},
   "source": [
    "We now permute the last two dimensions, essentially, this is just a transposition of the last two dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76682d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_diff_feature=ts_diff_feature.permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ed4b060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 60, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_diff_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cff150",
   "metadata": {},
   "source": [
    "Now, this is a batch (of size 1) of timeseries feature with 60 time steps, and 5 features in each step. We then expand the 5 features, say, to 32 by projection. As a reminder, nn.Linear automatically apply to the last dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b7d96c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_proj=nn.Linear(5,32)\n",
    "ts_feature_proj=linear_proj(ts_diff_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33f82b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 60, 32])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_feature_proj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e40c151",
   "metadata": {},
   "source": [
    "In this example, we will not go into too much detail, so let's make the most simple version. We then pass through a layer of RNN, then a layer of linear (to project to dimension 1 again). Learn more about RNN at https://docs.pytorch.org/docs/stable/generated/torch.nn.RNN.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f611ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_layer=nn.RNN(input_size=32,hidden_size=32,num_layers=1,nonlinearity=\"tanh\",batch_first=True,dropout=0)\n",
    "linear_end=nn.Linear(32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "207568f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_out=RNN_layer(ts_feature_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc91386",
   "metadata": {},
   "source": [
    "As a reminder, since no training is done, the RNN basically just applies the initial weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f31dfe8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 60, 32])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNN_out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e423255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_proj=linear_end(RNN_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ba4ee6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 60, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_proj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22784470",
   "metadata": {},
   "source": [
    "We will use sum, but we may change this to other functions, according to context or just feeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e293bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_out=torch.sum(out_proj,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b35c7bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.8542]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9f52c",
   "metadata": {},
   "source": [
    "## Creating the actual NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d007bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created 07/02/25\n",
    "#07/02/25: Moved to training.py \n",
    "class RV_RNN_conv(nn.Module):\n",
    "    \"\"\"\n",
    "    :param n_diff: Decides how many derivative features is wanted in the time series. \n",
    "    :param rnn_num_layer: num_layer parameter for rnn. \n",
    "    :param rnn_drop_out: dropout parameter for rnn. \n",
    "    :param rnn_act: Defaulted to \"tanh\". Nonlinearity parameter for rnn. \n",
    "    :param proj_dim: Defaulted to 32. Decided the dimension of projection before feeding into rnn. \n",
    "    :param rnn_hidden_size: Defaulted to 32. The hidden_size parameter for rnn. \n",
    "    \"\"\"\n",
    "    def __init__(self,n_diff,rnn_num_layer,rnn_drop_out,rnn_act=\"tanh\",proj_dim=32,rnn_hidden_size=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.frozen_conv=training.frozen_diff_conv(n_diff=n_diff)\n",
    "        self.linear_proj_input=nn.Linear(n_diff+1,proj_dim)\n",
    "        self.RNN_layer=nn.RNN(input_size=proj_dim,hidden_size=rnn_hidden_size,num_layers=rnn_num_layer,nonlinearity=rnn_act,batch_first=True,dropout=rnn_drop_out)\n",
    "        self.linear_post_rnn=nn.Linear(rnn_hidden_size,1)\n",
    "        self.frozen_list=[\"frozen_conv\"] \n",
    "        \n",
    "    def forward(self,x):\n",
    "        #First, unsqueese to add in one dimension in dim 1 as channel. This is needed for convolution. \n",
    "        x=torch.unsqueeze(x,dim=1)\n",
    "        x=self.frozen_conv(x)\n",
    "        x=x.permute(0,2,1)\n",
    "        x=self.linear_proj_input(x)\n",
    "        x=self.RNN_layer(x)[0]\n",
    "        x=self.linear_post_rnn(x)\n",
    "        \n",
    "        return torch.sum(x,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71432b31",
   "metadata": {},
   "source": [
    "## Basic testing on the RNN with frozen convolution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9583750",
   "metadata": {},
   "source": [
    "Test it first. As a reminder, the expected input dimensions are (Batch Size, Time Series Length), this is distinct from the example above in that the channel dimension is not present. The channel dimension is added with unsqueeze at dimension 1 so that the first convolution layer can be utilized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df3d8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_feature=torch.randn(10,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a33fd643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.0699e-01,  7.1855e-01,  1.0534e+00, -2.7195e-02, -5.3953e-01,\n",
       "         -2.1916e-01,  2.6058e-01,  9.4682e-01, -5.7270e-01,  6.1821e-01,\n",
       "          7.4706e-01,  8.5857e-01, -6.3387e-01,  1.0074e-01,  1.0871e+00,\n",
       "          4.4188e-01,  6.0410e-01, -3.4202e-02, -1.5008e-01,  8.9701e-01,\n",
       "         -6.9344e-01,  1.7957e+00, -5.8575e-01, -2.4478e+00,  7.0324e-01,\n",
       "         -1.2766e+00, -3.7510e-01, -3.5719e-01, -1.3143e+00,  6.8766e-01,\n",
       "         -2.5387e-01, -1.0662e+00,  5.9732e-01,  1.1774e+00,  1.7849e-01,\n",
       "         -7.2704e-02, -6.2228e-01, -9.0305e-01, -7.6060e-01,  2.4960e+00,\n",
       "         -3.7462e-01, -5.7164e-01,  7.3972e-02, -1.5861e+00, -7.6500e-01,\n",
       "          8.6943e-01,  4.5075e-01,  1.3651e-02,  2.1515e-01, -1.4589e+00,\n",
       "          1.1666e+00, -2.4947e-01, -6.6102e-01,  7.9651e-01,  7.1330e-01,\n",
       "         -5.8549e-01,  8.9783e-01, -5.2981e-01, -1.6684e-01, -5.2264e-01],\n",
       "        [ 6.6567e-01,  1.5717e-01, -8.9532e-01,  9.2971e-01, -6.0536e-01,\n",
       "          5.8591e-02, -5.4184e-01,  3.6571e-01, -4.7071e-01,  9.3300e-01,\n",
       "         -5.3932e-01, -8.8753e-01,  1.7115e+00,  2.2261e-02,  1.0817e+00,\n",
       "         -5.2810e-01,  4.6068e-01, -4.0759e-01, -2.1657e+00, -8.6611e-01,\n",
       "          1.1546e+00,  3.8663e-01, -3.0246e-01,  1.7155e-01,  1.5774e-01,\n",
       "          9.7471e-01, -1.0136e+00, -1.1826e+00, -1.0026e+00,  1.4194e-01,\n",
       "          3.0013e-01,  7.5464e-01,  1.3789e+00,  3.0984e-01,  9.0080e-01,\n",
       "         -2.6490e-01, -5.7665e-01, -1.6571e+00, -3.0413e-01,  1.2294e+00,\n",
       "          8.7073e-02, -1.5744e-01, -4.1935e-01, -4.0364e-01, -5.4075e-01,\n",
       "          4.4449e-01,  6.8356e-01,  2.0351e+00,  7.3060e-01,  3.7839e-01,\n",
       "          1.5160e+00,  8.3184e-01, -1.0474e+00,  1.3620e+00,  1.1588e-01,\n",
       "          1.5253e+00,  1.7736e-01,  8.9396e-01,  1.3331e+00, -1.1479e+00],\n",
       "        [-3.1383e-01,  1.7709e+00,  2.6152e-01,  4.4231e-01,  1.0152e+00,\n",
       "          2.5837e-01, -1.1688e+00, -5.6972e-01, -2.7012e-01, -3.6764e-01,\n",
       "          6.9320e-01,  6.5906e-01,  2.7373e-01, -4.1007e-01,  1.2787e+00,\n",
       "          3.7425e-01,  1.8299e-01,  1.2031e+00,  8.7180e-02, -2.2043e-01,\n",
       "          2.1431e-01,  8.1473e-01, -3.8800e-01,  8.1544e-01,  1.5113e+00,\n",
       "          8.5249e-01, -1.0512e+00, -1.5145e-01, -4.3640e-01,  6.4185e-01,\n",
       "         -3.6349e-01, -3.4011e-01, -1.6253e-02, -1.5880e+00, -1.5473e+00,\n",
       "         -1.2960e+00, -1.0107e+00,  2.2210e-02, -9.6097e-01, -1.9353e+00,\n",
       "         -1.3860e-01, -2.3329e-01, -8.0765e-01, -1.2327e-01, -1.0109e-01,\n",
       "         -2.0385e+00,  5.9140e-01, -6.8946e-01,  7.7740e-01, -1.5059e+00,\n",
       "         -1.4401e+00, -1.8734e+00, -5.5187e-01,  8.5734e-01,  8.0007e-01,\n",
       "         -7.4051e-02, -1.4025e+00,  9.7038e-01,  8.4608e-01, -7.2799e-01],\n",
       "        [-6.1188e-01, -3.7782e-01, -6.9515e-01,  1.7807e+00, -6.2075e-01,\n",
       "          3.7258e-01, -1.6661e+00,  3.5429e-01,  9.0524e-01, -1.5668e+00,\n",
       "         -1.0589e+00,  2.7080e-01, -1.2181e-01,  6.8743e-01,  6.4531e-01,\n",
       "          8.4906e-01,  3.5709e-03, -1.8106e+00, -4.3988e-03,  1.1574e+00,\n",
       "         -1.3036e+00,  9.2590e-01,  3.3343e-01,  4.5248e-02, -1.3245e+00,\n",
       "          3.3481e-01,  1.8628e+00, -4.5933e-01, -1.9357e+00, -1.3864e+00,\n",
       "         -2.6935e-01,  1.3430e+00,  4.7692e-01, -4.2193e-01, -6.1142e-01,\n",
       "         -5.6804e-01,  1.9160e+00,  7.5138e-01, -2.8756e-01, -5.6903e-01,\n",
       "          4.2587e-01,  2.6432e-01,  1.2855e+00,  4.6892e-01,  4.5061e-01,\n",
       "          7.0222e-02, -1.3397e+00,  2.4306e-02, -1.4951e+00, -1.2806e+00,\n",
       "         -2.0086e-01, -1.3639e+00,  2.8384e+00, -4.9187e-01,  1.3123e+00,\n",
       "          1.6335e+00, -2.0656e-01, -5.9957e-01, -1.3313e+00, -4.8102e-01],\n",
       "        [ 9.7902e-02, -4.8628e-02, -1.5149e+00,  1.8632e+00,  1.4713e-01,\n",
       "          6.5375e-01,  6.5687e-01, -6.3389e-01,  2.6316e+00,  4.8502e-01,\n",
       "          5.6849e-01, -1.4237e+00, -1.0267e+00,  2.3458e-01, -1.1950e+00,\n",
       "         -4.8989e-01,  7.3510e-01,  6.7091e-01,  2.2866e-01, -5.1608e-01,\n",
       "         -5.1772e-01, -4.2767e-01,  1.0707e+00,  3.4465e-01, -8.9433e-02,\n",
       "          1.0571e+00, -2.1829e-01, -1.2362e+00, -1.5092e+00, -2.7615e-01,\n",
       "         -2.8056e-01, -1.4141e+00, -1.8467e+00, -1.0360e-01,  1.3964e+00,\n",
       "          8.5886e-01, -3.9587e-01,  7.5805e-01,  3.8120e-01, -9.9700e-01,\n",
       "         -5.1492e-01, -1.6261e+00,  4.3083e-01, -7.8798e-01,  1.1481e+00,\n",
       "          1.3981e-01,  4.1850e-01,  8.0180e-01,  1.5969e+00,  2.1907e-01,\n",
       "          1.2043e-01, -9.7177e-01, -1.2572e-01,  2.0148e-01, -6.7343e-01,\n",
       "          7.4242e-01, -1.4649e+00,  1.7106e-01, -2.2052e-02,  3.7862e-01],\n",
       "        [-7.6046e-01,  7.7445e-01,  1.0674e+00,  6.0515e-01,  5.6979e-02,\n",
       "         -2.0358e-01,  1.3249e+00, -9.6556e-01, -1.2466e+00, -4.7124e-05,\n",
       "         -5.4907e-01,  1.6015e+00, -1.1331e+00, -3.4846e-01,  6.5020e-01,\n",
       "          1.8926e+00,  9.8035e-01,  5.4590e-02,  1.5252e+00,  3.9066e-01,\n",
       "          1.1330e+00, -4.0262e-01,  8.3597e-01, -1.9556e+00, -8.3535e-01,\n",
       "          1.5365e+00,  8.4672e-01,  7.6476e-01,  1.5271e+00, -3.1132e-01,\n",
       "         -5.8377e-01, -2.7592e-01,  6.5311e-01,  2.3342e-01,  3.5443e-01,\n",
       "         -1.5415e-01,  1.1575e+00,  6.4139e-01,  4.2455e-01, -2.2738e+00,\n",
       "         -1.5026e+00, -2.3264e-01,  1.0121e+00, -1.6208e+00, -1.1724e+00,\n",
       "          8.3668e-01,  8.8240e-01,  1.2998e+00, -2.3566e-01, -1.8489e+00,\n",
       "         -5.8500e-01,  5.0238e-01, -9.1927e-01, -6.3203e-01,  3.8581e-01,\n",
       "          7.2006e-01,  5.2369e-03, -1.7417e+00,  1.3047e+00,  7.0037e-01],\n",
       "        [-7.1683e-01,  1.5823e-01, -6.2349e-01,  2.1356e-01, -1.0327e+00,\n",
       "         -9.6736e-01,  8.5647e-01, -3.0849e-01,  2.1342e-01, -1.0970e+00,\n",
       "         -1.2257e-01, -4.1890e-01, -1.3747e+00, -1.0818e+00, -2.5439e-01,\n",
       "         -8.9761e-01, -5.9607e-01,  6.8220e-01, -7.3358e-01,  2.1102e-01,\n",
       "          5.0198e-01,  7.1287e-01,  4.7946e-01, -8.9818e-01, -7.8512e-01,\n",
       "          4.6087e-01,  2.3871e-01,  8.8274e-01, -3.7661e-01,  7.7403e-01,\n",
       "          7.0285e-01,  7.3269e-01, -1.1015e+00, -1.0830e-01,  2.0790e-01,\n",
       "         -1.2537e+00,  1.4871e+00, -2.9796e+00, -7.9944e-02, -6.0269e-01,\n",
       "         -3.0241e-01, -2.4495e+00,  1.4325e+00, -1.3013e+00, -1.4146e+00,\n",
       "          9.6618e-01, -6.8042e-01,  1.5329e-01,  5.5032e-02,  1.1112e+00,\n",
       "          1.5633e+00, -5.1884e-01,  1.1944e+00, -9.0681e-01, -4.9503e-01,\n",
       "          4.2033e-01, -8.4554e-01, -2.5663e-01,  1.8645e+00,  8.6548e-01],\n",
       "        [-1.0031e+00,  1.0106e+00,  9.0124e-01, -6.1037e-02,  1.2433e+00,\n",
       "         -2.8391e-01, -1.2603e+00, -1.1845e-01, -3.6038e-01,  1.2522e+00,\n",
       "          2.5964e+00,  2.7716e-01,  1.1813e+00,  2.4513e-02,  4.4231e-01,\n",
       "          9.9420e-01, -5.9467e-01,  7.0462e-01, -6.7140e-02, -7.9125e-01,\n",
       "          8.5289e-01, -4.2252e-01,  3.1974e-01,  1.0101e+00, -1.4330e-01,\n",
       "          1.2879e+00,  6.8264e-01, -3.7084e-01,  2.1376e-01, -1.4327e-01,\n",
       "         -5.7791e-01,  4.7522e-02, -1.4555e+00, -7.3328e-01,  1.5527e+00,\n",
       "         -9.9591e-02, -3.3893e-01,  7.5888e-01, -5.4199e-01, -1.0971e-01,\n",
       "         -4.0091e-01, -6.8656e-01,  8.7550e-02, -7.5264e-01, -1.1896e+00,\n",
       "          5.4613e-01,  6.4360e-03,  1.3247e+00,  1.4591e-01, -7.0127e-01,\n",
       "          3.0877e-01,  1.6118e+00,  2.0603e+00, -5.1066e-01,  2.1066e+00,\n",
       "         -5.9266e-02,  9.5798e-01, -1.4065e+00, -3.9040e-01, -3.0182e-01],\n",
       "        [ 5.7502e-02, -2.0137e+00, -1.7316e+00,  2.2969e-01,  9.6013e-01,\n",
       "         -1.2587e+00,  7.6647e-01, -1.8146e+00,  3.4629e-01,  4.7381e-02,\n",
       "          8.7413e-01,  1.3099e+00, -1.6870e+00, -1.6495e+00, -2.8130e+00,\n",
       "         -7.0553e-02, -7.5668e-02,  3.6507e-01,  6.6190e-01,  6.3048e-01,\n",
       "         -6.2638e-01,  4.0515e-01,  1.7310e+00,  1.6105e-01,  2.2605e-01,\n",
       "          1.3939e+00, -5.8125e-01,  6.7055e-01, -7.3460e-01,  2.0369e-01,\n",
       "         -2.2090e+00,  1.4305e-01, -5.9041e-01,  1.8142e+00,  7.9174e-01,\n",
       "         -9.2927e-01,  1.1031e+00, -1.1953e+00, -7.5676e-01,  6.2964e-01,\n",
       "         -4.7575e-02, -7.9687e-01,  1.4241e+00, -5.4193e-01,  1.8558e-01,\n",
       "         -1.5719e+00,  9.3078e-01, -1.3936e-01,  1.4639e+00,  9.8803e-01,\n",
       "         -7.1421e-02, -2.0910e+00, -8.5636e-01,  7.3226e-01,  7.3974e-01,\n",
       "         -1.2444e+00, -1.4409e+00,  1.5782e-01, -9.7425e-01, -6.7974e-01],\n",
       "        [ 2.2948e+00, -1.8640e+00, -5.5448e-01,  3.2076e-01, -2.3818e-01,\n",
       "          1.2985e+00, -1.6192e+00,  5.2382e-01,  1.3727e+00, -1.1024e+00,\n",
       "          5.4309e-01,  1.2560e+00, -1.7994e+00, -5.8652e-01, -1.2797e+00,\n",
       "          1.2038e+00, -2.3111e-01,  5.3263e-01, -1.4184e-01,  8.9734e-01,\n",
       "         -1.5853e+00, -7.6677e-01, -3.9792e-01,  2.5489e-01, -2.3910e+00,\n",
       "          1.4045e+00,  1.0795e+00, -6.9067e-01, -1.2928e+00, -1.3707e-01,\n",
       "         -5.3978e-01, -1.9825e+00, -1.4532e-01,  1.6245e+00, -6.2019e-01,\n",
       "         -1.8293e+00, -7.2871e-01,  6.8437e-01,  3.0371e-03, -6.0490e-01,\n",
       "         -3.5266e-01,  8.5015e-01, -4.6574e-01, -2.2452e+00, -9.5815e-01,\n",
       "          1.1328e+00, -8.0109e-01,  3.8276e-01, -1.6251e+00,  7.1061e-01,\n",
       "         -1.3349e+00,  3.2079e-01,  2.6394e+00, -9.4284e-01,  1.2536e+00,\n",
       "         -2.1126e+00, -8.2753e-01, -9.7723e-01, -7.8323e-01,  8.3279e-01]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72cd7276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.0699e-01,  7.1855e-01,  1.0534e+00, -2.7195e-02, -5.3953e-01,\n",
       "          -2.1916e-01,  2.6058e-01,  9.4682e-01, -5.7270e-01,  6.1821e-01,\n",
       "           7.4706e-01,  8.5857e-01, -6.3387e-01,  1.0074e-01,  1.0871e+00,\n",
       "           4.4188e-01,  6.0410e-01, -3.4202e-02, -1.5008e-01,  8.9701e-01,\n",
       "          -6.9344e-01,  1.7957e+00, -5.8575e-01, -2.4478e+00,  7.0324e-01,\n",
       "          -1.2766e+00, -3.7510e-01, -3.5719e-01, -1.3143e+00,  6.8766e-01,\n",
       "          -2.5387e-01, -1.0662e+00,  5.9732e-01,  1.1774e+00,  1.7849e-01,\n",
       "          -7.2704e-02, -6.2228e-01, -9.0305e-01, -7.6060e-01,  2.4960e+00,\n",
       "          -3.7462e-01, -5.7164e-01,  7.3972e-02, -1.5861e+00, -7.6500e-01,\n",
       "           8.6943e-01,  4.5075e-01,  1.3651e-02,  2.1515e-01, -1.4589e+00,\n",
       "           1.1666e+00, -2.4947e-01, -6.6102e-01,  7.9651e-01,  7.1330e-01,\n",
       "          -5.8549e-01,  8.9783e-01, -5.2981e-01, -1.6684e-01, -5.2264e-01]],\n",
       "\n",
       "        [[ 6.6567e-01,  1.5717e-01, -8.9532e-01,  9.2971e-01, -6.0536e-01,\n",
       "           5.8591e-02, -5.4184e-01,  3.6571e-01, -4.7071e-01,  9.3300e-01,\n",
       "          -5.3932e-01, -8.8753e-01,  1.7115e+00,  2.2261e-02,  1.0817e+00,\n",
       "          -5.2810e-01,  4.6068e-01, -4.0759e-01, -2.1657e+00, -8.6611e-01,\n",
       "           1.1546e+00,  3.8663e-01, -3.0246e-01,  1.7155e-01,  1.5774e-01,\n",
       "           9.7471e-01, -1.0136e+00, -1.1826e+00, -1.0026e+00,  1.4194e-01,\n",
       "           3.0013e-01,  7.5464e-01,  1.3789e+00,  3.0984e-01,  9.0080e-01,\n",
       "          -2.6490e-01, -5.7665e-01, -1.6571e+00, -3.0413e-01,  1.2294e+00,\n",
       "           8.7073e-02, -1.5744e-01, -4.1935e-01, -4.0364e-01, -5.4075e-01,\n",
       "           4.4449e-01,  6.8356e-01,  2.0351e+00,  7.3060e-01,  3.7839e-01,\n",
       "           1.5160e+00,  8.3184e-01, -1.0474e+00,  1.3620e+00,  1.1588e-01,\n",
       "           1.5253e+00,  1.7736e-01,  8.9396e-01,  1.3331e+00, -1.1479e+00]],\n",
       "\n",
       "        [[-3.1383e-01,  1.7709e+00,  2.6152e-01,  4.4231e-01,  1.0152e+00,\n",
       "           2.5837e-01, -1.1688e+00, -5.6972e-01, -2.7012e-01, -3.6764e-01,\n",
       "           6.9320e-01,  6.5906e-01,  2.7373e-01, -4.1007e-01,  1.2787e+00,\n",
       "           3.7425e-01,  1.8299e-01,  1.2031e+00,  8.7180e-02, -2.2043e-01,\n",
       "           2.1431e-01,  8.1473e-01, -3.8800e-01,  8.1544e-01,  1.5113e+00,\n",
       "           8.5249e-01, -1.0512e+00, -1.5145e-01, -4.3640e-01,  6.4185e-01,\n",
       "          -3.6349e-01, -3.4011e-01, -1.6253e-02, -1.5880e+00, -1.5473e+00,\n",
       "          -1.2960e+00, -1.0107e+00,  2.2210e-02, -9.6097e-01, -1.9353e+00,\n",
       "          -1.3860e-01, -2.3329e-01, -8.0765e-01, -1.2327e-01, -1.0109e-01,\n",
       "          -2.0385e+00,  5.9140e-01, -6.8946e-01,  7.7740e-01, -1.5059e+00,\n",
       "          -1.4401e+00, -1.8734e+00, -5.5187e-01,  8.5734e-01,  8.0007e-01,\n",
       "          -7.4051e-02, -1.4025e+00,  9.7038e-01,  8.4608e-01, -7.2799e-01]],\n",
       "\n",
       "        [[-6.1188e-01, -3.7782e-01, -6.9515e-01,  1.7807e+00, -6.2075e-01,\n",
       "           3.7258e-01, -1.6661e+00,  3.5429e-01,  9.0524e-01, -1.5668e+00,\n",
       "          -1.0589e+00,  2.7080e-01, -1.2181e-01,  6.8743e-01,  6.4531e-01,\n",
       "           8.4906e-01,  3.5709e-03, -1.8106e+00, -4.3988e-03,  1.1574e+00,\n",
       "          -1.3036e+00,  9.2590e-01,  3.3343e-01,  4.5248e-02, -1.3245e+00,\n",
       "           3.3481e-01,  1.8628e+00, -4.5933e-01, -1.9357e+00, -1.3864e+00,\n",
       "          -2.6935e-01,  1.3430e+00,  4.7692e-01, -4.2193e-01, -6.1142e-01,\n",
       "          -5.6804e-01,  1.9160e+00,  7.5138e-01, -2.8756e-01, -5.6903e-01,\n",
       "           4.2587e-01,  2.6432e-01,  1.2855e+00,  4.6892e-01,  4.5061e-01,\n",
       "           7.0222e-02, -1.3397e+00,  2.4306e-02, -1.4951e+00, -1.2806e+00,\n",
       "          -2.0086e-01, -1.3639e+00,  2.8384e+00, -4.9187e-01,  1.3123e+00,\n",
       "           1.6335e+00, -2.0656e-01, -5.9957e-01, -1.3313e+00, -4.8102e-01]],\n",
       "\n",
       "        [[ 9.7902e-02, -4.8628e-02, -1.5149e+00,  1.8632e+00,  1.4713e-01,\n",
       "           6.5375e-01,  6.5687e-01, -6.3389e-01,  2.6316e+00,  4.8502e-01,\n",
       "           5.6849e-01, -1.4237e+00, -1.0267e+00,  2.3458e-01, -1.1950e+00,\n",
       "          -4.8989e-01,  7.3510e-01,  6.7091e-01,  2.2866e-01, -5.1608e-01,\n",
       "          -5.1772e-01, -4.2767e-01,  1.0707e+00,  3.4465e-01, -8.9433e-02,\n",
       "           1.0571e+00, -2.1829e-01, -1.2362e+00, -1.5092e+00, -2.7615e-01,\n",
       "          -2.8056e-01, -1.4141e+00, -1.8467e+00, -1.0360e-01,  1.3964e+00,\n",
       "           8.5886e-01, -3.9587e-01,  7.5805e-01,  3.8120e-01, -9.9700e-01,\n",
       "          -5.1492e-01, -1.6261e+00,  4.3083e-01, -7.8798e-01,  1.1481e+00,\n",
       "           1.3981e-01,  4.1850e-01,  8.0180e-01,  1.5969e+00,  2.1907e-01,\n",
       "           1.2043e-01, -9.7177e-01, -1.2572e-01,  2.0148e-01, -6.7343e-01,\n",
       "           7.4242e-01, -1.4649e+00,  1.7106e-01, -2.2052e-02,  3.7862e-01]],\n",
       "\n",
       "        [[-7.6046e-01,  7.7445e-01,  1.0674e+00,  6.0515e-01,  5.6979e-02,\n",
       "          -2.0358e-01,  1.3249e+00, -9.6556e-01, -1.2466e+00, -4.7124e-05,\n",
       "          -5.4907e-01,  1.6015e+00, -1.1331e+00, -3.4846e-01,  6.5020e-01,\n",
       "           1.8926e+00,  9.8035e-01,  5.4590e-02,  1.5252e+00,  3.9066e-01,\n",
       "           1.1330e+00, -4.0262e-01,  8.3597e-01, -1.9556e+00, -8.3535e-01,\n",
       "           1.5365e+00,  8.4672e-01,  7.6476e-01,  1.5271e+00, -3.1132e-01,\n",
       "          -5.8377e-01, -2.7592e-01,  6.5311e-01,  2.3342e-01,  3.5443e-01,\n",
       "          -1.5415e-01,  1.1575e+00,  6.4139e-01,  4.2455e-01, -2.2738e+00,\n",
       "          -1.5026e+00, -2.3264e-01,  1.0121e+00, -1.6208e+00, -1.1724e+00,\n",
       "           8.3668e-01,  8.8240e-01,  1.2998e+00, -2.3566e-01, -1.8489e+00,\n",
       "          -5.8500e-01,  5.0238e-01, -9.1927e-01, -6.3203e-01,  3.8581e-01,\n",
       "           7.2006e-01,  5.2369e-03, -1.7417e+00,  1.3047e+00,  7.0037e-01]],\n",
       "\n",
       "        [[-7.1683e-01,  1.5823e-01, -6.2349e-01,  2.1356e-01, -1.0327e+00,\n",
       "          -9.6736e-01,  8.5647e-01, -3.0849e-01,  2.1342e-01, -1.0970e+00,\n",
       "          -1.2257e-01, -4.1890e-01, -1.3747e+00, -1.0818e+00, -2.5439e-01,\n",
       "          -8.9761e-01, -5.9607e-01,  6.8220e-01, -7.3358e-01,  2.1102e-01,\n",
       "           5.0198e-01,  7.1287e-01,  4.7946e-01, -8.9818e-01, -7.8512e-01,\n",
       "           4.6087e-01,  2.3871e-01,  8.8274e-01, -3.7661e-01,  7.7403e-01,\n",
       "           7.0285e-01,  7.3269e-01, -1.1015e+00, -1.0830e-01,  2.0790e-01,\n",
       "          -1.2537e+00,  1.4871e+00, -2.9796e+00, -7.9944e-02, -6.0269e-01,\n",
       "          -3.0241e-01, -2.4495e+00,  1.4325e+00, -1.3013e+00, -1.4146e+00,\n",
       "           9.6618e-01, -6.8042e-01,  1.5329e-01,  5.5032e-02,  1.1112e+00,\n",
       "           1.5633e+00, -5.1884e-01,  1.1944e+00, -9.0681e-01, -4.9503e-01,\n",
       "           4.2033e-01, -8.4554e-01, -2.5663e-01,  1.8645e+00,  8.6548e-01]],\n",
       "\n",
       "        [[-1.0031e+00,  1.0106e+00,  9.0124e-01, -6.1037e-02,  1.2433e+00,\n",
       "          -2.8391e-01, -1.2603e+00, -1.1845e-01, -3.6038e-01,  1.2522e+00,\n",
       "           2.5964e+00,  2.7716e-01,  1.1813e+00,  2.4513e-02,  4.4231e-01,\n",
       "           9.9420e-01, -5.9467e-01,  7.0462e-01, -6.7140e-02, -7.9125e-01,\n",
       "           8.5289e-01, -4.2252e-01,  3.1974e-01,  1.0101e+00, -1.4330e-01,\n",
       "           1.2879e+00,  6.8264e-01, -3.7084e-01,  2.1376e-01, -1.4327e-01,\n",
       "          -5.7791e-01,  4.7522e-02, -1.4555e+00, -7.3328e-01,  1.5527e+00,\n",
       "          -9.9591e-02, -3.3893e-01,  7.5888e-01, -5.4199e-01, -1.0971e-01,\n",
       "          -4.0091e-01, -6.8656e-01,  8.7550e-02, -7.5264e-01, -1.1896e+00,\n",
       "           5.4613e-01,  6.4360e-03,  1.3247e+00,  1.4591e-01, -7.0127e-01,\n",
       "           3.0877e-01,  1.6118e+00,  2.0603e+00, -5.1066e-01,  2.1066e+00,\n",
       "          -5.9266e-02,  9.5798e-01, -1.4065e+00, -3.9040e-01, -3.0182e-01]],\n",
       "\n",
       "        [[ 5.7502e-02, -2.0137e+00, -1.7316e+00,  2.2969e-01,  9.6013e-01,\n",
       "          -1.2587e+00,  7.6647e-01, -1.8146e+00,  3.4629e-01,  4.7381e-02,\n",
       "           8.7413e-01,  1.3099e+00, -1.6870e+00, -1.6495e+00, -2.8130e+00,\n",
       "          -7.0553e-02, -7.5668e-02,  3.6507e-01,  6.6190e-01,  6.3048e-01,\n",
       "          -6.2638e-01,  4.0515e-01,  1.7310e+00,  1.6105e-01,  2.2605e-01,\n",
       "           1.3939e+00, -5.8125e-01,  6.7055e-01, -7.3460e-01,  2.0369e-01,\n",
       "          -2.2090e+00,  1.4305e-01, -5.9041e-01,  1.8142e+00,  7.9174e-01,\n",
       "          -9.2927e-01,  1.1031e+00, -1.1953e+00, -7.5676e-01,  6.2964e-01,\n",
       "          -4.7575e-02, -7.9687e-01,  1.4241e+00, -5.4193e-01,  1.8558e-01,\n",
       "          -1.5719e+00,  9.3078e-01, -1.3936e-01,  1.4639e+00,  9.8803e-01,\n",
       "          -7.1421e-02, -2.0910e+00, -8.5636e-01,  7.3226e-01,  7.3974e-01,\n",
       "          -1.2444e+00, -1.4409e+00,  1.5782e-01, -9.7425e-01, -6.7974e-01]],\n",
       "\n",
       "        [[ 2.2948e+00, -1.8640e+00, -5.5448e-01,  3.2076e-01, -2.3818e-01,\n",
       "           1.2985e+00, -1.6192e+00,  5.2382e-01,  1.3727e+00, -1.1024e+00,\n",
       "           5.4309e-01,  1.2560e+00, -1.7994e+00, -5.8652e-01, -1.2797e+00,\n",
       "           1.2038e+00, -2.3111e-01,  5.3263e-01, -1.4184e-01,  8.9734e-01,\n",
       "          -1.5853e+00, -7.6677e-01, -3.9792e-01,  2.5489e-01, -2.3910e+00,\n",
       "           1.4045e+00,  1.0795e+00, -6.9067e-01, -1.2928e+00, -1.3707e-01,\n",
       "          -5.3978e-01, -1.9825e+00, -1.4532e-01,  1.6245e+00, -6.2019e-01,\n",
       "          -1.8293e+00, -7.2871e-01,  6.8437e-01,  3.0371e-03, -6.0490e-01,\n",
       "          -3.5266e-01,  8.5015e-01, -4.6574e-01, -2.2452e+00, -9.5815e-01,\n",
       "           1.1328e+00, -8.0109e-01,  3.8276e-01, -1.6251e+00,  7.1061e-01,\n",
       "          -1.3349e+00,  3.2079e-01,  2.6394e+00, -9.4284e-01,  1.2536e+00,\n",
       "          -2.1126e+00, -8.2753e-01, -9.7723e-01, -7.8323e-01,  8.3279e-01]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(ts_feature,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "455ed170",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=training.RV_RNN_conv(n_diff=4,rnn_act=\"tanh\",rnn_num_layer=2,rnn_drop_out=0.3,rnn_hidden_size=32,proj_dim=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85bf9cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model(ts_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc35034a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12.4432],\n",
       "        [14.1024],\n",
       "        [14.2141],\n",
       "        [10.5330],\n",
       "        [11.9638],\n",
       "        [10.5465],\n",
       "        [13.0989],\n",
       "        [10.8659],\n",
       "        [11.3673],\n",
       "        [12.0642]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bfbac7",
   "metadata": {},
   "source": [
    "## Training loop (basic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d4fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_time=np.load(\"../processed_data/recovered_time_id_order.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "825838c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In fold 0 :\n",
      "\n",
      "Train set end at 8117 .\n",
      "\n",
      "Test set start at 15516 end at 10890 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_split_list=data_processing.time_cross_val_split(list_time=list_time,n_split=1,percent_val_size=10,list_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72aedca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time_id,test_time_id=time_split_list[0][0],time_split_list[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3338fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RV_ts=pd.read_parquet(\"../processed_data/book_RV_ts_60_si.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62e2f34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b9cf070",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e42729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RV_ts[\"sub_int_RV_norm\"]=scalar.fit_transform(df_RV_ts[[\"sub_int_RV\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "995d9083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>sub_int_RV</th>\n",
       "      <th>sub_int_num</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>row_id</th>\n",
       "      <th>sub_int_RV_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0-5</td>\n",
       "      <td>-0.767245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0-11</td>\n",
       "      <td>-0.787110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0-16</td>\n",
       "      <td>-0.023625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0-31</td>\n",
       "      <td>-0.793956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0-62</td>\n",
       "      <td>-0.375815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25735915</th>\n",
       "      <td>6410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60</td>\n",
       "      <td>99</td>\n",
       "      <td>99-6410</td>\n",
       "      <td>-0.793956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25735916</th>\n",
       "      <td>10421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60</td>\n",
       "      <td>99</td>\n",
       "      <td>99-10421</td>\n",
       "      <td>-0.793956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25735917</th>\n",
       "      <td>25639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60</td>\n",
       "      <td>99</td>\n",
       "      <td>99-25639</td>\n",
       "      <td>-0.793956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25735918</th>\n",
       "      <td>25680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60</td>\n",
       "      <td>99</td>\n",
       "      <td>99-25680</td>\n",
       "      <td>-0.793956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25735919</th>\n",
       "      <td>27964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60</td>\n",
       "      <td>99</td>\n",
       "      <td>99-27964</td>\n",
       "      <td>-0.793956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25735920 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          time_id  sub_int_RV  sub_int_num  stock_id    row_id  \\\n",
       "0               5    0.000015            1         0       0-5   \n",
       "1              11    0.000004            1         0      0-11   \n",
       "2              16    0.000432            1         0      0-16   \n",
       "3              31    0.000000            1         0      0-31   \n",
       "4              62    0.000235            1         0      0-62   \n",
       "...           ...         ...          ...       ...       ...   \n",
       "25735915     6410    0.000000           60        99   99-6410   \n",
       "25735916    10421    0.000000           60        99  99-10421   \n",
       "25735917    25639    0.000000           60        99  99-25639   \n",
       "25735918    25680    0.000000           60        99  99-25680   \n",
       "25735919    27964    0.000000           60        99  99-27964   \n",
       "\n",
       "          sub_int_RV_norm  \n",
       "0               -0.767245  \n",
       "1               -0.787110  \n",
       "2               -0.023625  \n",
       "3               -0.793956  \n",
       "4               -0.375815  \n",
       "...                   ...  \n",
       "25735915        -0.793956  \n",
       "25735916        -0.793956  \n",
       "25735917        -0.793956  \n",
       "25735918        -0.793956  \n",
       "25735919        -0.793956  \n",
       "\n",
       "[25735920 rows x 6 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_RV_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de450e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"21\" halign=\"left\">sub_int_RV</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub_int_num</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0-1000</th>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>3.818799e-07</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>2.313288e-04</td>\n",
       "      <td>1.060893e-05</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-10000</th>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>3.154886e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>3.777703e-08</td>\n",
       "      <td>3.777703e-08</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-10005</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>4.375100e-04</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>2.552987e-03</td>\n",
       "      <td>5.364106e-04</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-10017</th>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>6.771948e-05</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>3.104404e-03</td>\n",
       "      <td>1.224910e-03</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.003287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-10030</th>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>2.586782e-04</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>4.842470e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99-9972</th>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>2.503467e-04</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>9.916624e-05</td>\n",
       "      <td>1.809852e-04</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99-9973</th>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>9.650211e-04</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>1.862600e-03</td>\n",
       "      <td>7.668418e-04</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.002115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99-9976</th>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>7.203531e-04</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>5.946683e-04</td>\n",
       "      <td>1.509652e-04</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99-9988</th>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>1.957402e-04</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1.440137e-04</td>\n",
       "      <td>3.200939e-05</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99-9993</th>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>1.798617e-04</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>2.728767e-04</td>\n",
       "      <td>2.108380e-04</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sub_int_RV                                                        \\\n",
       "sub_int_num         1         2         3         4         5             6    \n",
       "row_id                                                                         \n",
       "0-1000        0.000341  0.000000  0.000023  0.000000  0.000170  3.818799e-07   \n",
       "0-10000       0.000290  0.000191  0.000087  0.000193  0.000241  3.154886e-04   \n",
       "0-10005       0.000000  0.000000  0.001554  0.002177  0.002303  4.375100e-04   \n",
       "0-10017       0.000142  0.000142  0.001464  0.001086  0.000068  6.771948e-05   \n",
       "0-10030       0.000327  0.000058  0.000293  0.000842  0.000120  2.586782e-04   \n",
       "...                ...       ...       ...       ...       ...           ...   \n",
       "99-9972       0.000197  0.000181  0.000171  0.000172  0.000369  2.503467e-04   \n",
       "99-9973       0.000821  0.000346  0.000691  0.001591  0.000863  9.650211e-04   \n",
       "99-9976       0.000569  0.001101  0.001002  0.000430  0.000797  7.203531e-04   \n",
       "99-9988       0.000040  0.000069  0.000123  0.000056  0.000016  1.957402e-04   \n",
       "99-9993       0.000249  0.000179  0.000155  0.000025  0.000325  1.798617e-04   \n",
       "\n",
       "                                                     ...                      \\\n",
       "sub_int_num        7         8         9         10  ...        51        52   \n",
       "row_id                                               ...                       \n",
       "0-1000       0.000089  0.000552  0.000012  0.000000  ...  0.000265  0.000000   \n",
       "0-10000      0.000000  0.000247  0.000265  0.000000  ...  0.000202  0.000375   \n",
       "0-10005      0.000617  0.001199  0.002306  0.001215  ...  0.000000  0.000486   \n",
       "0-10017      0.000899  0.000064  0.000593  0.000451  ...  0.000029  0.000000   \n",
       "0-10030      0.000221  0.000436  0.000099  0.000008  ...  0.000410  0.000437   \n",
       "...               ...       ...       ...       ...  ...       ...       ...   \n",
       "99-9972      0.000349  0.000356  0.000390  0.000050  ...  0.000075  0.000185   \n",
       "99-9973      0.000504  0.001925  0.000641  0.000382  ...  0.001081  0.001095   \n",
       "99-9976      0.000586  0.000538  0.000570  0.000781  ...  0.000508  0.000406   \n",
       "99-9988      0.000071  0.000095  0.000063  0.000030  ...  0.000034  0.000176   \n",
       "99-9993      0.000080  0.000125  0.000126  0.000205  ...  0.000099  0.000370   \n",
       "\n",
       "                                                                   \\\n",
       "sub_int_num        53        54        55        56            57   \n",
       "row_id                                                              \n",
       "0-1000       0.000214  0.000003  0.000000  0.000118  2.313288e-04   \n",
       "0-10000      0.000616  0.000564  0.000000  0.000023  3.777703e-08   \n",
       "0-10005      0.000050  0.001761  0.001617  0.001801  2.552987e-03   \n",
       "0-10017      0.001293  0.002092  0.000994  0.000848  3.104404e-03   \n",
       "0-10030      0.000004  0.000215  0.000457  0.000183  4.842470e-04   \n",
       "...               ...       ...       ...       ...           ...   \n",
       "99-9972      0.000314  0.000318  0.000115  0.000143  9.916624e-05   \n",
       "99-9973      0.000425  0.000789  0.001295  0.000596  1.862600e-03   \n",
       "99-9976      0.000662  0.000338  0.000710  0.000179  5.946683e-04   \n",
       "99-9988      0.000140  0.000129  0.000175  0.000019  1.440137e-04   \n",
       "99-9993      0.000929  0.000328  0.000251  0.000204  2.728767e-04   \n",
       "\n",
       "                                               \n",
       "sub_int_num            58        59        60  \n",
       "row_id                                         \n",
       "0-1000       1.060893e-05  0.000111  0.000288  \n",
       "0-10000      3.777703e-08  0.000020  0.000310  \n",
       "0-10005      5.364106e-04  0.000872  0.000000  \n",
       "0-10017      1.224910e-03  0.001316  0.003287  \n",
       "0-10030      0.000000e+00  0.000756  0.000005  \n",
       "...                   ...       ...       ...  \n",
       "99-9972      1.809852e-04  0.000334  0.000089  \n",
       "99-9973      7.668418e-04  0.001035  0.002115  \n",
       "99-9976      1.509652e-04  0.000388  0.000403  \n",
       "99-9988      3.200939e-05  0.000041  0.000007  \n",
       "99-9993      2.108380e-04  0.000126  0.000353  \n",
       "\n",
       "[428932 rows x 60 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_RV_ts.pivot(index=\"row_id\",columns=[\"sub_int_num\"],values=[\"sub_int_RV\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "677c1b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target=pd.read_csv(\"../raw_data/kaggle_ORVP/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "252d6274",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target[\"row_id\"]=df_target[\"stock_id\"].astype(int).astype(str)+\"-\"+df_target[\"time_id\"].astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdb28caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>126</td>\n",
       "      <td>32751</td>\n",
       "      <td>0.003461</td>\n",
       "      <td>126-32751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>126</td>\n",
       "      <td>32753</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>126-32753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>126</td>\n",
       "      <td>32758</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>126-32758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>126</td>\n",
       "      <td>32763</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>126-32763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>126</td>\n",
       "      <td>32767</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>126-32767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        stock_id  time_id    target     row_id\n",
       "0              0        5  0.004136        0-5\n",
       "1              0       11  0.001445       0-11\n",
       "2              0       16  0.002168       0-16\n",
       "3              0       31  0.002195       0-31\n",
       "4              0       62  0.001747       0-62\n",
       "...          ...      ...       ...        ...\n",
       "428927       126    32751  0.003461  126-32751\n",
       "428928       126    32753  0.003113  126-32753\n",
       "428929       126    32758  0.004070  126-32758\n",
       "428930       126    32763  0.003357  126-32763\n",
       "428931       126    32767  0.002090  126-32767\n",
       "\n",
       "[428932 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b2a07cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ycoeusz/git/Realized_Volatility_Prediction_-erdos_deeplearning-/NNetwork/../proj_mod/training.py:284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_tab_copy.loc[:,\"sub_int_num\"]=np.nan\n",
      "/home/ycoeusz/git/Realized_Volatility_Prediction_-erdos_deeplearning-/NNetwork/../proj_mod/training.py:284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_tab_copy.loc[:,\"sub_int_num\"]=np.nan\n"
     ]
    }
   ],
   "source": [
    "train_dataset=training.RVdataset(time_id_list=train_time_id,ts_features=[\"sub_int_RV\"],df_ts_feat=df_RV_ts,df_target=df_target)\n",
    "test_dataset=training.RVdataset(time_id_list=test_time_id,ts_features=[\"sub_int_RV\"],df_ts_feat=df_RV_ts,df_target=df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7c432cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([386036, 60])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07885f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([386036, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0056627",
   "metadata": {},
   "source": [
    "Not entirely sure what the error code is about, I am literally doing exactly as suggested by the warning. https://stackoverflow.com/questions/23688307/settingwithcopywarning-even-when-using-loc says this is a false positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c408726",
   "metadata": {},
   "source": [
    "## Training with native values, with a 10000 times scaler on input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cc50c5",
   "metadata": {},
   "source": [
    "A key issue with our input timeseries is that all values are extremely close to zero, so a 10000 times scaler can help with expanding them a little. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2efd16ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "device=(torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b95e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model=training.RV_RNN_conv(n_diff=4,rnn_act=\"tanh\",rnn_drop_out=0,rnn_hidden_size=32,proj_dim=32,rnn_num_layer=1,input_scaler=10000).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3846a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer=optim.Adam(RNN_model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55dd6fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=256,shuffle=True)\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=256,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "292fcf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=[]\n",
    "val_loss=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "810267b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At  9.23012661933899  epoch  1 has training loss  tensor(0.2709, device='cuda:0')  and validation loss  tensor(0.2373, device='cuda:0') .\n",
      "\n",
      "At  48.45872116088867  epoch  5 has training loss  tensor(0.2545, device='cuda:0')  and validation loss  tensor(0.2368, device='cuda:0') .\n",
      "\n",
      "At  96.10576176643372  epoch  10 has training loss  tensor(0.2532, device='cuda:0')  and validation loss  tensor(0.2348, device='cuda:0') .\n",
      "\n",
      "At  144.24580764770508  epoch  15 has training loss  tensor(0.2527, device='cuda:0')  and validation loss  tensor(0.2345, device='cuda:0') .\n",
      "\n",
      "At  191.81521773338318  epoch  20 has training loss  tensor(0.2519, device='cuda:0')  and validation loss  tensor(0.2376, device='cuda:0') .\n",
      "\n",
      "At  238.4539635181427  epoch  25 has training loss  tensor(0.2517, device='cuda:0')  and validation loss  tensor(0.2395, device='cuda:0') .\n",
      "\n",
      "At  285.9001407623291  epoch  30 has training loss  tensor(0.2513, device='cuda:0')  and validation loss  tensor(0.2341, device='cuda:0') .\n",
      "\n",
      "At  333.9812955856323  epoch  35 has training loss  tensor(0.2519, device='cuda:0')  and validation loss  tensor(0.2350, device='cuda:0') .\n",
      "\n",
      "At  382.1108911037445  epoch  40 has training loss  tensor(0.2510, device='cuda:0')  and validation loss  tensor(0.2342, device='cuda:0') .\n",
      "\n",
      "At  430.8231027126312  epoch  45 has training loss  tensor(0.2508, device='cuda:0')  and validation loss  tensor(0.2367, device='cuda:0') .\n",
      "\n",
      "The validation loss has not improved for  10  epochs. Stopping current training loop.\n",
      "\n",
      "Best model state dictionary of this training loop is reloaded.\n",
      "\n",
      "According to validation loss, the best model is reached at epoch 39  with validation loss:  tensor(0.2339, device='cuda:0') .\n",
      " The total number of epoch trained is  49 .\n",
      " Training completed in:  469.75123953819275 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('frozen_conv.frozen_conv.weight',\n",
       "              tensor([[[-1.,  1.]]], device='cuda:0')),\n",
       "             ('linear_proj_input.weight',\n",
       "              tensor([[-1.0166e-02,  5.0208e-01,  2.3707e-01, -2.1755e-01, -8.7761e-02],\n",
       "                      [ 2.3073e-02, -7.2817e-02, -4.2685e-01, -7.6756e-02,  1.9212e-01],\n",
       "                      [-1.1001e-02, -6.2954e-02, -9.9150e-02, -8.2799e-02, -1.0407e-02],\n",
       "                      [-2.3012e-01, -2.2045e-01, -3.8083e-01, -3.3201e-02,  4.8838e-02],\n",
       "                      [ 4.0545e-01,  7.8722e-02, -2.1627e-01, -5.3211e-02, -1.9257e-01],\n",
       "                      [ 1.8553e-02,  4.5463e-01,  4.1530e-01,  1.6941e-01, -1.2382e-03],\n",
       "                      [ 1.9253e-02, -2.5209e-01, -1.8975e-01,  4.2143e-02, -5.0802e-02],\n",
       "                      [ 1.6210e-02, -4.9789e-01, -3.6411e-01,  3.1163e-02,  1.0125e-01],\n",
       "                      [-8.5251e-04,  2.5829e-01, -5.8556e-04, -2.2333e-01, -1.0621e-01],\n",
       "                      [-5.3765e-01, -7.1270e-01, -2.6129e-01,  2.8460e-01,  2.2052e-01],\n",
       "                      [ 2.6753e-03, -2.2497e-02, -1.0819e-02, -3.3320e-02, -1.4437e-02],\n",
       "                      [ 1.1822e-01,  3.6418e-01,  4.3424e-01, -1.9589e-01, -2.2076e-01],\n",
       "                      [-1.7937e-02, -4.1045e-01, -4.5582e-01, -2.3963e-01, -1.5297e-02],\n",
       "                      [ 5.2643e-01,  5.3989e-01, -1.7271e-03, -7.0753e-02, -5.7291e-04],\n",
       "                      [ 2.9092e-03,  4.5288e-02, -2.3818e-02, -9.7667e-02, -3.7789e-02],\n",
       "                      [ 1.5668e-01,  3.4355e-01, -2.9130e-01, -9.1078e-02,  4.5884e-02],\n",
       "                      [ 1.8180e-02,  3.0104e-01, -6.5105e-02, -3.0221e-01, -1.3640e-01],\n",
       "                      [ 8.5021e-02,  5.6720e-01,  2.9270e-01,  9.4537e-02,  5.0543e-02],\n",
       "                      [-6.6845e-02, -7.4374e-02, -2.2764e-01,  3.0029e-02, -3.2590e-02],\n",
       "                      [ 5.8631e-02, -6.0386e-01,  5.6737e-02,  8.1018e-02, -4.0123e-02],\n",
       "                      [-1.9298e-02,  1.1506e-01,  1.6077e-01,  1.2066e-02,  1.2383e-02],\n",
       "                      [-5.2743e-03, -3.9564e-01, -2.8900e-01,  8.3791e-03,  2.2048e-02],\n",
       "                      [-8.0413e-02,  1.4414e-01,  1.9158e-01,  1.3825e-01, -2.5461e-01],\n",
       "                      [ 1.2580e-01,  1.5982e-01, -2.8045e-01, -1.5678e-01,  8.1627e-02],\n",
       "                      [-6.1370e-03, -1.4894e-01, -4.8767e-02,  8.5243e-02,  3.3220e-02],\n",
       "                      [ 3.6392e-03,  1.1218e-01,  2.0304e-01,  1.2402e-01,  2.9177e-02],\n",
       "                      [ 4.1158e-01,  5.1973e-01,  4.6464e-01, -3.6364e-03,  2.1288e-01],\n",
       "                      [ 1.9025e-02, -5.9730e-01, -7.7716e-01, -4.9395e-01, -1.2632e-01],\n",
       "                      [ 4.0256e-01,  2.3909e-01,  5.5556e-02,  2.6690e-01,  1.0778e-01],\n",
       "                      [-1.8283e-02, -5.4196e-01, -5.4122e-01, -2.1766e-01,  1.1688e-02],\n",
       "                      [-1.1288e-02, -1.7204e-01, -2.2165e-01, -9.8190e-02, -3.2883e-02],\n",
       "                      [-6.0477e-01, -6.5345e-01, -2.4813e-01, -2.2104e-01, -3.5216e-02]],\n",
       "                     device='cuda:0')),\n",
       "             ('linear_proj_input.bias',\n",
       "              tensor([ 0.0370, -0.0483, -0.0148, -0.1435,  0.1869, -0.0182, -0.0690, -0.1252,\n",
       "                      -0.0078,  0.3191, -0.0121, -0.0344,  0.0321, -0.0233, -0.0020, -0.0249,\n",
       "                      -0.0207, -0.1385,  0.1418, -0.1355,  0.0190, -0.0434,  0.0321, -0.0370,\n",
       "                      -0.0124, -0.0026, -0.1719, -0.0730, -0.1272,  0.0599,  0.0650,  0.1398],\n",
       "                     device='cuda:0')),\n",
       "             ('RNN_layer.weight_ih_l0',\n",
       "              tensor([[ 0.1594, -0.0965,  0.0109,  ..., -0.0118,  0.2028,  0.1001],\n",
       "                      [-0.0291,  0.0880,  0.1628,  ..., -0.2393, -0.2392,  0.5394],\n",
       "                      [ 0.0824,  0.3141, -0.0990,  ..., -0.1905, -0.1305,  0.1003],\n",
       "                      ...,\n",
       "                      [ 0.0835,  0.0499,  0.0444,  ...,  0.1122,  0.0081,  0.1412],\n",
       "                      [-0.0444,  0.0677, -0.0121,  ...,  0.0739,  0.0154,  0.1453],\n",
       "                      [ 0.0483, -0.1397, -0.0748,  ...,  0.2984,  0.1866, -0.5527]],\n",
       "                     device='cuda:0')),\n",
       "             ('RNN_layer.weight_hh_l0',\n",
       "              tensor([[-0.4504,  0.0772,  0.1332,  ..., -0.1013,  0.2546, -0.1632],\n",
       "                      [ 0.2117,  0.3548, -0.2234,  ...,  0.4115, -0.1493, -0.1510],\n",
       "                      [-0.0083,  0.0595,  0.4284,  ..., -0.2112, -0.4362, -0.0600],\n",
       "                      ...,\n",
       "                      [ 0.2256, -0.0818,  0.0094,  ..., -0.3358, -0.2372,  0.0734],\n",
       "                      [-0.2035, -0.2106, -0.0286,  ..., -0.3444,  0.0473, -0.0997],\n",
       "                      [-0.2764, -0.4018,  0.2315,  ..., -0.2499,  0.1761,  0.4557]],\n",
       "                     device='cuda:0')),\n",
       "             ('RNN_layer.bias_ih_l0',\n",
       "              tensor([ 0.0775, -0.4375,  0.2424,  0.0067,  0.0453,  0.0860, -0.0262,  0.1131,\n",
       "                       0.1798, -0.0077, -0.2426, -0.0585,  0.1172,  0.1788,  0.2806, -0.3084,\n",
       "                      -0.2340, -0.0395, -0.1850, -0.0881,  0.0210,  0.1234,  0.1700, -0.0751,\n",
       "                       0.2974,  0.0756, -0.0442, -0.0330,  0.0236, -0.1184, -0.0880,  0.2710],\n",
       "                     device='cuda:0')),\n",
       "             ('RNN_layer.bias_hh_l0',\n",
       "              tensor([-0.1296, -0.2991, -0.0877, -0.3008, -0.2564, -0.0368, -0.1254, -0.0691,\n",
       "                       0.0060, -0.0222, -0.1819,  0.1888,  0.1329,  0.0858,  0.2088, -0.1173,\n",
       "                      -0.2822, -0.2405, -0.0842,  0.2293, -0.0317,  0.1421,  0.1304,  0.0725,\n",
       "                       0.2839, -0.0754,  0.1060, -0.0693, -0.0082, -0.0283, -0.0088,  0.5500],\n",
       "                     device='cuda:0')),\n",
       "             ('linear_post_rnn.weight',\n",
       "              tensor([[-0.0232, -0.1672, -0.0092,  0.0039, -0.0029,  0.3387, -0.1809, -0.1244,\n",
       "                        0.1942, -0.0953, -0.1711,  0.1484, -0.1462, -0.1847, -0.1398, -0.1481,\n",
       "                       -0.1776, -0.1282,  0.1233, -0.0350,  0.0719,  0.1375,  0.1104, -0.1111,\n",
       "                       -0.0748, -0.0766,  0.0508,  0.0894,  0.0058, -0.1909, -0.2919,  0.2012]],\n",
       "                     device='cuda:0')),\n",
       "             ('linear_post_rnn.bias', tensor([0.0481], device='cuda:0'))])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.reg_training_loop_rmspe(optimizer=optimizer,model=RNN_model,train_loader=train_loader,val_loader=test_loader,list_train_loss=train_loss,list_val_loss=val_loss,device=device,n_epochs=100,ot_steps=10,report_interval=5,eps=0,scaler=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66a81f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozen_conv.frozen_conv.weight False\n",
      "linear_proj_input.weight True\n",
      "linear_proj_input.bias True\n",
      "RNN_layer.weight_ih_l0 True\n",
      "RNN_layer.weight_hh_l0 True\n",
      "RNN_layer.bias_ih_l0 True\n",
      "RNN_layer.bias_hh_l0 True\n",
      "linear_post_rnn.weight True\n",
      "linear_post_rnn.bias True\n"
     ]
    }
   ],
   "source": [
    "for name,param in RNN_model.named_parameters():\n",
    "    print(name,param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aec32541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e88bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=[x.cpu() for x in train_loss]\n",
    "val_loss=[x.cpu() for x in val_loss]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c4ccf4",
   "metadata": {},
   "source": [
    "Above is an example of the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "05ee5053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "x=np.linspace(1,49,49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ebe3da0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f5b9bf97c50>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAGdCAYAAADqsoKGAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQwNJREFUeJzt3X10VPWB//FPHkhSDQkJgYRAIFpXEVugEkizKz6UOYTa30qN7CKiIPVgtYJCejhAz08CZbsJyNJoYWWL0npaEFZFXfVsRGNCaQ1gg5TWRVpdKRDzAPIjAVITyNzfH2MGxkwy9yZ3nm7er3PmQO7cufc7dx7uZ75PN8YwDEMAAABRLjbcBQAAALADoQYAADgCoQYAADgCoQYAADgCoQYAADgCoQYAADgCoQYAADgCoQYAADhCfLgLECput1uffvqpBg4cqJiYmHAXBwAAmGAYhs6ePavs7GzFxvZcF9NvQs2nn36qnJyccBcDAAD0wvHjxzVixIge1+k3oWbgwIGSPAclJSUlzKUBAABmtLS0KCcnx3se70m/CTWdTU4pKSmEGgAAooyZriN0FAYAAI5AqAEAAI5AqAEAAI5AqAEAAI5AqAEAAI5AqAEAAI5AqAEAAI5AqAEAAI7QbybfC5aODmnPHqm+Xho2TJo8WYqLC3epAADofwg1fbBzp/TYY9KJE5eWjRghPfmkVFQUvnIBANAf0fzUSzt3SjNm+AYaSaqr8yzfuTM85QIAoL8i1PRCR4enhsYwut7XuWzRIs96AAAgNAg1vbBnT9camssZhnT8uGc9AAAQGoSaXqivt3c9AADQd4SaXhg2zN71AABA3xFqemHyZM8op5gY//fHxEg5OZ71AABAaBBqeiEuzjNsW+oabDr/Li9nvhoAAEKJUNNLRUXSiy9Kw4f7Lh8xwrOceWoAAAitXoWajRs3Kjc3V0lJScrPz9f+/fu7XXfz5s2aPHmy0tLSlJaWJpfL1WX9mJgYv7cnnnjCu87p06c1e/ZspaSkaNCgQXrggQd07ty53hTfNkVF0tGjUlWVtG2b599PPiHQAAAQDpZDzY4dO1RcXKySkhIdOHBA48aNU2FhoZqamvyuX11drVmzZqmqqko1NTXKycnR1KlTVVdX512nvr7e57ZlyxbFxMTorrvu8q4ze/ZsffDBB3rrrbf0+uuv6ze/+Y0efPDBXjxle8XFSbfeKs2a5fmXJicAAMIjxjD8TSHXvfz8fE2cOFEbNmyQJLndbuXk5GjhwoVatmxZwMd3dHQoLS1NGzZs0Jw5c/yu893vfldnz55VZWWlJOnw4cMaM2aM3nvvPeXl5UmSKioqdPvtt+vEiRPKzs4OuN+WlhalpqaqublZKSkpZp8uAAAIIyvnb0s1Ne3t7aqtrZXL5bq0gdhYuVwu1dTUmNpGa2urLly4oPT0dL/3NzY26o033tADDzzgXVZTU6NBgwZ5A40kuVwuxcbGat++fX6309bWppaWFp8bAABwLkuh5tSpU+ro6FBmZqbP8szMTDU0NJjaxtKlS5Wdne0TjC733HPPaeDAgSq6rGNKQ0ODhg4d6rNefHy80tPTu91vaWmpUlNTvbecnBxT5QMAANEppKOfysrKtH37dr388stKSkryu86WLVs0e/bsbu83a/ny5Wpubvbejh8/3qftAQCAyBZvZeWMjAzFxcWpsbHRZ3ljY6OysrJ6fOy6detUVlamt99+W2PHjvW7zp49e3TkyBHt2LHDZ3lWVlaXjsgXL17U6dOnu91vYmKiEhMTAz0lAADgEJZqahISEjRhwgRvB17J01G4srJSBQUF3T5u7dq1Wr16tSoqKnz6xXzZs88+qwkTJmjcuHE+ywsKCnTmzBnV1tZ6l73zzjtyu93Kz8+38hQAAIBDWaqpkaTi4mLNnTtXeXl5mjRpksrLy3X+/HnNmzdPkjRnzhwNHz5cpaWlkqQ1a9ZoxYoV2rZtm3Jzc719YJKTk5WcnOzdbktLi1544QX927/9W5d9Xn/99Zo2bZrmz5+vTZs26cKFC1qwYIHuvvtuUyOfAACA81kONTNnztTJkye1YsUKNTQ0aPz48aqoqPB2Hj527JhiYy9VAD399NNqb2/XjBkzfLZTUlKilStXev/evn27DMPQrFmz/O5369atWrBggaZMmaLY2Fjdddddeuqpp6wWHwAAOJTleWqiFfPUAAAQfYI2Tw0AAECkItQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABHINQAAABH6FWo2bhxo3Jzc5WUlKT8/Hzt37+/23U3b96syZMnKy0tTWlpaXK5XH7XP3z4sO644w6lpqbqyiuv1MSJE3Xs2DHv/bfeeqtiYmJ8bg899FBvig8AABzIcqjZsWOHiouLVVJSogMHDmjcuHEqLCxUU1OT3/Wrq6s1a9YsVVVVqaamRjk5OZo6darq6uq863z88ce66aabNHr0aFVXV+vQoUN6/PHHlZSU5LOt+fPnq76+3ntbu3at1eIDAACHijEMw7DygPz8fE2cOFEbNmyQJLndbuXk5GjhwoVatmxZwMd3dHQoLS1NGzZs0Jw5cyRJd999twYMGKBf/epX3T7u1ltv1fjx41VeXm6luF4tLS1KTU1Vc3OzUlJSerUNAAAQWlbO35Zqatrb21VbWyuXy3VpA7GxcrlcqqmpMbWN1tZWXbhwQenp6ZI8oeiNN97Qtddeq8LCQg0dOlT5+fl65ZVXujx269atysjI0Ne+9jUtX75cra2t3e6nra1NLS0tPjcAAOBclkLNqVOn1NHRoczMTJ/lmZmZamhoMLWNpUuXKjs72xuMmpqadO7cOZWVlWnatGnatWuX7rzzThUVFWn37t3ex91zzz369a9/raqqKi1fvly/+tWvdO+993a7n9LSUqWmpnpvOTk5Vp4qAACIMvGh3FlZWZm2b9+u6upqb38Zt9stSZo+fboWL14sSRo/frzeffddbdq0Sbfccosk6cEHH/Ru5+tf/7qGDRumKVOm6OOPP9ZXv/rVLvtavny5iouLvX+3tLQQbAAAcDBLNTUZGRmKi4tTY2Ojz/LGxkZlZWX1+Nh169aprKxMu3bt0tixY322GR8frzFjxvisf/311/uMfvqy/Px8SdJHH33k9/7ExESlpKT43AAAgHNZCjUJCQmaMGGCKisrvcvcbrcqKytVUFDQ7ePWrl2r1atXq6KiQnl5eV22OXHiRB05csRn+Z///GeNGjWq220ePHhQkjRs2DArTwEAADiU5ean4uJizZ07V3l5eZo0aZLKy8t1/vx5zZs3T5I0Z84cDR8+XKWlpZKkNWvWaMWKFdq2bZtyc3O9fW+Sk5OVnJwsSVqyZIlmzpypm2++WbfddpsqKir02muvqbq6WpJnyPe2bdt0++23a/DgwTp06JAWL16sm2++2afWBwAA9GNGL/zsZz8zRo4caSQkJBiTJk0y9u7d673vlltuMebOnev9e9SoUYakLreSkhKfbT777LPGNddcYyQlJRnjxo0zXnnlFe99x44dM26++WYjPT3dSExMNK655hpjyZIlRnNzs+kyNzc3G5IsPQYAAISXlfO35XlqohXz1AAAEH2CNk8NAABApCLUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAAR+hVqNm4caNyc3OVlJSk/Px87d+/v9t1N2/erMmTJystLU1paWlyuVx+1z98+LDuuOMOpaam6sorr9TEiRN17Ngx7/2ff/65HnnkEQ0ePFjJycm666671NjY2JviAwAAB7Icanbs2KHi4mKVlJTowIEDGjdunAoLC9XU1OR3/erqas2aNUtVVVWqqalRTk6Opk6dqrq6Ou86H3/8sW666SaNHj1a1dXVOnTokB5//HElJSV511m8eLFee+01vfDCC9q9e7c+/fRTFRUV9eIpAwAAJ4oxDMOw8oD8/HxNnDhRGzZskCS53W7l5ORo4cKFWrZsWcDHd3R0KC0tTRs2bNCcOXMkSXfffbcGDBigX/3qV34f09zcrCFDhmjbtm2aMWOGJOnDDz/U9ddfr5qaGn3zm98MuN+WlhalpqaqublZKSkpZp8uAAAIIyvnb0s1Ne3t7aqtrZXL5bq0gdhYuVwu1dTUmNpGa2urLly4oPT0dEmeUPTGG2/o2muvVWFhoYYOHar8/Hy98sor3sfU1tbqwoULPvsdPXq0Ro4caXq/AADA2SyFmlOnTqmjo0OZmZk+yzMzM9XQ0GBqG0uXLlV2drY3oDQ1NencuXMqKyvTtGnTtGvXLt15550qKirS7t27JUkNDQ1KSEjQoEGDTO+3ra1NLS0tPjcAAOBc8aHcWVlZmbZv367q6mpvfxm32y1Jmj59uhYvXixJGj9+vN59911t2rRJt9xyS6/2VVpaqlWrVtlTcAAAEPEs1dRkZGQoLi6uy6ijxsZGZWVl9fjYdevWqaysTLt27dLYsWN9thkfH68xY8b4rH/99dd7Rz9lZWWpvb1dZ86cMb3f5cuXq7m52Xs7fvy42acJAACikKVQk5CQoAkTJqiystK7zO12q7KyUgUFBd0+bu3atVq9erUqKiqUl5fXZZsTJ07UkSNHfJb/+c9/1qhRoyRJEyZM0IABA3z2e+TIER07dqzb/SYmJiolJcXnBgAAnMty81NxcbHmzp2rvLw8TZo0SeXl5Tp//rzmzZsnSZozZ46GDx+u0tJSSdKaNWu0YsUKbdu2Tbm5ud4+MMnJyUpOTpYkLVmyRDNnztTNN9+s2267TRUVFXrttddUXV0tSUpNTdUDDzyg4uJipaenKyUlRQsXLlRBQYGpkU8AAMD5LIeamTNn6uTJk1qxYoUaGho0fvx4VVRUeDsPHzt2TLGxlyqAnn76abW3t3uHYncqKSnRypUrJUl33nmnNm3apNLSUj366KO67rrr9NJLL+mmm27yrv/Tn/5UsbGxuuuuu9TW1qbCwkL9+7//e2+eMwAAcCDL89REK+apAQAg+gRtnhoAAIBIRagBAACOQKgBAACOQKgBAACOQKgBAACOQKgBAACOQKgBAACOQKgBAACOQKgBAACOQKgBAACOQKgBAACOQKgBAACOQKgBAACOQKgBAACOQKgBAACOQKgBAACOQKgBAACOQKgBAACOQKgBAACOQKgBAACOQKgBAACOQKgBAACOQKgBAACOQKgBAACOQKgBAACOEB/uAvQHHR3Snj1Sfb00bJg0ebIUFxfuUgEA4CyEmiDbuVN67DHpxIlLy0aMkJ58UioqCl+5AABwGpqfgmjnTmnGDN9AI0l1dZ7lO3eGp1wAADgRoSZIOjo8NTSG0fW+zmWLFnnWAwAAfUeoCZI9e7rW0FzOMKTjxz3rAQCAviPUBEl9vb3rAQCAnhFqgmTYMHvXAwAAPSPUBMnkyZ5RTjEx/u+PiZFycjzrAQCAviPUBElcnGfYttQ12HT+XV7OfDUAANiFUBNERUXSiy9Kw4f7Lh8xwrOceWoAALBPr0LNxo0blZubq6SkJOXn52v//v3drrt582ZNnjxZaWlpSktLk8vl6rL+/fffr5iYGJ/btGnTfNbJzc3tsk5ZWVlvih9SRUXS0aNSVZW0bZvn308+IdAAAGA3yzMK79ixQ8XFxdq0aZPy8/NVXl6uwsJCHTlyREOHDu2yfnV1tWbNmqW///u/V1JSktasWaOpU6fqgw8+0PDLqjCmTZumX/ziF96/ExMTu2zrxz/+sebPn+/9e+DAgVaLHxZxcdKttwZej8spAADQe5ZDzfr16zV//nzNmzdPkrRp0ya98cYb2rJli5YtW9Zl/a1bt/r8/cwzz+ill15SZWWl5syZ412emJiorKysHvc9cODAgOtEKy6nAABA31hqfmpvb1dtba1cLtelDcTGyuVyqaamxtQ2WltbdeHCBaWnp/ssr66u1tChQ3Xdddfp4Ycf1meffdblsWVlZRo8eLC+8Y1v6IknntDFixe73U9bW5taWlp8bpHKyuUUOjqk6mrp+ec9/zIjMQAAHpZqak6dOqWOjg5lZmb6LM/MzNSHH35oahtLly5Vdna2TzCaNm2aioqKdNVVV+njjz/Wj370I337299WTU2N4r5of3n00Ud14403Kj09Xe+++66WL1+u+vp6rV+/3u9+SktLtWrVKitPLywCXU4hJsZzOYXp06VXXzVXm0MzFgCgXzIsqKurMyQZ7777rs/yJUuWGJMmTQr4+NLSUiMtLc34wx/+0ON6H3/8sSHJePvtt7td59lnnzXi4+ONzz//3O/9n3/+udHc3Oy9HT9+3JBkNDc3ByxnKFVVGYYnvvR8W7XKMGJiui6PifHcXnrJs72XXjKMESN81xkx4tL9AABEk+bmZtPnb0vNTxkZGYqLi1NjY6PP8sbGxoB9XdatW6eysjLt2rVLY8eO7XHdq6++WhkZGfroo4+6XSc/P18XL17U0aNH/d6fmJiolJQUn1skMnuZhCefDHxxzBdftP+q4DR3AQCihaVQk5CQoAkTJqiystK7zO12q7KyUgUFBd0+bu3atVq9erUqKiqUl5cXcD8nTpzQZ599pmE9XEPg4MGDio2N9TviKpqYvUzC6dPd39d5ccwf/MDeq4Lv3Cnl5kq33Sbdc4/n39zc3oUjAACCzfI8NcXFxdq8ebOee+45HT58WA8//LDOnz/vHQ01Z84cLV++3Lv+mjVr9Pjjj2vLli3Kzc1VQ0ODGhoadO7cOUnSuXPntGTJEu3du1dHjx5VZWWlpk+frmuuuUaFhYWSpJqaGpWXl+sPf/iD/vd//1dbt27V4sWLde+99yotLc2O4xA2Zi6n8KU+1d06ebL7+6xeFdxK52UAACJCb9q3fvaznxkjR440EhISjEmTJhl79+713nfLLbcYc+fO9f49atQoQ1KXW0lJiWEYhtHa2mpMnTrVGDJkiDFgwABj1KhRxvz5842GhgbvNmpra438/HwjNTXVSEpKMq6//nrjX//1X7vtT+OPlTa5UHvppUt9Y/z1l1m1yly/GzO3bdsu7ffiRU+fnm3bPP9evHhp+Zf75Xy5XDk5l9YHACBYrJy/YwzDX4OF87S0tCg1NVXNzc0R2b/G3zw1OTme60NNn+5p9qmr89+8FBMjZWT0XFPTqarKMxFgT/PipKd7mprMbgsAgGCxcv7m2k8RoqfLKZi5OObGjeavCh6oaenVV82V2WwnZwAAQsHyjMIInp4up9B5cUx/tSvl5ZfCz4wZngBzeY3O5VcFlwLPi/OlSaC7ZbaTMwAAoUDzU5QJNLFeT81YRUWeYdlmmpaGDJFOneq+uWvECE9NUue+zUz4x6SAAACrrJy/qamJMoEujllU5OmD0114MNtkNHu2p8mrp1qfzm2auW4V17YCAAQbNTX9jNmamqoqz9w4PdX6SJf653z5XdQZfl580fNvoHU6t0dtDgDgclbO34SafqajI/BIqsublnoKGZ3b+nKH48u3NXy45/89rdO5P7PXturcN+EHAJyP5id0q3MkVaAOxZ0Boafmrj17ug8rkmfbPd3fuc7x49JPfiKtXNk1aHWOyLq8NoemLACAPwzp7oc6R1J11qJ0GjHCNzwEYueQbjPXturoYKZjAED3aH7qx/rahGO2f45d3n5buv9+c01ZZp8HzVgAENlofoIpgUZSBdJ53aqe+ud01gb1tE5aWs8X7OxUXR24uavz+lZmnpfdzVgEJAAIL5qf0GtmZjp+8snA6zz2mL3l6mwW6+jwBKHnn/f8e/kVyq02Y/W0rc7tcUVzAAizoF2BKsJE8gUto91LL3W9AGZOjme5mXU6L6D55Qt6fvkCmm+/be6inVVV/vc3YoTv/sxesLOnbXXe76/snRckvfw4mNXdxUbDLVLLBcC5uKClH/SpCa6+zijcWXMi+R+R9eKL5i7sOWKEtH699M//3P28OCtXSiUlgZ9T51w9Pc2xs2OHVFxsrZ9Pb2aFjoTRXWbLRTMcADtZOn8HPWJFCGpqIp/ZGp/OGhB/tSL/+Z+Ba2HS083V+Pz614G3NWSI+dqj7p5jsGt97GC2XIGeXzhRywREJ2pq/KCmJjqY+ZXf0/Wt0tPtG5H1059Kixfbs61t26TERPtrfULBzCSLZmrIrEwXYLdIrf0CEBgzCvtBqHGW7sLP8897OuoGkp4u/b//13MzVmmpdO+99pTXzHD0jAzp5MnA26qq8ozuMtvME6qh+0OGdF/+cAUyydylPAg2QORiSDccr7vh6MOGmXv8Y495+tb0NKtyerq5bZm5orkUeDi6mUAjecKJ2ZoHO/rBmJ1ksafyWx1ub6ZcZh//2GPdT+wYE+OZ2HH69MCXBYl0oS57NB8rOFiQm8IiBn1q+gezI6kuXgzch8fstl54oed+Pi+95OnHYabvjZnbqlXm+7fY0Q+mqsq+sm/bZv61tKN/jtmyBxoxF+lCXXa790d/J/TEyvmbUAPHCdSZ+PIv3kBfpma3FSggmT25DhnSc4gaMcLccPS2NnPrdQaynoKPmXBntcO02eMeKJAF2pbZMLloUWR20DYj1J3L7d5fNIdJhAahxg9CTf9iZiSV3dvq6eRqV63PqlXmTtI//an5EBUo+HTWapkZddbbGrLLT2JW5hGyq5bJ7HEI9DoHS3f7tDrnkh3lsLq/no5XpI72g3XB/FwQavwg1PQ/dn7I7NiWHbU+ZmseFiwwt56ZW0/D0S8Pd2aen5mTmNkgYqYZzu5apnDUKvS0TyvNa4Zh/n3c3XpW92fnJJiIXMH+XBBq/CDUIBL0tdbH7EnFbE2Nmdvl/WDMNBsFmjk60Ens1782V66e5huyUsu0aJG5/VltojITIPraDGe27Nu2mT/x9LSe2VDdub+eym621rEzIMF+dv5YM/u56A1CjR+EGkSKvnyRmG3G6uxTY2c/mL4+v3AEMjO1TMFoojITIOxohjP7GtrVudxsEHn7bfsmwbTSuTxSRWJHaDtqV0JV20ao8YNQA6ew0oxlVz8YO5j9ld85k3NP5erNCTFQvxQ7m6gCBQg7m+FC2bm8c1uB3jNmr9Nm9pj29PoFk101GZHWEdqu2hWrzZG9Rajxg1ADJzHbjGVHPxi7WB1ebUeHabNfpnY1UZm5tIbZkGG2Ga6zWSxUncs7a316es+YDbDp6fZ0Lr+cHc1+ne+JvoaRSOwIbWftipXmyL4g1PhBqIHT9LXTZyc7R4oFKq+VmiEz/XPsrGWyo4nKzqYzs9vqrgNzsDqXd9c/pzdTGJgJSFaCgR3Nfp3rmN2nnSPTQlEbZWftCjU1YUSoAboXqqp9qzVDZoYD21nL1JcmKiu1K2ZuZprhzAw1tzuQmWkOsmsSTKvD++1o9rNrSgE7R4qZ/UyYYbV2xa7XuS8INX4QaoDIEI45hOwqd6AQZefsy2aa4cw8Tzs7l1s5QVkpe18DmZmOyWab/cz2BwrU+drqyDSzHbn7GnzsnmU7FE3YhBo/CDVA5Ii0OYTMChSizAQIs51tzTTDWSm3HZ3LrZ6g+lp2s7UK//f/Wg+Nfd1WoCkFzHYuNxPIzM7+3d0x9zeqLtD7z+z+7HidAyHU+EGoAWAHOy6tYWcznFl2dS63qi9lN1urEI5QY+YWaGSalZohM1MKWAk+Pb3/OkdGBtpfqPoDEWr8INQACBUzwSCUTWed7OpcHipmaxXsHELeWXPS0z7NTikQaGSalY7cdgUfMzWAoeoAbJaV83eMYRhGaK4HHl4tLS1KTU1Vc3OzUlJSwl0cAA7X0SHt2SPV10vDhkmTJ0txcdbX6e927pRmzPD8//KzVUyM598XX5SmT5dyc6W6Ot91Ll93+HDP/3taZ8QI6ZNPpFdf7XmfK1dKJSWBy15VJZ0+LT32mHTixKXlOTlSeblUVCRVV0u33RZ4W3apqpJuvdXz/+7ef88/L91zT+BtbdsmzZoV1OJKsnb+JtQAACLazp09B4POdQKFHynwOpdvr7t9mglRnQEpLq7n8NrREXhbGRnSyZM9HyOzzAQRs0Hr8oAUTIQaPwg1ABC9zNRqmQ0/gdYxs08zIerL2+tOoG3t2CEVF9sTfMwEETNB6/LQFmyWzt+9ad/asGGDMWrUKCMxMdGYNGmSsW/fvm7X/fnPf27cdNNNxqBBg4xBgwYZU6ZM6bL+3LlzDUk+t8LCQp91PvvsM+Oee+4xBg4caKSmphrf+973jLNnz5ouM31qAMD57JpR2IxQTk8Q6suehHK28UCC2lF4+/btRkJCgrFlyxbjgw8+MObPn28MGjTIaGxs9Lv+PffcY2zcuNF4//33jcOHDxv333+/kZqaapw4ccK7zty5c41p06YZ9fX13tvp06d9tjNt2jRj3Lhxxt69e409e/YY11xzjTFr1izT5SbUAADsFsrpCUJ92ZNwdGb3J6gdhfPz8zVx4kRt2LBBkuR2u5WTk6OFCxdq2bJlAR/f0dGhtLQ0bdiwQXPmzJEk3X///Tpz5oxeeeUVv485fPiwxowZo/fee095eXmSpIqKCt1+++06ceKEsrOzA+6X5icAQLQL1AxnpXnNjv2FgpXzd7yVDbe3t6u2tlbLly/3LouNjZXL5VJNTY2pbbS2turChQtKT0/3WV5dXa2hQ4cqLS1N3/rWt/Qv//IvGjx4sCSppqZGgwYN8gYaSXK5XIqNjdW+fft05513dtlPW1ub2travH+3tLRYeaoAAEScuLie+8QUFXk6MtsVRALtL9JYCjWnTp1SR0eHMjMzfZZnZmbqww8/NLWNpUuXKjs7Wy6Xy7ts2rRpKioq0lVXXaWPP/5YP/rRj/Ttb39bNTU1iouLU0NDg4YOHepb8Ph4paenq6Ghwe9+SktLtWrVKitPDwCAqBdtQcROlkJNX5WVlWn79u2qrq5WUlKSd/ndd9/t/f/Xv/51jR07Vl/96ldVXV2tKVOm9Gpfy5cvV3FxsffvlpYW5eTk9L7wAAAgosVaWTkjI0NxcXFqbGz0Wd7Y2KisrKweH7tu3TqVlZVp165dGjt2bI/rXn311crIyNBHH30kScrKylJTU5PPOhcvXtTp06e73W9iYqJSUlJ8bgAAwLkshZqEhARNmDBBlZWV3mVut1uVlZUqKCjo9nFr167V6tWrVVFR4dMvpjsnTpzQZ599pmHDhkmSCgoKdObMGdXW1nrXeeedd+R2u5Wfn2/lKQAAAIeyFGokqbi4WJs3b9Zzzz2nw4cP6+GHH9b58+c1b948SdKcOXN8OhKvWbNGjz/+uLZs2aLc3Fw1NDSooaFB586dkySdO3dOS5Ys0d69e3X06FFVVlZq+vTpuuaaa1RYWChJuv766zVt2jTNnz9f+/fv1+9+9zstWLBAd999t6mRTwAAwPks96mZOXOmTp48qRUrVqihoUHjx49XRUWFt/PwsWPHFBt7KSs9/fTTam9v14zO6RK/UFJSopUrVyouLk6HDh3Sc889pzNnzig7O1tTp07V6tWrlZiY6F1/69atWrBggaZMmaLY2Fjdddddeuqpp3r7vAEAgMNwmQQAABCxrJy/LTc/AQAARCJCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcARCDQAAcIRehZqNGzcqNzdXSUlJys/P1/79+7tdd/PmzZo8ebLS0tKUlpYml8vV4/oPPfSQYmJiVF5e7rM8NzdXMTExPreysrLeFB8AADiQ5VCzY8cOFRcXq6SkRAcOHNC4ceNUWFiopqYmv+tXV1dr1qxZqqqqUk1NjXJycjR16lTV1dV1Wffll1/W3r17lZ2d7XdbP/7xj1VfX++9LVy40GrxAQCAQ1kONevXr9f8+fM1b948jRkzRps2bdIVV1yhLVu2+F1/69at+sEPfqDx48dr9OjReuaZZ+R2u1VZWemzXl1dnRYuXKitW7dqwIABfrc1cOBAZWVleW9XXnml1eIDAACHshRq2tvbVVtbK5fLdWkDsbFyuVyqqakxtY3W1lZduHBB6enp3mVut1v33XeflixZohtuuKHbx5aVlWnw4MH6xje+oSeeeEIXL17sdt22tja1tLT43AAAgHPFW1n51KlT6ujoUGZmps/yzMxMffjhh6a2sXTpUmVnZ/sEozVr1ig+Pl6PPvpot4979NFHdeONNyo9PV3vvvuuli9frvr6eq1fv97v+qWlpVq1apWpMgEAgOhnKdT0VVlZmbZv367q6molJSVJkmpra/Xkk0/qwIEDiomJ6faxxcXF3v+PHTtWCQkJ+v73v6/S0lIlJiZ2WX/58uU+j2lpaVFOTo6NzwYAAEQSS81PGRkZiouLU2Njo8/yxsZGZWVl9fjYdevWqaysTLt27dLYsWO9y/fs2aOmpiaNHDlS8fHxio+P11//+lf98Ic/VG5ubrfby8/P18WLF3X06FG/9ycmJiolJcXnBgAAnMtSqElISNCECRN8Ovl2dvotKCjo9nFr167V6tWrVVFRoby8PJ/77rvvPh06dEgHDx703rKzs7VkyRK9+eab3W7z4MGDio2N1dChQ608BQAA4FCWm5+Ki4s1d+5c5eXladKkSSovL9f58+c1b948SdKcOXM0fPhwlZaWSvL0l1mxYoW2bdum3NxcNTQ0SJKSk5OVnJyswYMHa/DgwT77GDBggLKysnTddddJkmpqarRv3z7ddtttGjhwoGpqarR48WLde++9SktL69MBAAAAzmA51MycOVMnT57UihUr1NDQoPHjx6uiosLbefjYsWOKjb1UAfT000+rvb1dM2bM8NlOSUmJVq5caWqfiYmJ2r59u1auXKm2tjZdddVVWrx4sU+fGQAA0L/FGIZhhLsQodDS0qLU1FQ1NzfTvwYAgChh5fzNtZ8AAIAjEGoAAIAjEGoAAIAjEGoAAIAjEGoAAIAjEGoAAIAjEGoAAIAjEGoAAIAjEGoAAIAjEGoAAIAjEGoAAIAjEGoAAIAjEGoAAIAjEGoAAIAjxIe7AAD8cHdIJ/dIf6uXvjJMGjJZio0Ld6kAIKIRaoBIc3ynVPuY1Hri0rIrRkgTnpRyisJXLgCIcDQ/AZHk+E5pzwzfQCNJrXWe5cd3hqdcABAFCDVApHB3eGpoZPi584tltYs86wEAuiDUAJHi5J6uNTQ+DKn1uGc9AEAXhBogUvyt3t71AKCfoaMwECm+Msze9YBowWg/2IRQA0SKIZM9o5xa6+S/X02M5/4hk0NdMiB4GO0HG9H8BESK2DjPF7kkKeZLd37x94RyfsHCORjtB5sRaoBIklMkTX5RumK47/IrRniW88sVTsFoPwQBzU9ApMkpkoZPp48BnM3KaL/MW0NVKkQ5Qg0QiWLj+CKHszHaD0FA8xMAIPQY7YcgINQAAEKvc7Rfl07xnWKkK3IY7QdLCDUAgNBjtB+CgFADAAgPRvvBZnQUBgCED6P9YCNCDQAgvBjtB5vQ/AQAAByBUAMAAByhV6Fm48aNys3NVVJSkvLz87V///5u1928ebMmT56stLQ0paWlyeVy9bj+Qw89pJiYGJWXl/ssP336tGbPnq2UlBQNGjRIDzzwgM6dO9eb4gMAAAeyHGp27Nih4uJilZSU6MCBAxo3bpwKCwvV1NTkd/3q6mrNmjVLVVVVqqmpUU5OjqZOnaq6urou67788svau3evsrOzu9w3e/ZsffDBB3rrrbf0+uuv6ze/+Y0efPBBq8UHnMPdITVWS0ef9/zLNXIA9HMxhmH4u5pYt/Lz8zVx4kRt2LBBkuR2u5WTk6OFCxdq2bJlAR/f0dGhtLQ0bdiwQXPmzPEur6urU35+vt5880195zvf0aJFi7Ro0SJJ0uHDhzVmzBi99957ysvLkyRVVFTo9ttv14kTJ/yGoC9raWlRamqqmpublZKSYuUpA5Hn+E7PxQAvv3bOFSM8834wDBaAg1g5f1uqqWlvb1dtba1cLtelDcTGyuVyqaamxtQ2WltbdeHCBaWnp3uXud1u3XfffVqyZIluuOGGLo+pqanRoEGDvIFGklwul2JjY7Vv3z6/+2lra1NLS4vPDXCE4zulPTO6Xgywtc6z/PjO8JQrXKixAvAFS6Hm1KlT6ujoUGZmps/yzMxMNTQ0mNrG0qVLlZ2d7ROM1qxZo/j4eD366KN+H9PQ0KChQ4f6LIuPj1d6enq3+y0tLVVqaqr3lpOTY6p8QERzd3hqaOSvgvWLZbWL+s+J/fhO6b9ypcrbpHfv8fz7X7n9L9gBkBTi0U9lZWXavn27Xn75ZSUlJUmSamtr9eSTT+qXv/ylYmK6uwaIdcuXL1dzc7P3dvz4cdu2jX4oUmoDTu7pWkPjw5Baj3vWczpqrAB8iaXJ9zIyMhQXF6fGxkaf5Y2NjcrKyurxsevWrVNZWZnefvttjR071rt8z549ampq0siRI73LOjo69MMf/lDl5eU6evSosrKyunREvnjxok6fPt3tfhMTE5WYmGjl6QH+RVL/lb/V27tetApYYxXjqbEaPp2ZaYF+xFJNTUJCgiZMmKDKykrvMrfbrcrKShUUFHT7uLVr12r16tWqqKjw6RcjSffdd58OHTqkgwcPem/Z2dlasmSJ3nzzTUlSQUGBzpw5o9raWu/j3nnnHbndbuXn51t5CoA1kVYb8JVh9q4XraixAuCH5cskFBcXa+7cucrLy9OkSZNUXl6u8+fPa968eZKkOXPmaPjw4SotLZXk6S+zYsUKbdu2Tbm5ud4+MMnJyUpOTtbgwYM1ePBgn30MGDBAWVlZuu666yRJ119/vaZNm6b58+dr06ZNunDhghYsWKC7777b1MgnRAh3R3Rd3yUSawOGTPbUErXWdVOuGM/9QyYHZ/+R8hpSYwXAD8uhZubMmTp58qRWrFihhoYGjR8/XhUVFd7Ow8eOHVNs7KUKoKefflrt7e2aMWOGz3ZKSkq0cuVK0/vdunWrFixYoClTpig2NlZ33XWXnnrqKavFR7hEUhOOWVZqA0J13ZrYOM8x2zNDUox8g80XfdImlAcnaETSa0iNFQA/LM9TE62YpyaMOptwutQsfHESnvxiZAabo897RtQE8vfbpNxZwS/P5fwGjBxPoAnGsYy019Dd4RnlFKjG6o5PIrs2EEBAVs7fXKUbwRWJTThmRXJtQE6R55iFoikoEl/DcNZYAYhYXNASwRXNHTo7+6+ou6kGYjy1I8HqvxJIbJyn2St3luffYJ3AI/U1zCny1BBdMdx3+RUjIrf2D0BQUVMDe3TXgTSaO3RSG+ARya9hKGusAEQ8Qg36rqcOpJHchGNGZ22A3+dX3j9qAyL9NeyssQLQ7xFq0DfddSDtnMflph3hHYJsh/5eGxDuYeQAYBJ9atB7Zq5DdOCH0jd++sWyL/dNiaImnFD1X4lEnc1wkqL6NQTgeIQa9J7ZDqRJGXTojHZ0ygUQBWh+Qu9Z6UCaO6t/N+E4QX9vhgMQ8Qg16D2rHUjp0Bn9eA0BRDCan9B7kT6PCwCgXyHUoPfoQOoc7g6psdpzaYjGas/f6BuOKRByND+hb5jHJfpF0oUqnYJjCoQFF7Tsq+5m0u1vOA7RKdIuVOkEHFPAVlbO34SavuDXGKKZ90rX3Q3L50rXlnFMAdtZOX/Tp6a3On+NffnLq3Mm3eM7w1MuwKxIvVBlNOOYAmFFn5reCDiTboxUu8gzp0dsHE0ziEyRfKHKaMUxRSTox+ccQk1vWPk11n6aJqpI1o8//GG9UKVTj3ukX/wTztfPu0UQanrD7K+sE69KR55Utxd7/HKHQbu+6J16wrBbP//wh+1ClU4+7lz8E+EU6ALD/aCTOn1qesPsr6yjW9XjxR5rF12au+L4Tk8Hw8rbpHfv8fz7X7nW++bYtR2no09UeOYZcvpxZ+4mBFt38x+ZucDw5ecch2L0U294Rzj08GssMUNqOxl4W1OqPE1UdgwBZSipOYxQ8eW35iTH/nmG+tNxD9UxRf/SUy1nQrrnR2wgU6qi7lInVs7fND/1RuevsT0z5AkMl4eILwJE7mzpSHngbbXWSX9YJtOdjrtjtfNyf2alT1SUffh7JVQXqoyG425X0y0X/4TdAjUtXfeYue04vJM6oaa3As2km5BuLtS0nbTniz4aThiRghEqXYXiQpWRftzt7uvDxT9hFzM/Wo9uNbcth3dSJ9T0RU+/xtwd5joMJg4xt69AX/SRfsKIJIxQCY9IPu50sEQkM/Ojte2k53zSdkr9uZM6HYX7qvPXWO4sz7+d1ctmOwxeMdzcfgJ90UfyCSPScHXx8Aj3caeDJaKV2R+jubO/+E//7aROqAmmziaqLweXK0Zc+uVn1xd9uE8Y0YQRKuERzuPe06hAZgFGpDP7Y3TE9MDnnGCJkKvS0/wUbIE6DJrpdGzmi96u7fQXXF08PMJx3OlgiWhnZf6j2LjQd1KPoLmnGNIdKewaAspQUmuYqDA8QnXczQwjtzL9QuatvGcQHt5wLvn90Rqufl8hmEqEq3T7EfGhRmJGYcBujdXm5u4w08Hyjk+kulcj5hdpF3zuwyOUxz3SfrSGaO4p5qmJVnYNAe0PQ0n5AocZVjpYHnlSPTbd1r0auSOkIqj6v18J9XG3e/6jvn6PRuBUIoQaRB++wGGWlQ6WQyd339dn+HTPL9JInNyS4ejhEa7jbtePVju+RyNwKhFCDaILX+DW9edaLbs6WDZWR9wvUknMJB4uwTruofqs2vU9GoFTiRBq+jMzH6BIOiFa/SKJpLKHS3+v1bI6KrC7X8ER+ItUUkRW//cLwTjuofqs2hnIIvCq9ISaaGPXidrMByjSTohWvkjaT0dW2cMhXLVakRYm7RhGHoG/SCVFbthyOruPeyg/q3YGsgicSoRQE03sChlmPkBS5DXzmP2COPHqF50+I6jsoRauZolIC8Kd+trBMgJ/kUqK3LAVDJEUlq0e957KHurPqt2BLMLm/OrVjMIbN25Ubm6ukpKSlJ+fr/3793e77ubNmzV58mSlpaUpLS1NLpery/orV67U6NGjdeWVV3rX2bdvn886ubm5iomJ8bmVlZX1pvjRqTOIfDlhd56oj+80tx0zU8L//jHPLdKmjTf7RXJ0qyKu7JcLxcyb4Zgl1673aLB0d0kTs4+NxFmowzmTeChnkO1pRuhwsHLcA5U91J/VYAThnCLpjqOeuZz+fpvn3zs+CcsPGcuhZseOHSouLlZJSYkOHDigcePGqbCwUE1NTX7Xr66u1qxZs1RVVaWamhrl5ORo6tSpqqur865z7bXXasOGDfrjH/+o3/72t8rNzdXUqVN18qTvhFg//vGPVV9f770tXLjQavGjk53XpjHzAfrbCc+tp3WCNW18T1+UZr5IEocEmEgtzFPeh+rLOdTNEv3h+klmLnsSalbDll1BJJQhIxLDstnj3jkNQE9lD/VnNVhBuC8/GmxkOdSsX79e8+fP17x58zRmzBht2rRJV1xxhbZs2eJ3/a1bt+oHP/iBxo8fr9GjR+uZZ56R2+1WZWWld5177rlHLpdLV199tW644QatX79eLS0tOnTokM+2Bg4cqKysLO/tyiuvtFr86GRnkrezbd3udvpAX5Rmvki8F3QLIBx9DEL55RzqZon+cv2kCPpF6lMmM2HLriASyvdxJIflQMd9+HRzZU8cam5/dn1WI7XW0SaWQk17e7tqa2vlcrkubSA2Vi6XSzU1Naa20draqgsXLig9Pb3bffz85z9Xamqqxo0b53NfWVmZBg8erG984xt64okndPHixW7309bWppaWFp9b1LIzydvZtm7ntsx+UQb6Ihkx3dz+Qt3HINRfzqFuluhPHVbD8Ys0UA1LoLBlNYhEyhXNIz0s93TczZY9RqFvQozEWkebWOoofOrUKXV0dCgzM9NneWZmpj788ENT21i6dKmys7N9gpEkvf7667r77rvV2tqqYcOG6a233lJGRob3/kcffVQ33nij0tPT9e6772r58uWqr6/X+vXr/e6ntLRUq1atsvL0Ipedv7rNdHj8yhdv9L+FqFOk1Y5yPXX6dHeEr0NnT50BQz30NtSjEvpTh9VQM9v5urvh6FY/Xz3tLyE9tO/jcIdlM52T+zoNwOdN4RlBZPfsxBEipKOfysrKtH37dlVXVyspKcnnvttuu00HDx7UqVOntHnzZv3zP/+z9u3bp6FDPVVzxcXF3nXHjh2rhIQEff/731dpaakSExO77Gv58uU+j2lpaVFOTk6QnlmQ2TnywszJLu+LqslQfch6c8Lv7oskXEMMA514wvHlHMpRCZE6Oija2THU1+pUCJF0RfNwhuW+juSzUvbMW8MzgsiBl9Sx1PyUkZGhuLg4NTY2+ixvbGxUVlZWj49dt26dysrKtGvXLo0dO7bL/VdeeaWuueYaffOb39Szzz6r+Ph4Pfvss91uLz8/XxcvXtTRo0f93p+YmKiUlBSfW9Syuw3UTNVjKKsngzXEMFRVq2aq9sP15RyqPiDBaqcP5QibSGNXU4/Zz01rXeD9Hd1qbluXv4/78hoGqxk1UJns6DdkteyR2F9LirrPoKWamoSEBE2YMEGVlZX67ne/K0neTr8LFizo9nFr167VT37yE7355pvKy8sztS+32622trZu7z948KBiY2O9NTmOZ/evbjNVj6GqngzWEMNQlN1s1f7/+Sh8NRmh+jVm93s0Uue8CRW7mizNfm7aTgbeX9tJc1c073wf9/U1DEbNa6Ay2TVvTG/KHmk1J1H4GbTc/FRcXKy5c+cqLy9PkyZNUnl5uc6fP6958+ZJkubMmaPhw4ertLRUkrRmzRqtWLFC27ZtU25urhoaGiRJycnJSk5O1vnz5/WTn/xEd9xxh4YNG6ZTp05p48aNqqur0z/90z9JkmpqarRv3z7ddtttGjhwoGpqarR48WLde++9SktLs+tYRD67T9RmPkCh+JAFq+kiFGU3e+L57N2Im3kzKOx6j/aXa3z11GfDrhpMs5+vxCHm9mfmiuadfXPseA3tDMtmymRnv6EIm5jOkij9DFoONTNnztTJkye1YsUKNTQ0aPz48aqoqPB2Hj527JhiYy+1aj399NNqb2/XjBkzfLZTUlKilStXKi4uTh9++KGee+45nTp1SoMHD9bEiRO1Z88e3XDDDZI8TUnbt2/XypUr1dbWpquuukqLFy/26TPTb0RakrdDBE61bZqVE0/urOj9grOir+/R/nKRxkC/gu2qwTT7+UrwPyK1i0BXNLeztqOTlbDcXVA0W6ZxpYHLI1lrDo+2DrlR/BmMMQzDX6kdp6WlRampqWpubo7u/jVO5vdLPieyT/iN1Z75PgKZUnXpRB9J071Hot4c02jT3a/gzpDROc/Jf+UGrmG54xNz759Any93h7X99fQ+DtdrGGjklpky3fhT6cDiwOtF8/svkAj7DFo5f3PtJ0SOaPxF05umMyfWttkp3MN4g83Kr2A7azADfb7suqK5FJ7XMFBzidmRW4lDGMkXxZ/BXl37CQiaCJlq2zSHz84ZFk6f88ZKB2C7R/IF+nzZtb9Qv4ZmRoqZHbl1xXA+01H8GaSmBuiraO4MGImcPueN1V/Boa7BtGN/oX4NzQRFKyO3YuP692c6ij+DhBrADtHYdBapornjuBm9+RUc6ibLvu4v1K+h2aBoduSW1L8/01H8GaT5CbBLtDWdRTIHX5sm5NflCpdQvoZmg+KI6dbK1J8/01H6GWT0E4DI5dSRYt5OrZLfX8ERfNKwLBSvoZ0jt+ArAo6VlfM3oQYAwiEapzCIZP0pKPYzhBo/CDUAIk4E/Ap2FIKiIzFPDQBEA+Yssld/7twLSYQaAICTEBT7NUY/AQAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAARyDUAAAAR+g3Mwp3XuKqpaUlzCUBAABmdZ63zVyqst+EmrNnz0qScnJywlwSAABg1dmzZ5WamtrjOv3mKt1ut1uffvqpBg4cqJiYGFOPaWlpUU5Ojo4fP86VvUOI4x4eHPfw4LiHB8c9PHpz3A3D0NmzZ5Wdna3Y2J57zfSbmprY2FiNGDGiV49NSUnhTR8GHPfw4LiHB8c9PDju4WH1uAeqoelER2EAAOAIhBoAAOAIhJoeJCYmqqSkRImJieEuSr/CcQ8Pjnt4cNzDg+MeHsE+7v2mozAAAHA2amoAAIAjEGoAAIAjEGoAAIAjEGoAAIAjEGp6sHHjRuXm5iopKUn5+fnav39/uIvkKL/5zW/0j//4j8rOzlZMTIxeeeUVn/sNw9CKFSs0bNgwfeUrX5HL5dJf/vKX8BTWQUpLSzVx4kQNHDhQQ4cO1Xe/+10dOXLEZ53PP/9cjzzyiAYPHqzk5GTdddddamxsDFOJneHpp5/W2LFjvZOOFRQU6L//+7+993PMg6+srEwxMTFatGiRdxnH3X4rV65UTEyMz2306NHe+4N5zAk13dixY4eKi4tVUlKiAwcOaNy4cSosLFRTU1O4i+YY58+f17hx47Rx40a/969du1ZPPfWUNm3apH379unKK69UYWGhPv/88xCX1Fl2796tRx55RHv37tVbb72lCxcuaOrUqTp//rx3ncWLF+u1117TCy+8oN27d+vTTz9VUVFRGEsd/UaMGKGysjLV1tbq97//vb71rW9p+vTp+uCDDyRxzIPtvffe03/8x39o7NixPss57sFxww03qL6+3nv77W9/670vqMfcgF+TJk0yHnnkEe/fHR0dRnZ2tlFaWhrGUjmXJOPll1/2/u12u42srCzjiSee8C47c+aMkZiYaDz//PNhKKFzNTU1GZKM3bt3G4bhOc4DBgwwXnjhBe86hw8fNiQZNTU14SqmI6WlpRnPPPMMxzzIzp49a/zd3/2d8dZbbxm33HKL8dhjjxmGwXs9WEpKSoxx48b5vS/Yx5yaGj/a29tVW1srl8vlXRYbGyuXy6Wampowlqz/+OSTT9TQ0ODzGqSmpio/P5/XwGbNzc2SpPT0dElSbW2tLly44HPsR48erZEjR3LsbdLR0aHt27fr/PnzKigo4JgH2SOPPKLvfOc7PsdX4r0eTH/5y1+UnZ2tq6++WrNnz9axY8ckBf+Y95sLWlpx6tQpdXR0KDMz02d5ZmamPvzwwzCVqn9paGiQJL+vQed96Du3261FixbpH/7hH/S1r31NkufYJyQkaNCgQT7rcuz77o9//KMKCgr0+eefKzk5WS+//LLGjBmjgwcPcsyDZPv27Tpw4IDee++9LvfxXg+O/Px8/fKXv9R1112n+vp6rVq1SpMnT9af/vSnoB9zQg3Qjz3yyCP605/+5NPejeC57rrrdPDgQTU3N+vFF1/U3LlztXv37nAXy7GOHz+uxx57TG+99ZaSkpLCXZx+49vf/rb3/2PHjlV+fr5GjRql//zP/9RXvvKVoO6b5ic/MjIyFBcX16U3dmNjo7KyssJUqv6l8zjzGgTPggUL9Prrr6uqqkojRozwLs/KylJ7e7vOnDnjsz7Hvu8SEhJ0zTXXaMKECSotLdW4ceP05JNPcsyDpLa2Vk1NTbrxxhsVHx+v+Ph47d69W0899ZTi4+OVmZnJcQ+BQYMG6dprr9VHH30U9Pc6ocaPhIQETZgwQZWVld5lbrdblZWVKigoCGPJ+o+rrrpKWVlZPq9BS0uL9u3bx2vQR4ZhaMGCBXr55Zf1zjvv6KqrrvK5f8KECRowYIDPsT9y5IiOHTvGsbeZ2+1WW1sbxzxIpkyZoj/+8Y86ePCg95aXl6fZs2d7/89xD75z587p448/1rBhw4L/Xu9zV2OH2r59u5GYmGj88pe/NP7nf/7HePDBB41BgwYZDQ0N4S6aY5w9e9Z4//33jffff9+QZKxfv954//33jb/+9a+GYRhGWVmZMWjQIOPVV181Dh06ZEyfPt246qqrjL/97W9hLnl0e/jhh43U1FSjurraqK+v995aW1u96zz00EPGyJEjjXfeecf4/e9/bxQUFBgFBQVhLHX0W7ZsmbF7927jk08+MQ4dOmQsW7bMiImJMXbt2mUYBsc8VC4f/WQYHPdg+OEPf2hUV1cbn3zyifG73/3OcLlcRkZGhtHU1GQYRnCPOaGmBz/72c+MkSNHGgkJCcakSZOMvXv3hrtIjlJVVWVI6nKbO3euYRieYd2PP/64kZmZaSQmJhpTpkwxjhw5Et5CO4C/Yy7J+MUvfuFd529/+5vxgx/8wEhLSzOuuOIK48477zTq6+vDV2gH+N73vmeMGjXKSEhIMIYMGWJMmTLFG2gMg2MeKl8ONRx3+82cOdMYNmyYkZCQYAwfPtyYOXOm8dFHH3nvD+YxjzEMw+h7fQ8AAEB40acGAAA4AqEGAAA4AqEGAAA4AqEGAAA4AqEGAAA4AqEGAAA4AqEGAAA4AqEGAAA4AqEGAAA4AqEGAAA4AqEGAAA4AqEGAAA4wv8Hs1FaoJB4KKMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x,train_loss,c=\"blue\")\n",
    "plt.scatter(x,val_loss,c=\"orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b72925",
   "metadata": {},
   "source": [
    "## Try to normalize only the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2324a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model_norm_ts=training.RV_RNN_conv(n_diff=4,rnn_act=\"tanh\",rnn_drop_out=0,rnn_hidden_size=32,proj_dim=32,rnn_num_layer=1,input_scaler=1).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "68516afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_norm_ts=optim.Adam(RNN_model_norm_ts.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ae9de7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ycoeusz/git/Realized_Volatility_Prediction_-erdos_deeplearning-/NNetwork/../proj_mod/training.py:284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_tab_copy.loc[:,\"sub_int_num\"]=np.nan\n",
      "/home/ycoeusz/git/Realized_Volatility_Prediction_-erdos_deeplearning-/NNetwork/../proj_mod/training.py:284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_tab_copy.loc[:,\"sub_int_num\"]=np.nan\n"
     ]
    }
   ],
   "source": [
    "train_dataset=training.RVdataset(time_id_list=train_time_id,ts_features=[\"sub_int_RV_norm\"],df_ts_feat=df_RV_ts,df_target=df_target)\n",
    "test_dataset=training.RVdataset(time_id_list=test_time_id,ts_features=[\"sub_int_RV_norm\"],df_ts_feat=df_RV_ts,df_target=df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "509b7bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=256,shuffle=True)\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=256,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4056c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=[]\n",
    "val_loss=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b0a66b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At  9.253395318984985  epoch  1 has training loss  tensor(2.9448, device='cuda:0')  and validation loss  tensor(1.1058, device='cuda:0') .\n",
      "\n",
      "At  48.75172233581543  epoch  5 has training loss  tensor(2.8769, device='cuda:0')  and validation loss  tensor(2.5862, device='cuda:0') .\n",
      "\n",
      "At  97.71213459968567  epoch  10 has training loss  tensor(2.9242, device='cuda:0')  and validation loss  tensor(0.8594, device='cuda:0') .\n",
      "\n",
      "At  147.71976971626282  epoch  15 has training loss  tensor(2.8378, device='cuda:0')  and validation loss  tensor(1.9252, device='cuda:0') .\n",
      "\n",
      "At  196.09314608573914  epoch  20 has training loss  tensor(3.2858, device='cuda:0')  and validation loss  tensor(2.6849, device='cuda:0') .\n",
      "\n",
      "At  245.30493760108948  epoch  25 has training loss  tensor(3.2313, device='cuda:0')  and validation loss  tensor(0.7104, device='cuda:0') .\n",
      "\n",
      "The validation loss has not improved for  20  epochs. Stopping current training loop.\n",
      "\n",
      "Best model state dictionary of this training loop is reloaded.\n",
      "\n",
      "According to validation loss, the best model is reached at epoch 6  with validation loss:  tensor(0.4880, device='cuda:0') .\n",
      " The total number of epoch trained is  26 .\n",
      " Training completed in:  255.02213788032532 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('frozen_conv.frozen_conv.weight',\n",
       "              tensor([[[-1.,  1.]]], device='cuda:0')),\n",
       "             ('linear_proj_input.weight',\n",
       "              tensor([[ 1.0828e-01, -3.5643e-04, -4.8510e-02,  9.2966e-02,  1.7199e-01],\n",
       "                      [-1.3384e-04,  1.0969e-02,  2.6824e-01,  1.9068e-01,  3.3120e-03],\n",
       "                      [-1.6602e-01, -2.0993e-01,  6.0612e-02, -2.7175e-01, -1.6795e-01],\n",
       "                      [ 4.7048e-03, -5.7992e-02, -1.0478e-01, -8.5249e-02, -2.2236e-02],\n",
       "                      [-8.8054e-02,  4.5599e-01,  2.2072e-01, -4.8502e-01, -2.7809e-01],\n",
       "                      [ 1.0922e-01,  3.7854e-01, -4.7694e-02, -2.2077e-01, -1.8524e-01],\n",
       "                      [ 1.0693e-03, -5.5686e-01, -4.1609e-01,  1.7900e-02,  9.7641e-02],\n",
       "                      [ 1.9254e-03,  6.2266e-03, -1.6594e-01, -1.0216e-01,  3.2037e-03],\n",
       "                      [-2.9815e-01, -6.7881e-02,  3.8021e-02,  5.3011e-02,  2.3592e-01],\n",
       "                      [-1.4233e-01, -2.9872e-01,  5.9946e-03, -1.4386e-01,  6.6525e-02],\n",
       "                      [-2.9700e-01, -5.0067e-01, -2.4018e-01, -4.1914e-02,  3.3405e-01],\n",
       "                      [-3.4465e-03, -2.2489e-01, -4.1697e-01, -2.6115e-01, -3.9768e-02],\n",
       "                      [-1.3729e-02, -1.5358e-01, -4.9255e-01, -5.2888e-01, -1.9144e-01],\n",
       "                      [ 4.3853e-01,  5.4283e-01,  8.3261e-03, -2.5082e-01, -6.0635e-02],\n",
       "                      [-6.2886e-02,  1.3226e-01,  6.5470e-02, -4.4273e-01, -2.1636e-01],\n",
       "                      [-9.0540e-04,  9.2119e-03,  2.0281e-01,  2.5753e-01,  1.0481e-01],\n",
       "                      [ 1.8050e-01,  2.9001e-01,  1.0602e-01, -1.0164e-01, -1.3181e-01],\n",
       "                      [ 8.5242e-02, -1.5437e-01, -1.1280e-01, -1.5506e-01,  2.2094e-01],\n",
       "                      [-7.7641e-03, -2.8459e-01, -2.0669e-01, -6.2823e-02, -3.8141e-02],\n",
       "                      [-4.4102e-03, -5.3832e-02,  2.5127e-02,  4.3397e-02,  3.9043e-03],\n",
       "                      [ 5.0657e-03, -5.3100e-02,  1.4537e-04, -5.2692e-03,  3.7573e-02],\n",
       "                      [-5.6780e-03, -3.9278e-01, -1.2096e-01,  1.9784e-01,  1.0237e-01],\n",
       "                      [-3.5104e-02, -2.9125e-01, -3.3381e-02, -7.8289e-02, -5.2811e-02],\n",
       "                      [-3.9499e-02, -5.1352e-01, -5.7276e-02,  8.0790e-02, -1.8182e-02],\n",
       "                      [-1.9430e-01, -4.4458e-01,  7.1440e-02, -2.0738e-01, -2.1539e-01],\n",
       "                      [ 3.4442e-03,  4.2819e-01,  8.3298e-02, -1.8168e-01, -8.2070e-02],\n",
       "                      [ 2.4077e-03,  4.8407e-01,  1.1395e-01, -2.2401e-01, -1.1370e-01],\n",
       "                      [-4.6681e-03, -1.7516e-01, -1.5814e-01, -5.3554e-02, -8.5161e-03],\n",
       "                      [ 5.1981e-03,  2.8649e-01,  3.6626e-01,  1.1786e-01,  1.5095e-02],\n",
       "                      [ 1.5868e-01,  1.6812e-02, -4.5765e-01, -1.0884e-01, -3.0203e-02],\n",
       "                      [-3.3853e-01, -5.4049e-01, -1.0255e-01,  7.1259e-02,  2.2008e-01],\n",
       "                      [ 2.0000e-01,  4.0571e-01,  5.7446e-02,  1.8149e-01,  1.8485e-01]],\n",
       "                     device='cuda:0')),\n",
       "             ('linear_proj_input.bias',\n",
       "              tensor([-1.4999e-01, -4.2085e-05, -7.3953e-02,  1.8184e-03, -3.9400e-02,\n",
       "                       4.8158e-02,  2.4112e-04,  9.4833e-04, -1.8352e-01,  1.1432e-01,\n",
       "                      -1.3211e-01, -1.9908e-03, -7.2845e-03,  1.9407e-01, -2.8721e-02,\n",
       "                       7.6053e-05,  7.9066e-02,  3.6022e-02, -3.8123e-03, -1.8440e-03,\n",
       "                       2.2607e-03, -2.3105e-03, -1.6193e-02, -1.7982e-02, -8.7191e-02,\n",
       "                       1.5964e-03,  1.0399e-03, -2.3062e-03,  2.5702e-03,  6.9927e-02,\n",
       "                      -1.4969e-01,  1.0255e-01], device='cuda:0')),\n",
       "             ('RNN_layer.weight_ih_l0',\n",
       "              tensor([[-0.1216,  0.0026,  0.0191,  ..., -0.0087,  0.0567,  0.0403],\n",
       "                      [-0.1231,  0.0165,  0.0252,  ...,  0.0037,  0.0710,  0.1414],\n",
       "                      [ 0.0245,  0.0141, -0.0240,  ...,  0.0259, -0.0260, -0.0655],\n",
       "                      ...,\n",
       "                      [ 0.0551, -0.0033,  0.0025,  ..., -0.0126,  0.0484, -0.0123],\n",
       "                      [ 0.2100,  0.0132, -0.0019,  ...,  0.0072, -0.0084, -0.0070],\n",
       "                      [ 0.2177,  0.0015, -0.0149,  ...,  0.0164, -0.0620, -0.1351]],\n",
       "                     device='cuda:0')),\n",
       "             ('RNN_layer.weight_hh_l0',\n",
       "              tensor([[-7.4444e-01,  9.0473e-02, -1.1714e-01,  ...,  1.4301e-01,\n",
       "                        1.1834e-02,  1.3044e-01],\n",
       "                      [-6.0294e-02, -7.9138e-01,  5.3564e-02,  ..., -1.5334e-02,\n",
       "                       -9.6562e-03,  2.8242e-02],\n",
       "                      [ 1.6380e-01, -1.1649e-02, -7.7769e-01,  ..., -2.0777e-02,\n",
       "                       -4.5462e-04, -1.4975e-01],\n",
       "                      ...,\n",
       "                      [-1.8268e-01, -6.9647e-02,  2.1524e-02,  ..., -8.4417e-01,\n",
       "                       -1.3384e-01, -2.1724e-01],\n",
       "                      [-3.1176e-03,  4.4034e-02,  6.9508e-02,  ...,  1.4280e-01,\n",
       "                       -8.5487e-01, -8.8734e-02],\n",
       "                      [-3.3530e-02, -7.2675e-02,  2.4765e-02,  ...,  9.9962e-02,\n",
       "                        1.0483e-01, -9.9307e-01]], device='cuda:0')),\n",
       "             ('RNN_layer.bias_ih_l0',\n",
       "              tensor([-0.0899, -0.0463, -0.0584,  0.0558,  0.0659,  0.0159,  0.1084, -0.0404,\n",
       "                      -0.0505, -0.0219,  0.0539,  0.1193,  0.0278, -0.0552, -0.0019, -0.0921,\n",
       "                      -0.1327,  0.0262, -0.0470, -0.1105, -0.0086, -0.0594,  0.1121,  0.0594,\n",
       "                       0.0142, -0.0954, -0.0119, -0.0358,  0.0706,  0.0112,  0.0892,  0.0537],\n",
       "                     device='cuda:0')),\n",
       "             ('RNN_layer.bias_hh_l0',\n",
       "              tensor([ 0.0533,  0.0236,  0.0745, -0.0811, -0.0407, -0.0587, -0.0686,  0.0554,\n",
       "                       0.0554, -0.0397, -0.0644, -0.1290, -0.0159,  0.0539,  0.0060,  0.0949,\n",
       "                       0.1238, -0.0512,  0.1193,  0.0833, -0.0306,  0.0755, -0.1349, -0.0023,\n",
       "                      -0.0491,  0.0795,  0.0704, -0.0302, -0.0799, -0.0213, -0.0152,  0.0385],\n",
       "                     device='cuda:0')),\n",
       "             ('linear_post_rnn.weight',\n",
       "              tensor([[-2.1224e-04,  3.0841e-04, -1.9468e-04,  3.7841e-05,  2.3765e-04,\n",
       "                        9.3004e-05, -1.4856e-04, -8.5148e-05, -1.8218e-04, -4.4454e-05,\n",
       "                       -1.8165e-04,  1.5100e-04, -1.2195e-04, -1.7133e-05, -2.8369e-04,\n",
       "                       -1.0049e-04, -1.7237e-04,  2.8462e-04, -1.9050e-04,  4.9696e-05,\n",
       "                        2.8130e-04,  1.0488e-05,  3.8955e-05, -1.6240e-04,  9.5487e-06,\n",
       "                        9.8356e-05, -1.6425e-04,  1.1947e-04,  2.0782e-04, -1.0654e-04,\n",
       "                       -2.9825e-04,  2.5578e-03]], device='cuda:0')),\n",
       "             ('linear_post_rnn.bias', tensor([-6.6371e-06], device='cuda:0'))])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.reg_training_loop_rmspe(optimizer=optimizer_norm_ts,model=RNN_model_norm_ts,train_loader=train_loader,val_loader=test_loader,list_train_loss=train_loss,list_val_loss=val_loss,device=device,n_epochs=200,ot_steps=20,report_interval=5,eps=0,scaler=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d7684d",
   "metadata": {},
   "source": [
    "As a remark, I ran above loop twice, the keep learning to same model after the fact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ef1351bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozen_conv.frozen_conv.weight False\n",
      "linear_proj_input.weight True\n",
      "linear_proj_input.bias True\n",
      "RNN_layer.weight_ih_l0 True\n",
      "RNN_layer.weight_hh_l0 True\n",
      "RNN_layer.bias_ih_l0 True\n",
      "RNN_layer.bias_hh_l0 True\n",
      "linear_post_rnn.weight True\n",
      "linear_post_rnn.bias True\n"
     ]
    }
   ],
   "source": [
    "for name,param in RNN_model_norm_ts.named_parameters():\n",
    "    print(name,param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2bb082d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=[x.cpu() for x in train_loss]\n",
    "val_loss=[x.cpu() for x in val_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1802af02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b022072c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f5b77f26390>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "x=np.linspace(1,57,57)\n",
    "\n",
    "plt.scatter(x,train_loss,c=\"blue\")\n",
    "plt.scatter(x,val_loss,c=\"orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb605a91",
   "metadata": {},
   "source": [
    "It appears that only normalizing input ts is not a good idea. I should consider normalizing both the input and the target. Plus, in above, I fit transformed on both the training and the test set together, that is technically a data leak. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef78913",
   "metadata": {},
   "source": [
    "## Training with normalization on both input and target "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64325299",
   "metadata": {},
   "source": [
    "I have added normalization functionality to dataset creation function, so not we can create the train and test datasets without dataleaking. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f44fef1",
   "metadata": {},
   "source": [
    "I think setting eps to zero will still work, I do not see the possibility of a zero RV. Too tired today, will try tomorrow or something. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_3_11_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
