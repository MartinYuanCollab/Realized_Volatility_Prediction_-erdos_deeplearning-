{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77e9ebe",
   "metadata": {},
   "source": [
    "## RNN with frozen convolution layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b854e",
   "metadata": {},
   "source": [
    "The idea is inspired by an application of RNN in predicting if an online review (say, for a product) is positive or negative (1 or 0). Each word in the review sentence is first projected to a large dimension vector space and then sent into an RNN network sequentially. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f6b66b",
   "metadata": {},
   "source": [
    "We will make use of frozen convolution layer to create derivative like features for our time series features. Then, we use the derivative values of a certain time as if a \"word\" in a \"review\", the target RV as if the \"score\" of the \"review\" to train an RNN network that predicts the target RV with the time series features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd88f7",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "907e47ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, importlib\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from proj_mod import training, data_processing\n",
    "importlib.reload(training);\n",
    "importlib.reload(data_processing);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12efccd",
   "metadata": {},
   "source": [
    "Below is what Yuan needed to get his gpu working, do not run if you do not need it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd83d5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../dotenv_env/deep_learning.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd9ea17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.3.0\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get(\"HSA_OVERRIDE_GFX_VERSION\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb26cd",
   "metadata": {},
   "source": [
    "Let's say, we have following timeseries feature (created randomly), we will create the derivative features first. As a reminder, the zero dimension is the batch size, the dimension one is channel (needed for the first convolution layer, not really a \"thing\" for time series like features). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb3d0064",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_feature=torch.randn(1,1,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b28d5ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8983,  0.5294,  1.4815,  0.3880,  0.3053, -1.5859,  0.0210,\n",
       "          -0.6448, -1.3457, -1.2188, -0.0443, -1.0198, -0.6600, -0.2874,\n",
       "          -0.0385, -1.2630, -0.1093, -2.1785, -1.1292, -0.4585,  1.7239,\n",
       "           0.9651,  0.1646, -0.7025,  0.6864,  1.0771, -0.9391,  0.5244,\n",
       "           0.2303, -0.5098,  0.1424, -1.3571, -0.7195, -0.5237, -0.0643,\n",
       "           0.4805, -0.4966, -0.7618,  0.3211,  0.0881, -1.3531, -1.1748,\n",
       "          -2.0773, -0.4347,  0.3090,  0.9424,  0.8958,  1.6446,  1.7279,\n",
       "           0.7207, -0.0487, -0.8066, -0.2446,  0.1778, -0.6278, -2.7156,\n",
       "           0.2626,  1.2195, -0.9886, -1.0032]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "739b2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_create_diff=training.frozen_diff_conv(n_diff=4)\n",
    "ts_diff_feature=conv_create_diff(ts_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48f9ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-8.9833e-01,  5.2935e-01,  1.4815e+00,  3.8805e-01,  3.0529e-01,\n",
       "          -1.5859e+00,  2.1027e-02, -6.4484e-01, -1.3457e+00, -1.2188e+00,\n",
       "          -4.4305e-02, -1.0198e+00, -6.5998e-01, -2.8743e-01, -3.8456e-02,\n",
       "          -1.2630e+00, -1.0929e-01, -2.1785e+00, -1.1292e+00, -4.5855e-01,\n",
       "           1.7239e+00,  9.6507e-01,  1.6458e-01, -7.0247e-01,  6.8645e-01,\n",
       "           1.0771e+00, -9.3911e-01,  5.2441e-01,  2.3027e-01, -5.0984e-01,\n",
       "           1.4243e-01, -1.3571e+00, -7.1949e-01, -5.2366e-01, -6.4339e-02,\n",
       "           4.8050e-01, -4.9660e-01, -7.6183e-01,  3.2113e-01,  8.8130e-02,\n",
       "          -1.3531e+00, -1.1748e+00, -2.0773e+00, -4.3466e-01,  3.0903e-01,\n",
       "           9.4244e-01,  8.9581e-01,  1.6446e+00,  1.7279e+00,  7.2071e-01,\n",
       "          -4.8721e-02, -8.0657e-01, -2.4458e-01,  1.7781e-01, -6.2775e-01,\n",
       "          -2.7156e+00,  2.6262e-01,  1.2195e+00, -9.8861e-01, -1.0032e+00],\n",
       "         [ 1.4277e+00,  9.5216e-01, -1.0935e+00, -8.2756e-02, -1.8912e+00,\n",
       "           1.6069e+00, -6.6587e-01, -7.0087e-01,  1.2694e-01,  1.1745e+00,\n",
       "          -9.7550e-01,  3.5983e-01,  3.7255e-01,  2.4898e-01, -1.2246e+00,\n",
       "           1.1537e+00, -2.0692e+00,  1.0493e+00,  6.7064e-01,  2.1825e+00,\n",
       "          -7.5884e-01, -8.0049e-01, -8.6705e-01,  1.3889e+00,  3.9068e-01,\n",
       "          -2.0162e+00,  1.4635e+00, -2.9414e-01, -7.4011e-01,  6.5227e-01,\n",
       "          -1.4996e+00,  6.3764e-01,  1.9583e-01,  4.5932e-01,  5.4484e-01,\n",
       "          -9.7711e-01, -2.6522e-01,  1.0830e+00, -2.3300e-01, -1.4412e+00,\n",
       "           1.7825e-01, -9.0244e-01,  1.6426e+00,  7.4369e-01,  6.3342e-01,\n",
       "          -4.6635e-02,  7.4877e-01,  8.3366e-02, -1.0072e+00, -7.6943e-01,\n",
       "          -7.5785e-01,  5.6199e-01,  4.2239e-01, -8.0557e-01, -2.0878e+00,\n",
       "           2.9782e+00,  9.5686e-01, -2.2081e+00, -1.4631e-02,  0.0000e+00],\n",
       "         [-4.7552e-01, -2.0456e+00,  1.0107e+00, -1.8085e+00,  3.4982e+00,\n",
       "          -2.2728e+00, -3.5007e-02,  8.2781e-01,  1.0475e+00, -2.1500e+00,\n",
       "           1.3353e+00,  1.2722e-02, -1.2357e-01, -1.4736e+00,  2.3783e+00,\n",
       "          -3.2230e+00,  3.1185e+00, -3.7867e-01,  1.5118e+00, -2.9413e+00,\n",
       "          -4.1642e-02, -6.6564e-02,  2.2560e+00, -9.9824e-01, -2.4069e+00,\n",
       "           3.4797e+00, -1.7577e+00, -4.4597e-01,  1.3924e+00, -2.1518e+00,\n",
       "           2.1372e+00, -4.4180e-01,  2.6349e-01,  8.5520e-02, -1.5220e+00,\n",
       "           7.1189e-01,  1.3482e+00, -1.3160e+00, -1.2082e+00,  1.6195e+00,\n",
       "          -1.0807e+00,  2.5451e+00, -8.9893e-01, -1.1027e-01, -6.8005e-01,\n",
       "           7.9541e-01, -6.6541e-01, -1.0906e+00,  2.3782e-01,  1.1577e-02,\n",
       "           1.3198e+00, -1.3960e-01, -1.2280e+00, -1.2823e+00,  5.0661e+00,\n",
       "          -2.0214e+00, -3.1649e+00,  2.1935e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [-1.5701e+00,  3.0563e+00, -2.8192e+00,  5.3066e+00, -5.7710e+00,\n",
       "           2.2378e+00,  8.6282e-01,  2.1971e-01, -3.1975e+00,  3.4853e+00,\n",
       "          -1.3226e+00, -1.3629e-01, -1.3500e+00,  3.8519e+00, -5.6013e+00,\n",
       "           6.3415e+00, -3.4972e+00,  1.8905e+00, -4.4531e+00,  2.8997e+00,\n",
       "          -2.4921e-02,  2.3225e+00, -3.2542e+00, -1.4087e+00,  5.8867e+00,\n",
       "          -5.2374e+00,  1.3117e+00,  1.8383e+00, -3.5442e+00,  4.2890e+00,\n",
       "          -2.5790e+00,  7.0530e-01, -1.7797e-01, -1.6075e+00,  2.2338e+00,\n",
       "           6.3629e-01, -2.6641e+00,  1.0774e-01,  2.8277e+00, -2.7002e+00,\n",
       "           3.6258e+00, -3.4440e+00,  7.8866e-01, -5.6978e-01,  1.4755e+00,\n",
       "          -1.4608e+00, -4.2520e-01,  1.3284e+00, -2.2624e-01,  1.3083e+00,\n",
       "          -1.4594e+00, -1.0884e+00, -5.4327e-02,  6.3484e+00, -7.0874e+00,\n",
       "          -1.1436e+00,  5.3584e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 4.6265e+00, -5.8755e+00,  8.1258e+00, -1.1078e+01,  8.0088e+00,\n",
       "          -1.3750e+00, -6.4310e-01, -3.4172e+00,  6.6828e+00, -4.8079e+00,\n",
       "           1.1863e+00, -1.2137e+00,  5.2019e+00, -9.4532e+00,  1.1943e+01,\n",
       "          -9.8387e+00,  5.3877e+00, -6.3436e+00,  7.3528e+00, -2.9246e+00,\n",
       "           2.3475e+00, -5.5767e+00,  1.8455e+00,  7.2953e+00, -1.1124e+01,\n",
       "           6.5491e+00,  5.2667e-01, -5.3825e+00,  7.8332e+00, -6.8680e+00,\n",
       "           3.2843e+00, -8.8327e-01, -1.4295e+00,  3.8413e+00, -1.5976e+00,\n",
       "          -3.3004e+00,  2.7719e+00,  2.7199e+00, -5.5278e+00,  6.3259e+00,\n",
       "          -7.0698e+00,  4.2327e+00, -1.3584e+00,  2.0452e+00, -2.9363e+00,\n",
       "           1.0356e+00,  1.7536e+00, -1.5547e+00,  1.5345e+00, -2.7677e+00,\n",
       "           3.7109e-01,  1.0340e+00,  6.4027e+00, -1.3436e+01,  5.9439e+00,\n",
       "           6.5020e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_diff_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79075913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 60])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_diff_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3354b4",
   "metadata": {},
   "source": [
    "We now permute the last two dimensions, essentially, this is just a transposition of the last two dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76682d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_diff_feature=ts_diff_feature.permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ed4b060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 60, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_diff_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cff150",
   "metadata": {},
   "source": [
    "Now, this is a batch (of size 1) of timeseries feature with 60 time steps, and 5 features in each step. We then expand the 5 features, say, to 32 by projection. As a reminder, nn.Linear automatically apply to the last dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b7d96c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_proj=nn.Linear(5,32)\n",
    "ts_feature_proj=linear_proj(ts_diff_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33f82b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 60, 32])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_feature_proj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e40c151",
   "metadata": {},
   "source": [
    "In this example, we will not go into too much detail, so let's make the most simple version. We then pass through a layer of RNN, then a layer of linear (to project to dimension 1 again). Learn more about RNN at https://docs.pytorch.org/docs/stable/generated/torch.nn.RNN.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f611ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_layer=nn.RNN(input_size=32,hidden_size=32,num_layers=1,nonlinearity=\"tanh\",batch_first=True,dropout=0)\n",
    "linear_end=nn.Linear(32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "207568f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_out=RNN_layer(ts_feature_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc91386",
   "metadata": {},
   "source": [
    "As a reminder, since no training is done, the RNN basically just applies the initial weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f31dfe8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 60, 32])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNN_out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e423255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_proj=linear_end(RNN_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ba4ee6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 60, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_proj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22784470",
   "metadata": {},
   "source": [
    "We will use sum, but we may change this to other functions, according to context or just feeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e293bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_out=torch.sum(out_proj,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b35c7bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.8542]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9f52c",
   "metadata": {},
   "source": [
    "## Creating the actual NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d007bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created 07/02/25\n",
    "#07/02/25: Moved to training.py \n",
    "class RV_RNN_conv(nn.Module):\n",
    "    \"\"\"\n",
    "    :param n_diff: Decides how many derivative features is wanted in the time series. \n",
    "    :param rnn_num_layer: num_layer parameter for rnn. \n",
    "    :param rnn_drop_out: dropout parameter for rnn. \n",
    "    :param rnn_act: Defaulted to \"tanh\". Nonlinearity parameter for rnn. \n",
    "    :param proj_dim: Defaulted to 32. Decided the dimension of projection before feeding into rnn. \n",
    "    :param rnn_hidden_size: Defaulted to 32. The hidden_size parameter for rnn. \n",
    "    \"\"\"\n",
    "    def __init__(self,n_diff,rnn_num_layer,rnn_drop_out,rnn_act=\"tanh\",proj_dim=32,rnn_hidden_size=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.frozen_conv=training.frozen_diff_conv(n_diff=n_diff)\n",
    "        self.linear_proj_input=nn.Linear(n_diff+1,proj_dim)\n",
    "        self.RNN_layer=nn.RNN(input_size=proj_dim,hidden_size=rnn_hidden_size,num_layers=rnn_num_layer,nonlinearity=rnn_act,batch_first=True,dropout=rnn_drop_out)\n",
    "        self.linear_post_rnn=nn.Linear(rnn_hidden_size,1)\n",
    "        self.frozen_list=[\"frozen_conv\"] \n",
    "        \n",
    "    def forward(self,x):\n",
    "        #First, unsqueese to add in one dimension in dim 1 as channel. This is needed for convolution. \n",
    "        x=torch.unsqueeze(x,dim=1)\n",
    "        x=self.frozen_conv(x)\n",
    "        x=x.permute(0,2,1)\n",
    "        x=self.linear_proj_input(x)\n",
    "        x=self.RNN_layer(x)[0]\n",
    "        x=self.linear_post_rnn(x)\n",
    "        \n",
    "        return torch.sum(x,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71432b31",
   "metadata": {},
   "source": [
    "## Basic testing on the RNN with frozen convolution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9583750",
   "metadata": {},
   "source": [
    "Test it first. As a reminder, the expected input dimensions are (Batch Size, Time Series Length), this is distinct from the example above in that the channel dimension is not present. The channel dimension is added with unsqueeze at dimension 1 so that the first convolution layer can be utilized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df3d8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_feature=torch.randn(10,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a33fd643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.0699e-01,  7.1855e-01,  1.0534e+00, -2.7195e-02, -5.3953e-01,\n",
       "         -2.1916e-01,  2.6058e-01,  9.4682e-01, -5.7270e-01,  6.1821e-01,\n",
       "          7.4706e-01,  8.5857e-01, -6.3387e-01,  1.0074e-01,  1.0871e+00,\n",
       "          4.4188e-01,  6.0410e-01, -3.4202e-02, -1.5008e-01,  8.9701e-01,\n",
       "         -6.9344e-01,  1.7957e+00, -5.8575e-01, -2.4478e+00,  7.0324e-01,\n",
       "         -1.2766e+00, -3.7510e-01, -3.5719e-01, -1.3143e+00,  6.8766e-01,\n",
       "         -2.5387e-01, -1.0662e+00,  5.9732e-01,  1.1774e+00,  1.7849e-01,\n",
       "         -7.2704e-02, -6.2228e-01, -9.0305e-01, -7.6060e-01,  2.4960e+00,\n",
       "         -3.7462e-01, -5.7164e-01,  7.3972e-02, -1.5861e+00, -7.6500e-01,\n",
       "          8.6943e-01,  4.5075e-01,  1.3651e-02,  2.1515e-01, -1.4589e+00,\n",
       "          1.1666e+00, -2.4947e-01, -6.6102e-01,  7.9651e-01,  7.1330e-01,\n",
       "         -5.8549e-01,  8.9783e-01, -5.2981e-01, -1.6684e-01, -5.2264e-01],\n",
       "        [ 6.6567e-01,  1.5717e-01, -8.9532e-01,  9.2971e-01, -6.0536e-01,\n",
       "          5.8591e-02, -5.4184e-01,  3.6571e-01, -4.7071e-01,  9.3300e-01,\n",
       "         -5.3932e-01, -8.8753e-01,  1.7115e+00,  2.2261e-02,  1.0817e+00,\n",
       "         -5.2810e-01,  4.6068e-01, -4.0759e-01, -2.1657e+00, -8.6611e-01,\n",
       "          1.1546e+00,  3.8663e-01, -3.0246e-01,  1.7155e-01,  1.5774e-01,\n",
       "          9.7471e-01, -1.0136e+00, -1.1826e+00, -1.0026e+00,  1.4194e-01,\n",
       "          3.0013e-01,  7.5464e-01,  1.3789e+00,  3.0984e-01,  9.0080e-01,\n",
       "         -2.6490e-01, -5.7665e-01, -1.6571e+00, -3.0413e-01,  1.2294e+00,\n",
       "          8.7073e-02, -1.5744e-01, -4.1935e-01, -4.0364e-01, -5.4075e-01,\n",
       "          4.4449e-01,  6.8356e-01,  2.0351e+00,  7.3060e-01,  3.7839e-01,\n",
       "          1.5160e+00,  8.3184e-01, -1.0474e+00,  1.3620e+00,  1.1588e-01,\n",
       "          1.5253e+00,  1.7736e-01,  8.9396e-01,  1.3331e+00, -1.1479e+00],\n",
       "        [-3.1383e-01,  1.7709e+00,  2.6152e-01,  4.4231e-01,  1.0152e+00,\n",
       "          2.5837e-01, -1.1688e+00, -5.6972e-01, -2.7012e-01, -3.6764e-01,\n",
       "          6.9320e-01,  6.5906e-01,  2.7373e-01, -4.1007e-01,  1.2787e+00,\n",
       "          3.7425e-01,  1.8299e-01,  1.2031e+00,  8.7180e-02, -2.2043e-01,\n",
       "          2.1431e-01,  8.1473e-01, -3.8800e-01,  8.1544e-01,  1.5113e+00,\n",
       "          8.5249e-01, -1.0512e+00, -1.5145e-01, -4.3640e-01,  6.4185e-01,\n",
       "         -3.6349e-01, -3.4011e-01, -1.6253e-02, -1.5880e+00, -1.5473e+00,\n",
       "         -1.2960e+00, -1.0107e+00,  2.2210e-02, -9.6097e-01, -1.9353e+00,\n",
       "         -1.3860e-01, -2.3329e-01, -8.0765e-01, -1.2327e-01, -1.0109e-01,\n",
       "         -2.0385e+00,  5.9140e-01, -6.8946e-01,  7.7740e-01, -1.5059e+00,\n",
       "         -1.4401e+00, -1.8734e+00, -5.5187e-01,  8.5734e-01,  8.0007e-01,\n",
       "         -7.4051e-02, -1.4025e+00,  9.7038e-01,  8.4608e-01, -7.2799e-01],\n",
       "        [-6.1188e-01, -3.7782e-01, -6.9515e-01,  1.7807e+00, -6.2075e-01,\n",
       "          3.7258e-01, -1.6661e+00,  3.5429e-01,  9.0524e-01, -1.5668e+00,\n",
       "         -1.0589e+00,  2.7080e-01, -1.2181e-01,  6.8743e-01,  6.4531e-01,\n",
       "          8.4906e-01,  3.5709e-03, -1.8106e+00, -4.3988e-03,  1.1574e+00,\n",
       "         -1.3036e+00,  9.2590e-01,  3.3343e-01,  4.5248e-02, -1.3245e+00,\n",
       "          3.3481e-01,  1.8628e+00, -4.5933e-01, -1.9357e+00, -1.3864e+00,\n",
       "         -2.6935e-01,  1.3430e+00,  4.7692e-01, -4.2193e-01, -6.1142e-01,\n",
       "         -5.6804e-01,  1.9160e+00,  7.5138e-01, -2.8756e-01, -5.6903e-01,\n",
       "          4.2587e-01,  2.6432e-01,  1.2855e+00,  4.6892e-01,  4.5061e-01,\n",
       "          7.0222e-02, -1.3397e+00,  2.4306e-02, -1.4951e+00, -1.2806e+00,\n",
       "         -2.0086e-01, -1.3639e+00,  2.8384e+00, -4.9187e-01,  1.3123e+00,\n",
       "          1.6335e+00, -2.0656e-01, -5.9957e-01, -1.3313e+00, -4.8102e-01],\n",
       "        [ 9.7902e-02, -4.8628e-02, -1.5149e+00,  1.8632e+00,  1.4713e-01,\n",
       "          6.5375e-01,  6.5687e-01, -6.3389e-01,  2.6316e+00,  4.8502e-01,\n",
       "          5.6849e-01, -1.4237e+00, -1.0267e+00,  2.3458e-01, -1.1950e+00,\n",
       "         -4.8989e-01,  7.3510e-01,  6.7091e-01,  2.2866e-01, -5.1608e-01,\n",
       "         -5.1772e-01, -4.2767e-01,  1.0707e+00,  3.4465e-01, -8.9433e-02,\n",
       "          1.0571e+00, -2.1829e-01, -1.2362e+00, -1.5092e+00, -2.7615e-01,\n",
       "         -2.8056e-01, -1.4141e+00, -1.8467e+00, -1.0360e-01,  1.3964e+00,\n",
       "          8.5886e-01, -3.9587e-01,  7.5805e-01,  3.8120e-01, -9.9700e-01,\n",
       "         -5.1492e-01, -1.6261e+00,  4.3083e-01, -7.8798e-01,  1.1481e+00,\n",
       "          1.3981e-01,  4.1850e-01,  8.0180e-01,  1.5969e+00,  2.1907e-01,\n",
       "          1.2043e-01, -9.7177e-01, -1.2572e-01,  2.0148e-01, -6.7343e-01,\n",
       "          7.4242e-01, -1.4649e+00,  1.7106e-01, -2.2052e-02,  3.7862e-01],\n",
       "        [-7.6046e-01,  7.7445e-01,  1.0674e+00,  6.0515e-01,  5.6979e-02,\n",
       "         -2.0358e-01,  1.3249e+00, -9.6556e-01, -1.2466e+00, -4.7124e-05,\n",
       "         -5.4907e-01,  1.6015e+00, -1.1331e+00, -3.4846e-01,  6.5020e-01,\n",
       "          1.8926e+00,  9.8035e-01,  5.4590e-02,  1.5252e+00,  3.9066e-01,\n",
       "          1.1330e+00, -4.0262e-01,  8.3597e-01, -1.9556e+00, -8.3535e-01,\n",
       "          1.5365e+00,  8.4672e-01,  7.6476e-01,  1.5271e+00, -3.1132e-01,\n",
       "         -5.8377e-01, -2.7592e-01,  6.5311e-01,  2.3342e-01,  3.5443e-01,\n",
       "         -1.5415e-01,  1.1575e+00,  6.4139e-01,  4.2455e-01, -2.2738e+00,\n",
       "         -1.5026e+00, -2.3264e-01,  1.0121e+00, -1.6208e+00, -1.1724e+00,\n",
       "          8.3668e-01,  8.8240e-01,  1.2998e+00, -2.3566e-01, -1.8489e+00,\n",
       "         -5.8500e-01,  5.0238e-01, -9.1927e-01, -6.3203e-01,  3.8581e-01,\n",
       "          7.2006e-01,  5.2369e-03, -1.7417e+00,  1.3047e+00,  7.0037e-01],\n",
       "        [-7.1683e-01,  1.5823e-01, -6.2349e-01,  2.1356e-01, -1.0327e+00,\n",
       "         -9.6736e-01,  8.5647e-01, -3.0849e-01,  2.1342e-01, -1.0970e+00,\n",
       "         -1.2257e-01, -4.1890e-01, -1.3747e+00, -1.0818e+00, -2.5439e-01,\n",
       "         -8.9761e-01, -5.9607e-01,  6.8220e-01, -7.3358e-01,  2.1102e-01,\n",
       "          5.0198e-01,  7.1287e-01,  4.7946e-01, -8.9818e-01, -7.8512e-01,\n",
       "          4.6087e-01,  2.3871e-01,  8.8274e-01, -3.7661e-01,  7.7403e-01,\n",
       "          7.0285e-01,  7.3269e-01, -1.1015e+00, -1.0830e-01,  2.0790e-01,\n",
       "         -1.2537e+00,  1.4871e+00, -2.9796e+00, -7.9944e-02, -6.0269e-01,\n",
       "         -3.0241e-01, -2.4495e+00,  1.4325e+00, -1.3013e+00, -1.4146e+00,\n",
       "          9.6618e-01, -6.8042e-01,  1.5329e-01,  5.5032e-02,  1.1112e+00,\n",
       "          1.5633e+00, -5.1884e-01,  1.1944e+00, -9.0681e-01, -4.9503e-01,\n",
       "          4.2033e-01, -8.4554e-01, -2.5663e-01,  1.8645e+00,  8.6548e-01],\n",
       "        [-1.0031e+00,  1.0106e+00,  9.0124e-01, -6.1037e-02,  1.2433e+00,\n",
       "         -2.8391e-01, -1.2603e+00, -1.1845e-01, -3.6038e-01,  1.2522e+00,\n",
       "          2.5964e+00,  2.7716e-01,  1.1813e+00,  2.4513e-02,  4.4231e-01,\n",
       "          9.9420e-01, -5.9467e-01,  7.0462e-01, -6.7140e-02, -7.9125e-01,\n",
       "          8.5289e-01, -4.2252e-01,  3.1974e-01,  1.0101e+00, -1.4330e-01,\n",
       "          1.2879e+00,  6.8264e-01, -3.7084e-01,  2.1376e-01, -1.4327e-01,\n",
       "         -5.7791e-01,  4.7522e-02, -1.4555e+00, -7.3328e-01,  1.5527e+00,\n",
       "         -9.9591e-02, -3.3893e-01,  7.5888e-01, -5.4199e-01, -1.0971e-01,\n",
       "         -4.0091e-01, -6.8656e-01,  8.7550e-02, -7.5264e-01, -1.1896e+00,\n",
       "          5.4613e-01,  6.4360e-03,  1.3247e+00,  1.4591e-01, -7.0127e-01,\n",
       "          3.0877e-01,  1.6118e+00,  2.0603e+00, -5.1066e-01,  2.1066e+00,\n",
       "         -5.9266e-02,  9.5798e-01, -1.4065e+00, -3.9040e-01, -3.0182e-01],\n",
       "        [ 5.7502e-02, -2.0137e+00, -1.7316e+00,  2.2969e-01,  9.6013e-01,\n",
       "         -1.2587e+00,  7.6647e-01, -1.8146e+00,  3.4629e-01,  4.7381e-02,\n",
       "          8.7413e-01,  1.3099e+00, -1.6870e+00, -1.6495e+00, -2.8130e+00,\n",
       "         -7.0553e-02, -7.5668e-02,  3.6507e-01,  6.6190e-01,  6.3048e-01,\n",
       "         -6.2638e-01,  4.0515e-01,  1.7310e+00,  1.6105e-01,  2.2605e-01,\n",
       "          1.3939e+00, -5.8125e-01,  6.7055e-01, -7.3460e-01,  2.0369e-01,\n",
       "         -2.2090e+00,  1.4305e-01, -5.9041e-01,  1.8142e+00,  7.9174e-01,\n",
       "         -9.2927e-01,  1.1031e+00, -1.1953e+00, -7.5676e-01,  6.2964e-01,\n",
       "         -4.7575e-02, -7.9687e-01,  1.4241e+00, -5.4193e-01,  1.8558e-01,\n",
       "         -1.5719e+00,  9.3078e-01, -1.3936e-01,  1.4639e+00,  9.8803e-01,\n",
       "         -7.1421e-02, -2.0910e+00, -8.5636e-01,  7.3226e-01,  7.3974e-01,\n",
       "         -1.2444e+00, -1.4409e+00,  1.5782e-01, -9.7425e-01, -6.7974e-01],\n",
       "        [ 2.2948e+00, -1.8640e+00, -5.5448e-01,  3.2076e-01, -2.3818e-01,\n",
       "          1.2985e+00, -1.6192e+00,  5.2382e-01,  1.3727e+00, -1.1024e+00,\n",
       "          5.4309e-01,  1.2560e+00, -1.7994e+00, -5.8652e-01, -1.2797e+00,\n",
       "          1.2038e+00, -2.3111e-01,  5.3263e-01, -1.4184e-01,  8.9734e-01,\n",
       "         -1.5853e+00, -7.6677e-01, -3.9792e-01,  2.5489e-01, -2.3910e+00,\n",
       "          1.4045e+00,  1.0795e+00, -6.9067e-01, -1.2928e+00, -1.3707e-01,\n",
       "         -5.3978e-01, -1.9825e+00, -1.4532e-01,  1.6245e+00, -6.2019e-01,\n",
       "         -1.8293e+00, -7.2871e-01,  6.8437e-01,  3.0371e-03, -6.0490e-01,\n",
       "         -3.5266e-01,  8.5015e-01, -4.6574e-01, -2.2452e+00, -9.5815e-01,\n",
       "          1.1328e+00, -8.0109e-01,  3.8276e-01, -1.6251e+00,  7.1061e-01,\n",
       "         -1.3349e+00,  3.2079e-01,  2.6394e+00, -9.4284e-01,  1.2536e+00,\n",
       "         -2.1126e+00, -8.2753e-01, -9.7723e-01, -7.8323e-01,  8.3279e-01]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72cd7276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.0699e-01,  7.1855e-01,  1.0534e+00, -2.7195e-02, -5.3953e-01,\n",
       "          -2.1916e-01,  2.6058e-01,  9.4682e-01, -5.7270e-01,  6.1821e-01,\n",
       "           7.4706e-01,  8.5857e-01, -6.3387e-01,  1.0074e-01,  1.0871e+00,\n",
       "           4.4188e-01,  6.0410e-01, -3.4202e-02, -1.5008e-01,  8.9701e-01,\n",
       "          -6.9344e-01,  1.7957e+00, -5.8575e-01, -2.4478e+00,  7.0324e-01,\n",
       "          -1.2766e+00, -3.7510e-01, -3.5719e-01, -1.3143e+00,  6.8766e-01,\n",
       "          -2.5387e-01, -1.0662e+00,  5.9732e-01,  1.1774e+00,  1.7849e-01,\n",
       "          -7.2704e-02, -6.2228e-01, -9.0305e-01, -7.6060e-01,  2.4960e+00,\n",
       "          -3.7462e-01, -5.7164e-01,  7.3972e-02, -1.5861e+00, -7.6500e-01,\n",
       "           8.6943e-01,  4.5075e-01,  1.3651e-02,  2.1515e-01, -1.4589e+00,\n",
       "           1.1666e+00, -2.4947e-01, -6.6102e-01,  7.9651e-01,  7.1330e-01,\n",
       "          -5.8549e-01,  8.9783e-01, -5.2981e-01, -1.6684e-01, -5.2264e-01]],\n",
       "\n",
       "        [[ 6.6567e-01,  1.5717e-01, -8.9532e-01,  9.2971e-01, -6.0536e-01,\n",
       "           5.8591e-02, -5.4184e-01,  3.6571e-01, -4.7071e-01,  9.3300e-01,\n",
       "          -5.3932e-01, -8.8753e-01,  1.7115e+00,  2.2261e-02,  1.0817e+00,\n",
       "          -5.2810e-01,  4.6068e-01, -4.0759e-01, -2.1657e+00, -8.6611e-01,\n",
       "           1.1546e+00,  3.8663e-01, -3.0246e-01,  1.7155e-01,  1.5774e-01,\n",
       "           9.7471e-01, -1.0136e+00, -1.1826e+00, -1.0026e+00,  1.4194e-01,\n",
       "           3.0013e-01,  7.5464e-01,  1.3789e+00,  3.0984e-01,  9.0080e-01,\n",
       "          -2.6490e-01, -5.7665e-01, -1.6571e+00, -3.0413e-01,  1.2294e+00,\n",
       "           8.7073e-02, -1.5744e-01, -4.1935e-01, -4.0364e-01, -5.4075e-01,\n",
       "           4.4449e-01,  6.8356e-01,  2.0351e+00,  7.3060e-01,  3.7839e-01,\n",
       "           1.5160e+00,  8.3184e-01, -1.0474e+00,  1.3620e+00,  1.1588e-01,\n",
       "           1.5253e+00,  1.7736e-01,  8.9396e-01,  1.3331e+00, -1.1479e+00]],\n",
       "\n",
       "        [[-3.1383e-01,  1.7709e+00,  2.6152e-01,  4.4231e-01,  1.0152e+00,\n",
       "           2.5837e-01, -1.1688e+00, -5.6972e-01, -2.7012e-01, -3.6764e-01,\n",
       "           6.9320e-01,  6.5906e-01,  2.7373e-01, -4.1007e-01,  1.2787e+00,\n",
       "           3.7425e-01,  1.8299e-01,  1.2031e+00,  8.7180e-02, -2.2043e-01,\n",
       "           2.1431e-01,  8.1473e-01, -3.8800e-01,  8.1544e-01,  1.5113e+00,\n",
       "           8.5249e-01, -1.0512e+00, -1.5145e-01, -4.3640e-01,  6.4185e-01,\n",
       "          -3.6349e-01, -3.4011e-01, -1.6253e-02, -1.5880e+00, -1.5473e+00,\n",
       "          -1.2960e+00, -1.0107e+00,  2.2210e-02, -9.6097e-01, -1.9353e+00,\n",
       "          -1.3860e-01, -2.3329e-01, -8.0765e-01, -1.2327e-01, -1.0109e-01,\n",
       "          -2.0385e+00,  5.9140e-01, -6.8946e-01,  7.7740e-01, -1.5059e+00,\n",
       "          -1.4401e+00, -1.8734e+00, -5.5187e-01,  8.5734e-01,  8.0007e-01,\n",
       "          -7.4051e-02, -1.4025e+00,  9.7038e-01,  8.4608e-01, -7.2799e-01]],\n",
       "\n",
       "        [[-6.1188e-01, -3.7782e-01, -6.9515e-01,  1.7807e+00, -6.2075e-01,\n",
       "           3.7258e-01, -1.6661e+00,  3.5429e-01,  9.0524e-01, -1.5668e+00,\n",
       "          -1.0589e+00,  2.7080e-01, -1.2181e-01,  6.8743e-01,  6.4531e-01,\n",
       "           8.4906e-01,  3.5709e-03, -1.8106e+00, -4.3988e-03,  1.1574e+00,\n",
       "          -1.3036e+00,  9.2590e-01,  3.3343e-01,  4.5248e-02, -1.3245e+00,\n",
       "           3.3481e-01,  1.8628e+00, -4.5933e-01, -1.9357e+00, -1.3864e+00,\n",
       "          -2.6935e-01,  1.3430e+00,  4.7692e-01, -4.2193e-01, -6.1142e-01,\n",
       "          -5.6804e-01,  1.9160e+00,  7.5138e-01, -2.8756e-01, -5.6903e-01,\n",
       "           4.2587e-01,  2.6432e-01,  1.2855e+00,  4.6892e-01,  4.5061e-01,\n",
       "           7.0222e-02, -1.3397e+00,  2.4306e-02, -1.4951e+00, -1.2806e+00,\n",
       "          -2.0086e-01, -1.3639e+00,  2.8384e+00, -4.9187e-01,  1.3123e+00,\n",
       "           1.6335e+00, -2.0656e-01, -5.9957e-01, -1.3313e+00, -4.8102e-01]],\n",
       "\n",
       "        [[ 9.7902e-02, -4.8628e-02, -1.5149e+00,  1.8632e+00,  1.4713e-01,\n",
       "           6.5375e-01,  6.5687e-01, -6.3389e-01,  2.6316e+00,  4.8502e-01,\n",
       "           5.6849e-01, -1.4237e+00, -1.0267e+00,  2.3458e-01, -1.1950e+00,\n",
       "          -4.8989e-01,  7.3510e-01,  6.7091e-01,  2.2866e-01, -5.1608e-01,\n",
       "          -5.1772e-01, -4.2767e-01,  1.0707e+00,  3.4465e-01, -8.9433e-02,\n",
       "           1.0571e+00, -2.1829e-01, -1.2362e+00, -1.5092e+00, -2.7615e-01,\n",
       "          -2.8056e-01, -1.4141e+00, -1.8467e+00, -1.0360e-01,  1.3964e+00,\n",
       "           8.5886e-01, -3.9587e-01,  7.5805e-01,  3.8120e-01, -9.9700e-01,\n",
       "          -5.1492e-01, -1.6261e+00,  4.3083e-01, -7.8798e-01,  1.1481e+00,\n",
       "           1.3981e-01,  4.1850e-01,  8.0180e-01,  1.5969e+00,  2.1907e-01,\n",
       "           1.2043e-01, -9.7177e-01, -1.2572e-01,  2.0148e-01, -6.7343e-01,\n",
       "           7.4242e-01, -1.4649e+00,  1.7106e-01, -2.2052e-02,  3.7862e-01]],\n",
       "\n",
       "        [[-7.6046e-01,  7.7445e-01,  1.0674e+00,  6.0515e-01,  5.6979e-02,\n",
       "          -2.0358e-01,  1.3249e+00, -9.6556e-01, -1.2466e+00, -4.7124e-05,\n",
       "          -5.4907e-01,  1.6015e+00, -1.1331e+00, -3.4846e-01,  6.5020e-01,\n",
       "           1.8926e+00,  9.8035e-01,  5.4590e-02,  1.5252e+00,  3.9066e-01,\n",
       "           1.1330e+00, -4.0262e-01,  8.3597e-01, -1.9556e+00, -8.3535e-01,\n",
       "           1.5365e+00,  8.4672e-01,  7.6476e-01,  1.5271e+00, -3.1132e-01,\n",
       "          -5.8377e-01, -2.7592e-01,  6.5311e-01,  2.3342e-01,  3.5443e-01,\n",
       "          -1.5415e-01,  1.1575e+00,  6.4139e-01,  4.2455e-01, -2.2738e+00,\n",
       "          -1.5026e+00, -2.3264e-01,  1.0121e+00, -1.6208e+00, -1.1724e+00,\n",
       "           8.3668e-01,  8.8240e-01,  1.2998e+00, -2.3566e-01, -1.8489e+00,\n",
       "          -5.8500e-01,  5.0238e-01, -9.1927e-01, -6.3203e-01,  3.8581e-01,\n",
       "           7.2006e-01,  5.2369e-03, -1.7417e+00,  1.3047e+00,  7.0037e-01]],\n",
       "\n",
       "        [[-7.1683e-01,  1.5823e-01, -6.2349e-01,  2.1356e-01, -1.0327e+00,\n",
       "          -9.6736e-01,  8.5647e-01, -3.0849e-01,  2.1342e-01, -1.0970e+00,\n",
       "          -1.2257e-01, -4.1890e-01, -1.3747e+00, -1.0818e+00, -2.5439e-01,\n",
       "          -8.9761e-01, -5.9607e-01,  6.8220e-01, -7.3358e-01,  2.1102e-01,\n",
       "           5.0198e-01,  7.1287e-01,  4.7946e-01, -8.9818e-01, -7.8512e-01,\n",
       "           4.6087e-01,  2.3871e-01,  8.8274e-01, -3.7661e-01,  7.7403e-01,\n",
       "           7.0285e-01,  7.3269e-01, -1.1015e+00, -1.0830e-01,  2.0790e-01,\n",
       "          -1.2537e+00,  1.4871e+00, -2.9796e+00, -7.9944e-02, -6.0269e-01,\n",
       "          -3.0241e-01, -2.4495e+00,  1.4325e+00, -1.3013e+00, -1.4146e+00,\n",
       "           9.6618e-01, -6.8042e-01,  1.5329e-01,  5.5032e-02,  1.1112e+00,\n",
       "           1.5633e+00, -5.1884e-01,  1.1944e+00, -9.0681e-01, -4.9503e-01,\n",
       "           4.2033e-01, -8.4554e-01, -2.5663e-01,  1.8645e+00,  8.6548e-01]],\n",
       "\n",
       "        [[-1.0031e+00,  1.0106e+00,  9.0124e-01, -6.1037e-02,  1.2433e+00,\n",
       "          -2.8391e-01, -1.2603e+00, -1.1845e-01, -3.6038e-01,  1.2522e+00,\n",
       "           2.5964e+00,  2.7716e-01,  1.1813e+00,  2.4513e-02,  4.4231e-01,\n",
       "           9.9420e-01, -5.9467e-01,  7.0462e-01, -6.7140e-02, -7.9125e-01,\n",
       "           8.5289e-01, -4.2252e-01,  3.1974e-01,  1.0101e+00, -1.4330e-01,\n",
       "           1.2879e+00,  6.8264e-01, -3.7084e-01,  2.1376e-01, -1.4327e-01,\n",
       "          -5.7791e-01,  4.7522e-02, -1.4555e+00, -7.3328e-01,  1.5527e+00,\n",
       "          -9.9591e-02, -3.3893e-01,  7.5888e-01, -5.4199e-01, -1.0971e-01,\n",
       "          -4.0091e-01, -6.8656e-01,  8.7550e-02, -7.5264e-01, -1.1896e+00,\n",
       "           5.4613e-01,  6.4360e-03,  1.3247e+00,  1.4591e-01, -7.0127e-01,\n",
       "           3.0877e-01,  1.6118e+00,  2.0603e+00, -5.1066e-01,  2.1066e+00,\n",
       "          -5.9266e-02,  9.5798e-01, -1.4065e+00, -3.9040e-01, -3.0182e-01]],\n",
       "\n",
       "        [[ 5.7502e-02, -2.0137e+00, -1.7316e+00,  2.2969e-01,  9.6013e-01,\n",
       "          -1.2587e+00,  7.6647e-01, -1.8146e+00,  3.4629e-01,  4.7381e-02,\n",
       "           8.7413e-01,  1.3099e+00, -1.6870e+00, -1.6495e+00, -2.8130e+00,\n",
       "          -7.0553e-02, -7.5668e-02,  3.6507e-01,  6.6190e-01,  6.3048e-01,\n",
       "          -6.2638e-01,  4.0515e-01,  1.7310e+00,  1.6105e-01,  2.2605e-01,\n",
       "           1.3939e+00, -5.8125e-01,  6.7055e-01, -7.3460e-01,  2.0369e-01,\n",
       "          -2.2090e+00,  1.4305e-01, -5.9041e-01,  1.8142e+00,  7.9174e-01,\n",
       "          -9.2927e-01,  1.1031e+00, -1.1953e+00, -7.5676e-01,  6.2964e-01,\n",
       "          -4.7575e-02, -7.9687e-01,  1.4241e+00, -5.4193e-01,  1.8558e-01,\n",
       "          -1.5719e+00,  9.3078e-01, -1.3936e-01,  1.4639e+00,  9.8803e-01,\n",
       "          -7.1421e-02, -2.0910e+00, -8.5636e-01,  7.3226e-01,  7.3974e-01,\n",
       "          -1.2444e+00, -1.4409e+00,  1.5782e-01, -9.7425e-01, -6.7974e-01]],\n",
       "\n",
       "        [[ 2.2948e+00, -1.8640e+00, -5.5448e-01,  3.2076e-01, -2.3818e-01,\n",
       "           1.2985e+00, -1.6192e+00,  5.2382e-01,  1.3727e+00, -1.1024e+00,\n",
       "           5.4309e-01,  1.2560e+00, -1.7994e+00, -5.8652e-01, -1.2797e+00,\n",
       "           1.2038e+00, -2.3111e-01,  5.3263e-01, -1.4184e-01,  8.9734e-01,\n",
       "          -1.5853e+00, -7.6677e-01, -3.9792e-01,  2.5489e-01, -2.3910e+00,\n",
       "           1.4045e+00,  1.0795e+00, -6.9067e-01, -1.2928e+00, -1.3707e-01,\n",
       "          -5.3978e-01, -1.9825e+00, -1.4532e-01,  1.6245e+00, -6.2019e-01,\n",
       "          -1.8293e+00, -7.2871e-01,  6.8437e-01,  3.0371e-03, -6.0490e-01,\n",
       "          -3.5266e-01,  8.5015e-01, -4.6574e-01, -2.2452e+00, -9.5815e-01,\n",
       "           1.1328e+00, -8.0109e-01,  3.8276e-01, -1.6251e+00,  7.1061e-01,\n",
       "          -1.3349e+00,  3.2079e-01,  2.6394e+00, -9.4284e-01,  1.2536e+00,\n",
       "          -2.1126e+00, -8.2753e-01, -9.7723e-01, -7.8323e-01,  8.3279e-01]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(ts_feature,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "455ed170",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=training.RV_RNN_conv(n_diff=4,rnn_act=\"tanh\",rnn_num_layer=2,rnn_drop_out=0.3,rnn_hidden_size=32,proj_dim=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85bf9cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model(ts_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc35034a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12.4432],\n",
       "        [14.1024],\n",
       "        [14.2141],\n",
       "        [10.5330],\n",
       "        [11.9638],\n",
       "        [10.5465],\n",
       "        [13.0989],\n",
       "        [10.8659],\n",
       "        [11.3673],\n",
       "        [12.0642]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bfbac7",
   "metadata": {},
   "source": [
    "## Training loop (basic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06d4fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_time=np.load(\"../processed_data/recovered_time_id_order.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "825838c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In fold 0 :\n",
      "\n",
      "Train set end at 8117 .\n",
      "\n",
      "Test set start at 15516 end at 10890 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_split_list=data_processing.time_cross_val_split(list_time=list_time,n_split=1,percent_val_size=10,list_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72aedca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time_id,test_time_id=time_split_list[0][0],time_split_list[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3338fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RV_ts=pd.read_parquet(\"../processed_data/book_RV_ts_60_si.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de450e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"21\" halign=\"left\">sub_int_RV</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub_int_num</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0-1000</th>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>3.818799e-07</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>2.313288e-04</td>\n",
       "      <td>1.060893e-05</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-10000</th>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>3.154886e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>3.777703e-08</td>\n",
       "      <td>3.777703e-08</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-10005</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>4.375100e-04</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>2.552987e-03</td>\n",
       "      <td>5.364106e-04</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-10017</th>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>6.771948e-05</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>3.104404e-03</td>\n",
       "      <td>1.224910e-03</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.003287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-10030</th>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>2.586782e-04</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>4.842470e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99-9972</th>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>2.503467e-04</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>9.916624e-05</td>\n",
       "      <td>1.809852e-04</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99-9973</th>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>9.650211e-04</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>1.862600e-03</td>\n",
       "      <td>7.668418e-04</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.002115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99-9976</th>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>7.203531e-04</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>5.946683e-04</td>\n",
       "      <td>1.509652e-04</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99-9988</th>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>1.957402e-04</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1.440137e-04</td>\n",
       "      <td>3.200939e-05</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99-9993</th>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>1.798617e-04</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>2.728767e-04</td>\n",
       "      <td>2.108380e-04</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sub_int_RV                                                        \\\n",
       "sub_int_num         1         2         3         4         5             6    \n",
       "row_id                                                                         \n",
       "0-1000        0.000341  0.000000  0.000023  0.000000  0.000170  3.818799e-07   \n",
       "0-10000       0.000290  0.000191  0.000087  0.000193  0.000241  3.154886e-04   \n",
       "0-10005       0.000000  0.000000  0.001554  0.002177  0.002303  4.375100e-04   \n",
       "0-10017       0.000142  0.000142  0.001464  0.001086  0.000068  6.771948e-05   \n",
       "0-10030       0.000327  0.000058  0.000293  0.000842  0.000120  2.586782e-04   \n",
       "...                ...       ...       ...       ...       ...           ...   \n",
       "99-9972       0.000197  0.000181  0.000171  0.000172  0.000369  2.503467e-04   \n",
       "99-9973       0.000821  0.000346  0.000691  0.001591  0.000863  9.650211e-04   \n",
       "99-9976       0.000569  0.001101  0.001002  0.000430  0.000797  7.203531e-04   \n",
       "99-9988       0.000040  0.000069  0.000123  0.000056  0.000016  1.957402e-04   \n",
       "99-9993       0.000249  0.000179  0.000155  0.000025  0.000325  1.798617e-04   \n",
       "\n",
       "                                                     ...                      \\\n",
       "sub_int_num        7         8         9         10  ...        51        52   \n",
       "row_id                                               ...                       \n",
       "0-1000       0.000089  0.000552  0.000012  0.000000  ...  0.000265  0.000000   \n",
       "0-10000      0.000000  0.000247  0.000265  0.000000  ...  0.000202  0.000375   \n",
       "0-10005      0.000617  0.001199  0.002306  0.001215  ...  0.000000  0.000486   \n",
       "0-10017      0.000899  0.000064  0.000593  0.000451  ...  0.000029  0.000000   \n",
       "0-10030      0.000221  0.000436  0.000099  0.000008  ...  0.000410  0.000437   \n",
       "...               ...       ...       ...       ...  ...       ...       ...   \n",
       "99-9972      0.000349  0.000356  0.000390  0.000050  ...  0.000075  0.000185   \n",
       "99-9973      0.000504  0.001925  0.000641  0.000382  ...  0.001081  0.001095   \n",
       "99-9976      0.000586  0.000538  0.000570  0.000781  ...  0.000508  0.000406   \n",
       "99-9988      0.000071  0.000095  0.000063  0.000030  ...  0.000034  0.000176   \n",
       "99-9993      0.000080  0.000125  0.000126  0.000205  ...  0.000099  0.000370   \n",
       "\n",
       "                                                                   \\\n",
       "sub_int_num        53        54        55        56            57   \n",
       "row_id                                                              \n",
       "0-1000       0.000214  0.000003  0.000000  0.000118  2.313288e-04   \n",
       "0-10000      0.000616  0.000564  0.000000  0.000023  3.777703e-08   \n",
       "0-10005      0.000050  0.001761  0.001617  0.001801  2.552987e-03   \n",
       "0-10017      0.001293  0.002092  0.000994  0.000848  3.104404e-03   \n",
       "0-10030      0.000004  0.000215  0.000457  0.000183  4.842470e-04   \n",
       "...               ...       ...       ...       ...           ...   \n",
       "99-9972      0.000314  0.000318  0.000115  0.000143  9.916624e-05   \n",
       "99-9973      0.000425  0.000789  0.001295  0.000596  1.862600e-03   \n",
       "99-9976      0.000662  0.000338  0.000710  0.000179  5.946683e-04   \n",
       "99-9988      0.000140  0.000129  0.000175  0.000019  1.440137e-04   \n",
       "99-9993      0.000929  0.000328  0.000251  0.000204  2.728767e-04   \n",
       "\n",
       "                                               \n",
       "sub_int_num            58        59        60  \n",
       "row_id                                         \n",
       "0-1000       1.060893e-05  0.000111  0.000288  \n",
       "0-10000      3.777703e-08  0.000020  0.000310  \n",
       "0-10005      5.364106e-04  0.000872  0.000000  \n",
       "0-10017      1.224910e-03  0.001316  0.003287  \n",
       "0-10030      0.000000e+00  0.000756  0.000005  \n",
       "...                   ...       ...       ...  \n",
       "99-9972      1.809852e-04  0.000334  0.000089  \n",
       "99-9973      7.668418e-04  0.001035  0.002115  \n",
       "99-9976      1.509652e-04  0.000388  0.000403  \n",
       "99-9988      3.200939e-05  0.000041  0.000007  \n",
       "99-9993      2.108380e-04  0.000126  0.000353  \n",
       "\n",
       "[428932 rows x 60 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_RV_ts.pivot(index=\"row_id\",columns=[\"sub_int_num\"],values=[\"sub_int_RV\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "677c1b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target=pd.read_csv(\"../raw_data/kaggle_ORVP/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "252d6274",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target[\"row_id\"]=df_target[\"stock_id\"].astype(int).astype(str)+\"-\"+df_target[\"time_id\"].astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdb28caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428927</th>\n",
       "      <td>126</td>\n",
       "      <td>32751</td>\n",
       "      <td>0.003461</td>\n",
       "      <td>126-32751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428928</th>\n",
       "      <td>126</td>\n",
       "      <td>32753</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>126-32753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428929</th>\n",
       "      <td>126</td>\n",
       "      <td>32758</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>126-32758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>126</td>\n",
       "      <td>32763</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>126-32763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>126</td>\n",
       "      <td>32767</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>126-32767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        stock_id  time_id    target     row_id\n",
       "0              0        5  0.004136        0-5\n",
       "1              0       11  0.001445       0-11\n",
       "2              0       16  0.002168       0-16\n",
       "3              0       31  0.002195       0-31\n",
       "4              0       62  0.001747       0-62\n",
       "...          ...      ...       ...        ...\n",
       "428927       126    32751  0.003461  126-32751\n",
       "428928       126    32753  0.003113  126-32753\n",
       "428929       126    32758  0.004070  126-32758\n",
       "428930       126    32763  0.003357  126-32763\n",
       "428931       126    32767  0.002090  126-32767\n",
       "\n",
       "[428932 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b2a07cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ycoeusz/git/Realized_Volatility_Prediction_-erdos_deeplearning-/NNetwork/../proj_mod/training.py:275: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_tab_copy.loc[:,\"sub_int_num\"]=np.nan\n",
      "/home/ycoeusz/git/Realized_Volatility_Prediction_-erdos_deeplearning-/NNetwork/../proj_mod/training.py:275: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_tab_copy.loc[:,\"sub_int_num\"]=np.nan\n"
     ]
    }
   ],
   "source": [
    "train_dataset=training.RVdataset(time_id_list=train_time_id,ts_features=[\"sub_int_RV\"],df_ts_feat=df_RV_ts,df_target=df_target)\n",
    "test_dataset=training.RVdataset(time_id_list=test_time_id,ts_features=[\"sub_int_RV\"],df_ts_feat=df_RV_ts,df_target=df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7c432cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([386036, 60])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07885f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([386036, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0056627",
   "metadata": {},
   "source": [
    "Not entirely sure what the error code is about, I am literally doing exactly as suggested by the warning. https://stackoverflow.com/questions/23688307/settingwithcopywarning-even-when-using-loc says this is a false positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2efd16ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "device=(torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b95e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model=training.RV_RNN_conv(n_diff=4,rnn_act=\"tanh\",rnn_drop_out=0,rnn_hidden_size=32,proj_dim=32,rnn_num_layer=1).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3846a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer=optim.Adam(RNN_model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "55dd6fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=256,shuffle=True)\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=256,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "292fcf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=[]\n",
    "val_loss=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "810267b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At  12.731416463851929  epoch  1 has training loss  tensor(0.2636, device='cuda:0')  and validation loss  tensor(0.2465, device='cuda:0') .\n",
      "\n",
      "At  65.36981534957886  epoch  5 has training loss  tensor(0.2542, device='cuda:0')  and validation loss  tensor(0.2378, device='cuda:0') .\n",
      "\n",
      "At  131.1433711051941  epoch  10 has training loss  tensor(0.2530, device='cuda:0')  and validation loss  tensor(0.2405, device='cuda:0') .\n",
      "\n",
      "At  197.14231967926025  epoch  15 has training loss  tensor(0.2525, device='cuda:0')  and validation loss  tensor(0.2353, device='cuda:0') .\n",
      "\n",
      "At  262.7027373313904  epoch  20 has training loss  tensor(0.2520, device='cuda:0')  and validation loss  tensor(0.2355, device='cuda:0') .\n",
      "\n",
      "At  328.84028601646423  epoch  25 has training loss  tensor(0.2512, device='cuda:0')  and validation loss  tensor(0.2355, device='cuda:0') .\n",
      "\n",
      "At  394.21911120414734  epoch  30 has training loss  tensor(0.2512, device='cuda:0')  and validation loss  tensor(0.2375, device='cuda:0') .\n",
      "\n",
      "At  459.89581966400146  epoch  35 has training loss  tensor(0.2511, device='cuda:0')  and validation loss  tensor(0.2346, device='cuda:0') .\n",
      "\n",
      "At  525.3399140834808  epoch  40 has training loss  tensor(0.2505, device='cuda:0')  and validation loss  tensor(0.2366, device='cuda:0') .\n",
      "\n",
      "At  591.0108163356781  epoch  45 has training loss  tensor(0.2505, device='cuda:0')  and validation loss  tensor(0.2342, device='cuda:0') .\n",
      "\n",
      "At  656.605872631073  epoch  50 has training loss  tensor(0.2502, device='cuda:0')  and validation loss  tensor(0.2379, device='cuda:0') .\n",
      "\n",
      "At  722.3282012939453  epoch  55 has training loss  tensor(0.2499, device='cuda:0')  and validation loss  tensor(0.2360, device='cuda:0') .\n",
      "\n",
      "At  788.1603856086731  epoch  60 has training loss  tensor(0.2498, device='cuda:0')  and validation loss  tensor(0.2359, device='cuda:0') .\n",
      "\n",
      "The validation loss has not improved for  10  epochs. Stopping current training loop.\n",
      "\n",
      "Best model state dictionary of this training loop is reloaded.\n",
      "\n",
      "According to validation loss, the best model is reached at epoch 51  with validation loss:  tensor(0.2340, device='cuda:0') .\n",
      " The total number of epoch trained is  61 .\n",
      " Training completed in:  801.4168922901154 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('frozen_conv.frozen_conv.weight',\n",
       "              tensor([[[-0.5742,  0.8249]]], device='cuda:0')),\n",
       "             ('linear_proj_input.weight',\n",
       "              tensor([[-0.0010,  0.0140, -0.0248, -0.0627, -0.0048],\n",
       "                      [-0.0101, -0.1089,  0.2952,  0.7073,  0.6144],\n",
       "                      [ 0.0114, -0.4419, -0.4059,  0.2944,  0.5035],\n",
       "                      [ 0.0105, -0.0283,  0.1350,  0.1587,  0.0275],\n",
       "                      [ 0.0115,  0.0615, -0.2302, -0.5448, -0.2916],\n",
       "                      [ 0.0177, -0.0194, -0.2500, -0.3596, -0.1169],\n",
       "                      [-0.0426,  0.3984,  0.2889,  0.0072,  0.4095],\n",
       "                      [-0.3627, -0.5428, -0.0983, -0.3433, -0.2666],\n",
       "                      [-0.0452,  0.1052,  0.1317, -0.0240,  0.0493],\n",
       "                      [ 0.1153, -0.0013,  0.5546,  0.4882,  0.1270],\n",
       "                      [ 0.0209, -0.0862,  0.0433,  0.2496,  0.1208],\n",
       "                      [ 0.0843, -0.2831, -0.1314, -0.1532,  0.1174],\n",
       "                      [ 0.1329, -0.4190, -0.8514, -0.5761, -0.0456],\n",
       "                      [-0.0901, -0.0507,  0.1532, -0.1264, -0.6370],\n",
       "                      [ 0.0233, -0.0301,  0.0850,  0.0791,  0.0400],\n",
       "                      [ 0.0103, -0.0132, -0.0172, -0.0313,  0.0112],\n",
       "                      [ 0.0179, -0.2605, -0.2713, -0.4835,  0.5503],\n",
       "                      [ 0.3679,  0.3694, -0.3611,  0.0833,  0.2049],\n",
       "                      [ 0.1926,  0.6323,  0.5884,  0.0823, -0.1637],\n",
       "                      [ 0.0357,  0.6058,  0.4701,  0.1145,  0.0587],\n",
       "                      [ 0.0825, -0.2932, -0.5307, -0.2750, -0.0528],\n",
       "                      [ 0.0190, -0.0227,  0.0123,  0.0339,  0.0383],\n",
       "                      [-0.0528,  0.2803,  0.2555, -0.3986,  0.4775],\n",
       "                      [-0.2669,  0.3133, -0.5719, -0.4551,  0.2065],\n",
       "                      [ 0.0010, -0.3380, -0.0653,  0.4477,  0.2569],\n",
       "                      [ 0.0452, -0.2006, -0.1431,  0.3212,  0.3149],\n",
       "                      [ 0.0266, -0.2057, -0.5958, -0.5501, -0.4116],\n",
       "                      [-0.1395,  0.3316,  0.5020,  0.2884,  0.1465],\n",
       "                      [ 0.3262,  0.7264, -0.0330, -0.1059, -0.1689],\n",
       "                      [ 0.0114, -0.0223, -0.1999, -0.1779,  0.1615],\n",
       "                      [ 0.0365,  0.0179,  0.2637,  0.4233, -0.1694],\n",
       "                      [-0.0915, -0.4354, -0.4500, -0.4426, -0.2059]], device='cuda:0')),\n",
       "             ('linear_proj_input.bias',\n",
       "              tensor([ 0.0100,  0.0071,  0.0844, -0.0034,  0.0269,  0.0238, -0.0045,  0.0793,\n",
       "                       0.0625, -0.3188, -0.0099,  0.0415,  0.0757,  0.0760, -0.0203,  0.0051,\n",
       "                       0.0982,  0.0564, -0.3540,  0.1301,  0.0385, -0.0146,  0.0923,  0.2359,\n",
       "                       0.0144,  0.0318,  0.0294,  0.0225, -0.0627,  0.0361, -0.1484,  0.2764],\n",
       "                     device='cuda:0')),\n",
       "             ('RNN_layer.weight_ih_l0',\n",
       "              tensor([[-0.1278,  0.0376, -0.1842,  ..., -0.1980, -0.0040,  0.4868],\n",
       "                      [-0.1214, -0.6813, -0.2087,  ..., -0.6877,  0.8646,  0.1304],\n",
       "                      [-0.0069,  0.1466, -0.0187,  ..., -0.0277,  0.0916, -0.3438],\n",
       "                      ...,\n",
       "                      [-0.0225, -0.0358,  0.1499,  ...,  0.0340, -0.0016,  0.0703],\n",
       "                      [ 0.0815,  0.1674,  0.2160,  ..., -0.0316, -0.0213, -0.1411],\n",
       "                      [-0.0575, -0.3069, -0.4484,  ..., -0.3001,  0.3361,  0.0985]],\n",
       "                     device='cuda:0')),\n",
       "             ('RNN_layer.weight_hh_l0',\n",
       "              tensor([[ 0.2189,  0.0428,  0.0844,  ...,  0.2504, -0.0268,  0.0073],\n",
       "                      [ 0.0630, -0.2894, -0.0723,  ...,  0.2273, -0.0201, -0.2922],\n",
       "                      [-0.2359,  0.0580,  0.5384,  ...,  0.1263,  0.1494,  0.1284],\n",
       "                      ...,\n",
       "                      [-0.2278, -0.0027, -0.6881,  ..., -0.2839,  0.3137,  0.5305],\n",
       "                      [-0.4135,  0.3994,  0.3394,  ...,  0.0406, -0.2632,  0.4696],\n",
       "                      [-0.3352,  0.2047, -0.3199,  ...,  0.3021, -0.0407, -0.7308]],\n",
       "                     device='cuda:0')),\n",
       "             ('RNN_layer.bias_ih_l0',\n",
       "              tensor([ 0.0130,  0.1700, -0.1462, -0.0976, -0.0626, -0.1343, -0.2688, -0.0255,\n",
       "                       0.0917,  0.0529, -0.1381, -0.1915, -0.3057, -0.0766, -0.1117, -0.1336,\n",
       "                      -0.4264, -0.1256,  0.0339,  0.2293,  0.1963, -0.0931,  0.2635, -0.0293,\n",
       "                      -0.0093, -0.1926,  0.0024, -0.3110, -0.1134, -0.0915, -0.2517,  0.2798],\n",
       "                     device='cuda:0')),\n",
       "             ('RNN_layer.bias_hh_l0',\n",
       "              tensor([ 0.0424,  0.1134, -0.1306, -0.1947,  0.1363, -0.2255, -0.1964,  0.0232,\n",
       "                      -0.0263, -0.0247, -0.1665,  0.0081, -0.2575, -0.1038,  0.1560,  0.0181,\n",
       "                      -0.4006, -0.2014, -0.1151,  0.3788,  0.1222,  0.0599,  0.1869, -0.0109,\n",
       "                       0.0263, -0.2602,  0.2703, -0.3092, -0.0599, -0.1606, -0.3937,  0.1160],\n",
       "                     device='cuda:0')),\n",
       "             ('linear_post_rnn.weight',\n",
       "              tensor([[-0.0009,  0.0021,  0.1302, -0.0460,  0.1906, -0.2232,  0.0866, -0.1921,\n",
       "                        0.0141, -0.3874,  0.0812, -0.0047,  0.1224, -0.3435,  0.1172,  0.1629,\n",
       "                       -0.2513, -0.2081,  0.0180,  0.0354, -0.1746,  0.3808, -0.0896, -0.0624,\n",
       "                       -0.2368,  0.0778,  0.0821,  0.0196,  0.0193, -0.0776, -0.1531,  0.0097]],\n",
       "                     device='cuda:0')),\n",
       "             ('linear_post_rnn.bias', tensor([0.1542], device='cuda:0'))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.reg_training_loop_rmspe(optimizer=optimizer,model=RNN_model,train_loader=train_loader,val_loader=test_loader,list_train_loss=train_loss,list_val_loss=val_loss,device=device,n_epochs=100,ot_steps=10,report_interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aec32541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0e88bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=[x.cpu() for x in train_loss]\n",
    "val_loss=[x.cpu() for x in val_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3a2ce619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(1, 62)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(1,62)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c4ccf4",
   "metadata": {},
   "source": [
    "Above is an example of the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "05ee5053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "x=np.linspace(1,61,61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ebe3da0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f7830c44190>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAGhCAYAAACZCkVQAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQ21JREFUeJzt3Xt4VPWB//HPJCGJGAghQELCQGy9gLaAEpLSipclD9B2F1qki3ghpV28VFFIi+D+nhLQtQnIUqxQWWnd9umKaBVq7T5FBROKS4AaZGnrpUqxYMwFdE2QS4Dk+/tjyJBJ5nLmlpk5eb+eZx6Yc5szJ2fmfOZ7Ow5jjBEAAECCS4r1DgAAAEQCoQYAANgCoQYAANgCoQYAANgCoQYAANgCoQYAANgCoQYAANgCoQYAANgCoQYAANgCoQYAANhCSKFm3bp1KigoUHp6uoqLi7V3716fy27YsEETJ05UVlaWsrKyVFJS4nX5t99+W9OmTVNmZqYuvvhijR8/XocPH3bPv+GGG+RwODwed911Vyi7DwAAbCjoUPPss8+qrKxM5eXl2rdvn8aMGaMpU6aoqanJ6/LV1dWaPXu2qqqqVFNTI6fTqcmTJ6uurs69zMGDB3Xttddq5MiRqq6u1oEDB/TDH/5Q6enpHtuaN2+e6uvr3Y+VK1cGu/sAAMCmHMHe0LK4uFjjx4/X2rVrJUnt7e1yOp2aP3++lixZEnD9trY2ZWVlae3atZozZ44k6eabb1afPn30q1/9yud6N9xwg8aOHas1a9YEs7tu7e3t+uijj9SvXz85HI6QtgEAAHqWMUbHjx9XXl6ekpIClMWYILS2tprk5GSzZcsWj+lz5swx06ZNs7SNlpYWk56ebl566SVjjDFtbW0mIyPDPPTQQ2by5Mlm8ODBpqioqNtrXH/99WbQoEEmOzvbXHXVVWbJkiXmxIkTPl/n9OnTprm52f146623jCQePHjw4MGDRwI+jhw5EjBjpCgIx44dU1tbm3Jycjym5+Tk6J133rG0jcWLFysvL08lJSWSpKamJn322WeqrKzUv/3bv2nFihXaunWrZsyYoaqqKl1//fWSpFtuuUUjRoxQXl6eDhw4oMWLF+vdd9/V5s2bvb5ORUWFli9f3m36kSNH1L9//2DeNgAAiJGWlhY5nU7169cv4LJBhZpwVVZWatOmTaqurna3l2lvb5ckTZ8+XQsXLpQkjR07Vrt27dL69evdoeaOO+5wb+eLX/yihg4dqkmTJungwYP6/Oc/3+21HnzwQZWVlbmfdxyU/v37E2oAAEgwVpqOBNVQeNCgQUpOTlZjY6PH9MbGRuXm5vpdd9WqVaqsrNQrr7yi0aNHe2wzJSVFV155pcfyo0aN8uj91FVxcbEk6f333/c6Py0tzR1gCDIAANhfUKEmNTVV48aN0/bt293T2tvbtX37dk2YMMHneitXrtTDDz+srVu3qrCwsNs2x48fr3fffddj+l//+leNGDHC5zb3798vSRo6dGgwbwEAANhU0NVPZWVlKi0tVWFhoYqKirRmzRqdOHFCc+fOlSTNmTNH+fn5qqiokCStWLFCS5cu1caNG1VQUKCGhgZJUkZGhjIyMiRJixYt0qxZs3Tdddfpxhtv1NatW/XSSy+purpakqvL98aNG/W1r31N2dnZOnDggBYuXKjrrrvOo9QHAAD0Ypa6LHXx+OOPm+HDh5vU1FRTVFRkdu/e7dFLqbS01P18xIgRXlsxl5eXe2zz5z//ubn00ktNenq6GTNmjPnNb37jnnf48GFz3XXXmYEDB5q0tDRz6aWXmkWLFpnm5mbL+9zc3GwkBbUOAACIrWCu30GPU5OoWlpalJmZqebmZtrXAACQIIK5fnPvJwAAYAuEGgAAYAuEGgAAYAs9OvieHbW1STt3SvX10tCh0sSJUnJyrPcKAIDeh1AThs2bpfvvlz788MK0YcOkxx6TZsyI3X4BANAbUf0Uos2bpZkzPQONJNXVuab7uCUVAACIEkJNCNraXCU03jrDd0xbsMC1HAAA6BmEmhDs3Nm9hKYzY6QjR1zLAQCAnkGoCUF9fWSXAwAA4SPUhMDqPTS51yYAAD2HUBOCiRNdvZwcDu/zHQ7J6XQtBwAAegahJgTJya5u21L3YNPxfM0axqsBAKAnEWpCNGOG9PzzUn6+5/Rhw1zTGacGAICexeB7YZgxQ5o+nRGFAQCIB4SaMCUnSzfcEOu9AAAAVD8BAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbINQAAABbCCnUrFu3TgUFBUpPT1dxcbH27t3rc9kNGzZo4sSJysrKUlZWlkpKSrwu//bbb2vatGnKzMzUxRdfrPHjx+vw4cPu+adPn9Y999yj7OxsZWRk6KabblJjY2Mouw8AAGwo6FDz7LPPqqysTOXl5dq3b5/GjBmjKVOmqKmpyevy1dXVmj17tqqqqlRTUyOn06nJkyerrq7OvczBgwd17bXXauTIkaqurtaBAwf0wx/+UOnp6e5lFi5cqJdeekm//vWvtWPHDn300UeaMWNGCG8ZAADYkcMYY4JZobi4WOPHj9fatWslSe3t7XI6nZo/f76WLFkScP22tjZlZWVp7dq1mjNnjiTp5ptvVp8+ffSrX/3K6zrNzc0aPHiwNm7cqJkzZ0qS3nnnHY0aNUo1NTX60pe+FPB1W1palJmZqebmZvXv39/q2wUAADEUzPU7qJKaM2fOqLa2ViUlJRc2kJSkkpIS1dTUWNrGyZMndfbsWQ0cOFCSKxT993//ty6//HJNmTJFQ4YMUXFxsX7zm9+416mtrdXZs2c9XnfkyJEaPny4z9dtbW1VS0uLxwMAANhXUKHm2LFjamtrU05Ojsf0nJwcNTQ0WNrG4sWLlZeX5w4oTU1N+uyzz1RZWampU6fqlVde0Te/+U3NmDFDO3bskCQ1NDQoNTVVAwYMsPy6FRUVyszMdD+cTmcwbxUAACSYlJ58scrKSm3atEnV1dXu9jLt7e2SpOnTp2vhwoWSpLFjx2rXrl1av369rr/++pBe68EHH1RZWZn7eUtLC8EGAAAbCyrUDBo0SMnJyd16HTU2Nio3N9fvuqtWrVJlZaW2bdum0aNHe2wzJSVFV155pcfyo0aN0uuvvy5Jys3N1ZkzZ/Tpp596lNb4e920tDSlpaUF8/YAAEACC6r6KTU1VePGjdP27dvd09rb27V9+3ZNmDDB53orV67Uww8/rK1bt6qwsLDbNsePH693333XY/pf//pXjRgxQpI0btw49enTx+N13333XR0+fNjv6wIAgN4j6OqnsrIylZaWqrCwUEVFRVqzZo1OnDihuXPnSpLmzJmj/Px8VVRUSJJWrFihpUuXauPGjSooKHC3gcnIyFBGRoYkadGiRZo1a5auu+463Xjjjdq6dateeuklVVdXS5IyMzP13e9+V2VlZRo4cKD69++v+fPna8KECZZ6PgEAAPsLOtTMmjVLR48e1dKlS9XQ0KCxY8dq69at7sbDhw8fVlLShQKgJ554QmfOnHF3xe5QXl6uZcuWSZK++c1vav369aqoqNB9992nK664Qi+88IKuvfZa9/I//vGPlZSUpJtuukmtra2aMmWKfvrTn4byngEAgA0FPU5NomKcGgAAEk/UxqkBAACIV4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgC4QaAABgCymx3oHerK1N2rlTqq+Xhg6VJk6UkpNjvVcAACQmQk2MbN4s3X+/9OGHF6YNGyY99pg0Y0bs9gsAgERF9VMMbN4szZzpGWgkqa7ONX3z5tjsFwAAiYxQE0VtbVJ1tfTMM65/29pcj/vvl4zpvnzHtAULXMsBAADrqH6KEl/VS/PmdS+h6cwY6cgRV1ubG26I+m4CAGAbhJoo6Khe6loaU1cnlZdb20Z9feT3CwAAO6P6KcKsVC9ZMXRo5PYJAIDeIKRQs27dOhUUFCg9PV3FxcXau3evz2U3bNigiRMnKisrS1lZWSopKem2/Le//W05HA6Px9SpUz2WKSgo6LZMZWVlKLsfVTt3+q9eCsThkJxOV/duAABgXdCh5tlnn1VZWZnKy8u1b98+jRkzRlOmTFFTU5PX5aurqzV79mxVVVWppqZGTqdTkydPVl1dncdyU6dOVX19vfvxzDPPdNvWQw895LHM/Pnzg939qAum2sjh8P58zRrGqwEAIFhBh5rVq1dr3rx5mjt3rq688kqtX79effv21VNPPeV1+aefflrf+973NHbsWI0cOVI/+9nP1N7eru3bt3ssl5aWptzcXPcjKyur27b69evnsczFF18c7O5HndVqo+XLpfx8z2nDhknPP884NQAAhCKoUHPmzBnV1taqpKTkwgaSklRSUqKamhpL2zh58qTOnj2rgQMHekyvrq7WkCFDdMUVV+juu+/Wxx9/3G3dyspKZWdn6+qrr9ajjz6qc+fO+Xyd1tZWtbS0eDx6wsSJrnDStRSmQ0f10v/7f9IHH0hVVdLGja5/Dx0i0AAAEKqgej8dO3ZMbW1tysnJ8Ziek5Ojd955x9I2Fi9erLy8PI9gNHXqVM2YMUOXXHKJDh48qH/913/VV7/6VdXU1Cj5fD3Mfffdp2uuuUYDBw7Url279OCDD6q+vl6rV6/2+joVFRVavnx5MG8vIpKTXaMCz5zpCjCdGwd7q16i2zYAAJHhMMZ6n5yPPvpI+fn52rVrlyZMmOCe/sADD2jHjh3as2eP3/UrKyu1cuVKVVdXa/To0T6X+9vf/qbPf/7z2rZtmyZNmuR1maeeekp33nmnPvvsM6WlpXWb39raqtbWVvfzlpYWOZ1ONTc3q3///oHeati8jVPjdLoCDaUxAABY09LSoszMTEvX76BKagYNGqTk5GQ1NjZ6TG9sbFRubq7fdVetWqXKykpt27bNb6CRpM997nMaNGiQ3n//fZ+hpri4WOfOndMHH3ygK664otv8tLQ0r2Gnp8yYIU2fzg0rAQDoKUGFmtTUVI0bN07bt2/XN77xDUlyN/q99957fa63cuVKPfLII3r55ZdVWFgY8HU+/PBDffzxxxrqp9Xt/v37lZSUpCFDhgTzFnpUcnLo1UvcwRsAgOAEPaJwWVmZSktLVVhYqKKiIq1Zs0YnTpzQ3LlzJUlz5sxRfn6+KioqJEkrVqzQ0qVLtXHjRhUUFKihoUGSlJGRoYyMDH322Wdavny5brrpJuXm5urgwYN64IEHdOmll2rKlCmSpJqaGu3Zs0c33nij+vXrp5qaGi1cuFC33Xab115SiY47eAMAEAITgscff9wMHz7cpKammqKiIrN79273vOuvv96Ulpa6n48YMcJI6vYoLy83xhhz8uRJM3nyZDN48GDTp08fM2LECDNv3jzT0NDg3kZtba0pLi42mZmZJj093YwaNcr86Ec/MqdPn7a8z83NzUaSaW5uDuUt95gXXjDG4TDG1cT4wsPhcD1eeMG13LlzxlRVGbNxo+vfc+diudcAAERHMNfvoBoKJ7JgGhrFSlubVFDge0Rih8NVYrN6tbRwISU5AAD7C+b6zb2f4kigWyx03MH7W9/qvlxdnasb+ebN0d1HAADiFaEmjoRzZ+6O8rYFC1wlPgAA9DZBNxRG9IR7Z+6OkpzqaldPKV89p+hZBQCwI0JNHOm4xUJdnedIxMH653+WPvnkwvPO7W3oWQUAsCuqn+JIxy0WJN938Laic6CRLrS3eeAB17+0xwEA2BGhJs7MmOG6U7e3O3g/95z/m2X60tExfPVq7yVAtMcBANgB1U9xyN8tFpKTvd8s0wp/gcVqexwAAOIV49QkIG/tYgYO7F7tFIqu2+na3oZGxgCAnhTM9ZtQk6C6hou2NqmkJPKv01HV9fzzrn9pZAwA6EmEGi/sFmq66hiN2F/PqeRkqb09+Gorh+NCCU7XdTuHHoINACDSGFG4FwrUc8rhkMrKvM8PxBjp449pZAwAiG+EGhvx13Pq+eellSu9zx84MLzX7dzIuLpaeuYZ179dQ05bm+/5/uYBAGAF1U82FKgxb7Ta4/hrZOxv0D+JtjoAAO9oU+NFbwo1wbLSHicUHdVcP/iBtGqV9/Y4vl6PtjoAAIk2NQiSlZGMs7OjN+ifr3Ula211qLoCAEiEGpznrz3OCy9ITz7peh5ssJFCDxkdbXV27vQdXDZvdpUy3XijdMstrn8LCrjlAwD0RlQ/wYO/9jjRHPTPnwULXIGra5ub2bN9V2tJVF0BgB3QpsYLQk1k9NSgf+FyOFzB59Ah13NfQY0RkgEgvgVz/ebeTwhKcrJ0ww0Xnre1ucJDNAb961g3lOqrjqqrRx6RNmyg1xUA9AaU1CBsmze7brIpeQaXrr2fvM3veN61J1QoN+y0il5XAJA46P2EHhXqoH8djZBfeMH7vAULorO/wfS6omcVACQOSmoQMcEO+heobcvOna7eTLGyfLnvqitKcQCgZ9BQ2AtCTeKxOihgT1ddSReqp2hoDADRRfUTbMHKTToXLfJedbV8eXT2qXP11PPP+x8jJ1DVFVVbABBZlNQg7nkbH8fplNas8V1aIkXn1g+BdG4c/cwzvquu/N0Li6otALiA6icvCDWJLZRqHn+9smLV60ryfy8siZ5XANAZocYLQk3v5K+UR/I+71/+RSovj94++Rt7p/OggbTNAQBCjVeEmt4r2F5XUmyqrjrbts21jzRABtDbEWq8INQgGFaqrqKp6z21aG8DoLei9xMQJn8DCj73nOvfUO5YblXXm4TW1blClpWeVfSqAtBbce8nwIcZM6Tp071XXSUnu0JGKCU3odwLyxjXay1Y4Fp34cLQ72cVztg6jMsDIJ5R/QSEyFcj5Jtv9n2vK8n3vbDCYfV+VlLoXckTtRs6QQxIbLSp8YJQg2jwdcEMNLaOt/ld29FEksNxYfv+upL7KpnqaGMUrW7o4dxiw59EDWIALiDUeEGoQU8L9kLd1iaVlMRmXztCz0UXdQ8Aq1dLZWWe07uuO2yY9P770q5dkQ8eoQYTK0HMV4izghIgoGcQarwg1CDeWb3XVU8Kps3Q4MHS0aMXnkcieIQ6UGHHsfQXxHyFODtXxQGJiFDjBaEGiSDWXckjKdzgIYU+UGF1dWh3eLdSnRbtqjgAnujSDSSoWHclj6TON/9sa+ve1by62n+gkfx3RzdGOnLEVQXUVX19ZPbZ2/7cf7/3gBlo3UTAcABIdHTpBuJMKF3JrdzPKjvbe0PhaOoIHo88Im3Y0L1hdCR4CzBDh4a+vc5h6YYbPOft3Ok/iPlbN95RpQY7INQAcSg52ftFsaMkx9vFx9f9rDrPC3VsnQ6hruvtXlqR6unlLcBMnOh63+G0T/IWlqyWANXXB397jlg2UPZVpdYx6CNVakgUhBogwfgryZH8z/MViE6d8l2K09F2ZfXq7oP+dW0cHGn+Birs2K+O+3V1Xe+xx8ILcR090jofyyFDrK373nvd2wtFYoDEaJSmBKpS6xj08R//MbTebUBPoqEw0Mt4u2C++KLvBsrShV/qXdf98pelz38+8j22Ag1UaLVRrq8QEGqIy8+XTp/2v66/8YDCGSBx9mxrPcGCLcmx2qg6UO+2cEqm6B4Pf4K6fpteorm52Ugyzc3Nsd4VIC698IIxw4YZ47psuh5Op2t6oPUcDtej87pdn/t7DBzo+3VD3a8O584ZU1VlzMaNrn/PnfO/zw6HMYsWed//ztN8vd/sbOvvu+v62dnBHbfO6zqdxvz6192P1bBhF46Vt2OxcWPo++twuLbt7W/U8br+5vn6+3aeH0vejhd6XjDXb0INALdQv8R9BY/ly61dILdt8/+60bi4+Nrn557rPt1b+Ajn/fbko3NQ8xYewtlnf0HMXzizEh47AlM4wjlv4jls9TbBXL+pfgIQEd6qECT/Awr6G2smnNcNp9Htzp3WqmO2bXO9Tud1n3tOuuWW0N5HLMSyZ5xkbRyiaI1ULflvu9QbxyKK12pAqp+8oKQGiI1AVT3h/PKNxq9pq9UxGzd2X7eqKvYlM+GUtoRS9RXtx+DBwf99O845fyVAvs4dKyV1Tqf9qqLC/SxFs6qO6icvCDVA7ITbLsbXNqNRdWE1mFRVdV/33DnX+4zHcBDosXx5979R10ARD4+uf9+uF9PWVmvVh8FWmVn5+yeqcD9L0a6qi3qoWbt2rRkxYoRJS0szRUVFZs+ePT6XffLJJ821115rBgwYYAYMGGAmTZrUbfnS0lIjyeMxZcoUj2U+/vhjc8stt5h+/fqZzMxM853vfMccP37c8j4TaoDYiuQvuY7w4O/CFeqv6UDBJNC2rTSc9tfIONCFNZwG2f4eGzf6DgjxFtL8NYweNKhn9sFbSV2k9GQD5XA/S9H6cdFZVEPNpk2bTGpqqnnqqafMX/7yFzNv3jwzYMAA09jY6HX5W265xaxbt868+eab5u233zbf/va3TWZmpvnwww/dy5SWlpqpU6ea+vp69+OTTz7x2M7UqVPNmDFjzO7du83OnTvNpZdeambPnm15vwk1gH2EU5piRbhVZv5KpgLNC9Qry1/j5nDCh69jFSikhdpjSzImOTn+AlO4x6tDJBvdR7OBstXPkrfG/NH8cdFZVENNUVGRueeee9zP29raTF5enqmoqLC0/rlz50y/fv3ML3/5S/e00tJSM336dJ/rvPXWW0aS+eMf/+ie9vvf/944HA5TV1dn6XUJNYB9hNPuxapodCW3Mi/Q6/pa11f4CPSwcuEJNYh5+3/H8869nxIp2HQcr9bW4P6GgbrWd/4benvNjjAd6VIcq5+lrsMuBNNzLtyquqiFmtbWVpOcnGy2bNniMX3OnDlm2rRplrbR0tJi0tPTzUsvveSeVlpaajIzM83gwYPN5Zdfbu666y5z7Ngx9/yf//znZsCAAR7bOXv2rElOTjabN2/2+jqnT582zc3N7seRI0cINYBNRLukpkOsximJdNd6X+EhmCqCUINYoJDmbX68tOUJpsSs87g8/oKJr3VDHUqgaylOsOdOqI3bgwmi4VbVRS3U1NXVGUlm165dHtMXLVpkioqKLG3j7rvvNp/73OfMqVOn3NOeeeYZ8+KLL5oDBw6YLVu2mFGjRpnx48ebc+f/Go888oi5/PLLu21r8ODB5qc//anX1ykvLzdd2+kQagB7CLfdi535KwWIdGNtK68baJ63+dFqy2Olysxfe53OAdHftqMZEHyFJSsDHXr7O/RE4/a4LakJN9RUVFSYrKws87//+79+lzt48KCRZLZt22aMCS3UUFID2Fs0u4rbVSKNkBuJkaq7lvhYbbsUas+qWD0C9eiyWroUjRAZ121qwql+evTRR01mZqZHuxh/Bg0aZNavX2+MCa36qSva1AD2E+3SB8RWoFGfA5W2BNvuJdC5k4jjEAUKHf5Kebq2owm0LV/bDlfUGwrfe++97udtbW0mPz/fb0PhFStWmP79+5uamhpLr3HkyBHjcDjMiy++aIy50FD4jTfecC/z8ssv01AYQEKVPiB4wTaMjlQ7IW9CvU9WPD86l6Z0PR7btlnbhrcxjiL54yLqXbrT0tLML37xC/PWW2+ZO+64wwwYMMA0NDQYY4y5/fbbzZIlS9zLV1ZWmtTUVPP88897dNnuGGPm+PHj5gc/+IGpqakxhw4dMtu2bTPXXHONueyyy8zp06fd25k6daq5+uqrzZ49e8zrr79uLrvsMrp0A0Av1tMldT1VUhOL3mChDCbpLxAl1IjCjz/+uBk+fLhJTU01RUVFZvfu3e55119/vSktLXU/HzFihPHWYLe8vNwYY8zJkyfN5MmTzeDBg02fPn3MiBEjzLx589whqcPHH39sZs+ebTIyMkz//v3N3LlzGXwPAHq5WAxU5+8iH+otJ/w1UB42LLzxgKw8fPVQioe2a9zQ0gtuaAkACFfHzS4l1+W9Q+ebXUrdb6bpdEo33yytWuV/3RkzvN9Y8sUXfb+uMeHflLSqSrrhBu/zvN0c1OmU1qzpmRt7BnP9JtQAABAEKxd5f3cADzUg+FtX8h56Aum4G/qhQ/7vyB3LO3gTarwg1AAAIiWci3y01vUVeqyWEMUrQo0XhBoAgN1Fo4Qo1gg1XhBqAAC9WSyrkMIRzPU7pYf2CQAAxFBysu/GwHaRFOsdAAAAiARCDQAAsAVCDQAAsAVCDQAAsAVCDQAAsAVCDQAAsAVCDQAAsAVCDQAAsAVCDQAAsAVCDQAAsAVCDQAAsAVCDQAAsAVCDQAAsAVCDQAAsAVCDQAAsAVCDQAAsAVCDQAAsAVCDQAAsAVCDQAAsAVCDQAAsAVCDQAAsAVCDQAAsAVCDQAAsIWUWO9Awmtvk47ulE7VSxcNlQZPlJKSY71XAAD0OoSacBzZLNXeL5388MK0vsOkcY9Jzhmx2y8AAHohqp9CdWSztHOmZ6CRpJN1rulHNsdmvwAA6KUINaFob3OV0Mh4mXl+Wu0C13IAAKBHEGpCcXRn9xIaD0Y6ecS1HAAA6BGEmlCcqo/scgAAIGyEmlBcNDSyywEAgLARakIxeKKrl5McPhZwSH2druUAAECPINSEIinZ1W1bUvdgc/75uDWMVwMAQA8i1ITKOUOa+LzUN99zet9hrumMUwMAQI9i8L1wOGdI+dMZURgAgDhAqAlXUrKUc0Os9wIAgF6P6icAAGALhBoAAGALhBoAAGALhBoAAGALhBoAAGALhBoAAGALIYWadevWqaCgQOnp6SouLtbevXt9LrthwwZNnDhRWVlZysrKUklJid/l77rrLjkcDq1Zs8ZjekFBgRwOh8ejsrIylN0HAAA2FHSoefbZZ1VWVqby8nLt27dPY8aM0ZQpU9TU1OR1+erqas2ePVtVVVWqqamR0+nU5MmTVVdX123ZLVu2aPfu3crLy/O6rYceekj19fXux/z584PdfQAAYFNBh5rVq1dr3rx5mjt3rq688kqtX79effv21VNPPeV1+aefflrf+973NHbsWI0cOVI/+9nP1N7eru3bt3ssV1dXp/nz5+vpp59Wnz59vG6rX79+ys3NdT8uvvjiYHcfAADYVFCh5syZM6qtrVVJScmFDSQlqaSkRDU1NZa2cfLkSZ09e1YDBw50T2tvb9ftt9+uRYsW6aqrrvK5bmVlpbKzs3X11Vfr0Ucf1blz53wu29raqpaWFo8HAACwr6Buk3Ds2DG1tbUpJyfHY3pOTo7eeecdS9tYvHix8vLyPILRihUrlJKSovvuu8/nevfdd5+uueYaDRw4ULt27dKDDz6o+vp6rV692uvyFRUVWr58uaV9AgAAia9H7/1UWVmpTZs2qbq6Wunp6ZKk2tpaPfbYY9q3b58cDofPdcvKytz/Hz16tFJTU3XnnXeqoqJCaWlp3ZZ/8MEHPdZpaWmR0+mM4LsBAADxJKjqp0GDBik5OVmNjY0e0xsbG5Wbm+t33VWrVqmyslKvvPKKRo8e7Z6+c+dONTU1afjw4UpJSVFKSor+/ve/6/vf/74KCgp8bq+4uFjnzp3TBx984HV+Wlqa+vfv7/EAAAD2FVSoSU1N1bhx4zwa+XY0+p0wYYLP9VauXKmHH35YW7duVWFhoce822+/XQcOHND+/fvdj7y8PC1atEgvv/yyz23u379fSUlJGjJkSDBvAQAA2FTQ1U9lZWUqLS1VYWGhioqKtGbNGp04cUJz586VJM2ZM0f5+fmqqKiQ5Govs3TpUm3cuFEFBQVqaGiQJGVkZCgjI0PZ2dnKzs72eI0+ffooNzdXV1xxhSSppqZGe/bs0Y033qh+/fqppqZGCxcu1G233aasrKywDgAAALCHoEPNrFmzdPToUS1dulQNDQ0aO3astm7d6m48fPjwYSUlXSgAeuKJJ3TmzBnNnDnTYzvl5eVatmyZpddMS0vTpk2btGzZMrW2tuqSSy7RwoULPdrMAACA3s1hjDGx3ome0NLSoszMTDU3N9O+BgCABBHM9Zt7PwEAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFsg1AAAAFtIifUOwEba26SjO6VT9dJFQ6XBE6Wk5FjvFQCglyDUIDKObJZq75dOfnhhWt9h0rjHJOeM2O0XAKDXoPoJ4TuyWdo50zPQSNLJOtf0I5tjs18AgF6FUIPwtLe5SmhkvMw8P612gWs5AACiiFCD8Bzd2b2ExoORTh5xLQcAQBQRahCeU/WRXQ4AgBCFFGrWrVungoICpaenq7i4WHv37vW57IYNGzRx4kRlZWUpKytLJSUlfpe/66675HA4tGbNGo/pn3zyiW699Vb1799fAwYM0He/+1199tlnoew+IumioZFdDgCAEAUdap599lmVlZWpvLxc+/bt05gxYzRlyhQ1NTV5Xb66ulqzZ89WVVWVampq5HQ6NXnyZNXV1XVbdsuWLdq9e7fy8vK6zbv11lv1l7/8Ra+++qp+97vf6Q9/+IPuuOOOYHcfkTZ4oquXkxw+FnBIfZ2u5QAAiCKHMcZbC0+fiouLNX78eK1du1aS1N7eLqfTqfnz52vJkiUB129ra1NWVpbWrl2rOXPmuKfX1dWpuLhYL7/8sr7+9a9rwYIFWrBggSTp7bff1pVXXqk//vGPKiwslCRt3bpVX/va1/Thhx96DUFdtbS0KDMzU83Nzerfv38wbxmBdPR+kuTZYPh80Jn4PN26AQAhCeb6HVRJzZkzZ1RbW6uSkpILG0hKUklJiWpqaixt4+TJkzp79qwGDhzontbe3q7bb79dixYt0lVXXdVtnZqaGg0YMMAdaCSppKRESUlJ2rNnj9fXaW1tVUtLi8cDUeKc4QouffM9p/cdRqABAPSYoAbfO3bsmNra2pSTk+MxPScnR++8846lbSxevFh5eXkewWjFihVKSUnRfffd53WdhoYGDRkyxHPHU1I0cOBANTQ0eF2noqJCy5cvt7RPiADnDCl/eugjCjMaMQAgTD06onBlZaU2bdqk6upqpaenS5Jqa2v12GOPad++fXI4fLXLCN6DDz6osrIy9/OWlhY5nc6IbR9eJCVLOTcEvx6jEQMAIiCo6qdBgwYpOTlZjY2NHtMbGxuVm5vrd91Vq1apsrJSr7zyikaPHu2evnPnTjU1NWn48OFKSUlRSkqK/v73v+v73/++CgoKJEm5ubndGiKfO3dOn3zyic/XTUtLU//+/T0eiEOMRgwAiJCgQk1qaqrGjRun7du3u6e1t7dr+/btmjBhgs/1Vq5cqYcfflhbt271aBcjSbfffrsOHDig/fv3ux95eXlatGiRXn75ZUnShAkT9Omnn6q2tta93muvvab29nYVFxcH8xYQTxiNGAAQQUFXP5WVlam0tFSFhYUqKirSmjVrdOLECc2dO1eSNGfOHOXn56uiokKSq73M0qVLtXHjRhUUFLjbwGRkZCgjI0PZ2dnKzs72eI0+ffooNzdXV1xxhSRp1KhRmjp1qubNm6f169fr7Nmzuvfee3XzzTdb6vmEOBXMaMShVGsBAHqVoEPNrFmzdPToUS1dulQNDQ0aO3astm7d6m48fPjwYSUlXSgAeuKJJ3TmzBnNnDnTYzvl5eVatmyZ5dd9+umnde+992rSpElKSkrSTTfdpJ/85CfB7j7iCaMRAwAiKOhxahIV49TEocZqafuNgZebVEVJDQD0UlEbpwaIKEYjBgBEEKEGsZOU7Oq2Lal7sDn/fNwaxqsBAFhCqEFsMRoxACBCenTwPcCrcEcjBgBAhBrEi1BHIwYA4DyqnwAAgC0QagAAgC0QagAAgC0QagAAgC0QagAAgC3Q+wmJob2NLt8AAL8INYh/RzZLtfd73tG77zDXaMQMzgc7IsQDISHUIL4d2SztnCmpy31XT9a5pjPqMAJJtIBAiO85iXZuICBCTaLqDR/G9jbXl3vXQCOdn+aQahe4RiO223tHZCRaQCDE95xEOzdgCQ2FE9GRzdJvC6TtN0q7bnH9+9sC13Q7ObrT8wunGyOdPOJaDuiqIyB0PYc6AkK8fV4Chni5Qnx7Ww/ulE0l2rkBywg10dTeJjVWSx884/o3El9GvenDeKo+ssuh90jEgECI7xmJeG7AMqqfoiUaRZu9rTrmoqGRXQ6JK9jq1mACQrzcc4wQ3zMS8dyAZYSaaIhWvXhv+zAOnugKgifr5D3IOVzzB0/s6T1DTwrlB0IiBgRCfM9IxHMDllH9FGnRLNrsbR/GpGTXhUuS5Ogy8/zzcWvsUSoF70Ktbk3EgNAR4rud6x0cUl9n/Ib4aFS3R0MinhuwjFATadGsF0/0D2MoX3rOGa6Srb75ntP7DqMniN2F8wMhEQNCIof4ROq8kIjnBiwj1ERaNEtTEvnDGM6XnnOGNO0DaVKV9OWNrn+nHSLQ2F04PxASNSAkYohPtM4LiXpuwBLa1ERaNEtTOj6MO2fK9eHr/Au204dRcpWExMsYNpFoY5SUbI92QrAu3B8IHQHBa3ucNfEZECTXfuVPj844VJEe3ypROy8k6rkRbTYY/4xQE2nRbtwa6MMouUpA4mVAqUT90ot3NvjyCSgSPxCiGRCiKRoh3kqD697Qy6xDop4b0WKTwQgJNZFmtTQlnA+Orw9j3YvxNxppIn/pxSubfPkEFKkfCJTyWSstlXpHL7POODdcbDSSNW1qoqEn6sU7PowFsy98KK02quzJXgqJ/qUXbxKt/UI4aPsQGVYaXO+5o/f0MoMnmw1GSElNtPR00abVEpG/PCId3NBzv/L50ouc3liVR9uH8Fn5bjjzse95/s4rxpJKfDYrTSfURFNPFm1aLen4U3n3aZ2LGCMdxPjSixybfflYRtuH8IRdCurnvOqJ6nZEl81K0wk18SrYBnthlXSc/zW25w4p+X7pVARLcfjSixybffkEhbYP1nX97kgbEpnt2q2XGVxsVppOqIlHoTQEDVgiEoiPIuhINBSL9pdeb+gJJNnuywdR4O2746J8KTVbOvOJQvtu6NhOjHqZ9ZbPd6zYrDTdYYwJ4yxPHC0tLcrMzFRzc7P69+8f693xzVcr9I5SDX/hwr2u1L1EJJw/8/mTetqh8Me0iPSXU2/pCSS5jt9vCwJ/+YT7dwq0D1xg4pPf7w7j5f+dnvsNPT1wXvnSmz7fseT32qGY934K5vpN76d4Em4rdH+9rr64PIwdC+PWDp117bEViUAT655APdmTLFK9gULd50QaCr+3sdKIPDXbVWrTWd9h0sQXpOInz0/wc15JPXtvp3j4fPcWiTiStQ+U1MSTxmrXhSKQSVX+2xd4+zUtBfiVb8GXN7oCSTxwl1r4ajjbA78uA/2KjFaphtfXdVqrygv1l284JYiIPqvfHf+wTXIkez8n/Z1XUs+WmMTD57s3itOS2GCu37SpiSeRagjqq1Glzwa7FsVTW41Y9wQKNFjVqB9If38mOheBUNsvhDrAlp27ksfpl3jQrH53nG7y/cMkngb1jPXnu7eyQYN8Qk0shdpLIdRw4avB7kXDpLZTgevU46mhWCx7AlmpJnz70e6zInkRCPbLJ5xgYtcLjJ3aa0SqEXnX8ypWgbY39/RDWAg1sRJSL4UIhIuAv8YSpNt1LHsCBbzI+xLDUo1wgokdLzBWS60SpSQnWj1YYhVo7d7TL9B5FY/nXTzukxeEmljw9YV66qNO06IYLrz9yk+0sSZi2Q0xrIt3jEo1wgkmdrvAWC19aG+X3lyYGCU50RoPKlaB1mbdjD0EKiGMxxLEeNwnH+j91NPC6qUQ5caYzhnStA9cDZG/vNH177RDcXfSSortfYEicfHu6VKNcIJJxwWm23Hu4HA1KE2UC4zV0of/+VZi9byJRg+WWAVau973K1CPrjcfiL8eXwnWC43eTz0tEr0UcEE4PYFCFXC8GAsC9WCLtHDHuIn2OBY9WbT9wTOuLukhi/OeN5E8lrEeGykWn+9oCdijS67vfOOrq3wMzrs46YVG76d4FoleCrEUb/WqsbgvkN+i/kBiVGwebvVENKsne7poO+xShThvGB3JHiyxvs2Jne77ZaUtns9AI8XkvEvATgKEmp6WyO0T4rVeNRbdEH1e5J3SiJult1edn+DnIhCtgOhru1aCib99isYFJtRu5uEI+5Yi5yVSw+hwxLq9nQ26GUuK3PnSk+ddAnYSINT0tERtABeLi0+883eRH/Ql/xeBaAXEQNv1t89W9imSF5hYdRcOWPpgMejE4w+PaLFTiUmsROp86cnzLgF/hNOmJhbi/D4b3cRJvWrI/JU+RLM6zde2ozU6b0TuG9aDIwZHagTtUPlqr3HNv0v7ymJ7jy3Yj5W2eI5kybT7mB/LNjWx/SzQpibexbo4N1gJWK/q5q/0QYpudZq3Uo1olU6Es92eKjHpGvJO1llbL1DRdqjB1F/pgyM5scZtQvyz0j5pZNn5quswzrtI/lCLdZuqEBBqYiWRinMTsF5VUoAqs5u8rxPt6jSrAbGpOrjeb+EEz54Ird7CZdoga+v6K9oOtxrPV3VaJH549HSbKcQ/K+dVoKprf6JRrZ1gP8IJNbGUKA3gErBe1dKtDLyK8qi/VoPf6/98fmTp8wJ9MYUTPKMdWn2Fy9ZjAVYM0L4s2u28wvnhEas2U4h/gc6raN7bLdTzOYF+hBNqEFgiNm4O+VYGUlSr06wGv86BRgp8oQ4neEYztPoNl50FWbTdU1VmofzwiFbYsnNj/d5W+hTovIrGvd323CEl3y+dinCpZpwJaUThdevWqaCgQOnp6SouLtbevXt9LrthwwZNnDhRWVlZysrKUklJSbflly1bppEjR+riiy92L7Nnzx6PZQoKCuRwODwelZWVoew+gpWIo3tGoirsVL3ry6Kx2jVgW2O163k4Ao7O68v5L6vaBd73IZxRf6M5YrDVcNm1KirQKLjBVJn1JCslhL7+hrHYbjw4stnVGHX7ja5BEbff6HoeZyPVxjUrn4czH3sGGiluRwUOR9Ch5tlnn1VZWZnKy8u1b98+jRkzRlOmTFFTU5PX5aurqzV79mxVVVWppqZGTqdTkydPVl3dhUaCl19+udauXas//elPev3111VQUKDJkyfr6NGjHtt66KGHVF9f737Mnz8/2N1HqKIxBHs0RaIq7Ph7kf+y9RsQA/FzoQ4neEYztFoNl9f8OLjbc8RrO69oha14DXHhSrAh+ONWyOd5ggdiL4Lu0l1cXKzx48dr7dq1kqT29nY5nU7Nnz9fS5YsCbh+W1ubsrKytHbtWs2ZM8frMh3dt7Zt26ZJkyZJcpXULFiwQAsWLAhmd7ttMy66dCeyRCkmDutWBg4pdaCPu6VHqIuzt7YR7tcM4MsbfY82Hc6w8uEOSe/t3Di6MzrdtmPdHdwXq7dgmPBfrh8IVj9HVrfr79yIN4k+VEQ8sfp58Cfcz0oUrw1R69J95swZ1dbW6sEHH3RPS0pKUklJiWpqaixt4+TJkzp79qwGDhzo8zWefPJJZWZmasyYMR7zKisr9fDDD2v48OG65ZZbtHDhQqWkeH8Lra2tam1tdT9vaWmxtH8IIEHqVa0PsOZvXhTba3hreNfeJlWVBF7XXylUOA36otEw9prV0WmPFa/tvKyWEO5bKLV2KokO1LYhERvrB5LIQ0XEm0iMkh1OqWYcNWAPqvrp2LFjamtrU05Ojsf0nJwcNTQ0WNrG4sWLlZeXp5ISzy/v3/3ud8rIyFB6erp+/OMf69VXX9WgQRfq2e+77z5t2rRJVVVVuvPOO/WjH/1IDzzwgM/XqaioUGZmpvvhdDqDeKewBb9VZi+4Ht7mfXG5q/7ZpwgV9XcExILZrn9zbohM25au2w0meIWyrr8qhNdnSSM6Sg4iWLUVD+28vLW3stpmqtWzaj1gdYvd7pQu9UwVYqTbxMWrsKq1zws1EMdZFWJQ1U8fffSR8vPztWvXLk2YMME9/YEHHtCOHTu6Ne7tqrKyUitXrlR1dbVGjx7tMe/EiROqr6/XsWPHtGHDBr322mvas2ePhgwZ4nVbTz31lO6880599tlnSktL6zbfW0mN0+mk+qk3CnZE4cPPxa6o366jTV+9WnpzYeTvthyruzgHGtTR698wkBjfKb2nRbsKMdqlB/FYFe/tPV80TGo75aM6XQqrmq+HqhCjVv00aNAgJScnq7Gx0WN6Y2OjcnNz/a67atUqVVZWatu2bd0CjSRdfPHFuvTSS3XppZfqS1/6ki677DL9/Oc/96jq6qy4uFjnzp3TBx98oCuuuKLb/LS0NK9hB72Qvyozb/NiWdSfYANdWa5CSB8kTfsg8heBWIyfYaVrtbe/Ydrg7iU0HgJUtyTauRFINKsQo9393UpgisXgi74+D3UvRmdU4DisQgwq1KSmpmrcuHHavn27vvGNb0hyNRTevn277r33Xp/rrVy5Uo888ohefvllFRYWWnqt9vZ2j5KWrvbv36+kpCSfJTlAyGLdXiOBBroKqgohWu2xerKdl9XxcaYd6v43PFkn1dwW+DX8HdNEOjcCidQQ/F0v8tlfju4YRlYCkxS7wRe9fR6iFYjjsBdi0IPvlZWVqbS0VIWFhSoqKtKaNWt04sQJzZ07V5I0Z84c5efnq6KiQpK0YsUKLV26VBs3blRBQYG77U1GRoYyMjJ04sQJPfLII5o2bZqGDh2qY8eOad26daqrq9O3vvUtSVJNTY327NmjG2+8Uf369VNNTY0WLlyo2267TVlZWZE6FoBLPNzvJFEaZNuxAas/wf4y7fw3bKy29hqBjlWinBud+SpdCPdi6+v2G35Hqw6j9MDqIHfeqnpiPfhiNAJxHH7+gw41s2bN0tGjR7V06VI1NDRo7Nix2rp1q7vx8OHDh5WUdKH98RNPPKEzZ85o5syZHtspLy/XsmXLlJycrHfeeUe//OUvdezYMWVnZ2v8+PHauXOnrrrqKkmuqqRNmzZp2bJlam1t1SWXXKKFCxeqrKwsnPcO+Ga3ov5oiXWpVk8L55dpbztWHQKVLkT6tgABb79xXiilB1YHufM1LxY3rO0s0oE4Ds/poMepSVSMU4OQxGNjwHhjtwas/oTbuLU3HSvJd/AI9/0GbKBqgZUGyN7uLG+lCjESr91ZvI7LJPXIOR3M9Tuk2yQAvUY43aN7i0QbbToc4Xat7k3HKpq3dgjr3m4Wu797u33DvgUhvmYXwZYSxWHbFbc4O6e5oSWA8NmpAas/kWhv1VuOVTR7xoR88bb4Nwq3aiuQYNuYxGHbFQ9xdE4TagBERiI2YA1FJNpb9YZjFc3SBasX767d6K38jcK6s7yRUrMDjwljlxG0O4uTc5pQAwDBiqNfpnErmqULVi/y//i+9PGu4P5GwdxZ3ltgkiLfczIeemQmCEINAIQiTn6Zxq1oli5YvcinpEavauuaH/u+KWk0ek7SI9MSQg0AIPKiXboQrYu81ZKjvvm+A1O0SvIoIQyILt0AgOiJ9v25Ij3sgru7eIASpjDvZwTronbvJwAAghLt0oVIVwPSfiWhEWoAANGVaO2PaL+SsAg1AAB0RfuVhESoAQDAm0QrYQK3SQAAAPZAqAEAALZAqAEAALZAqAEAALZAqAEAALZAqAEAALZAqAEAALZAqAEAALZAqAEAALbQa0YU7rgZeUtLS4z3BAAAWNVx3e64jvvTa0LN8ePHJUlOpzPGewIAAIJ1/PhxZWZm+l3GYaxEHxtob2/XRx99pH79+snhcAS1bktLi5xOp44cOaL+/ftHaQ/tgWMVHI6XdRwr6zhW1nGsrIvVsTLG6Pjx48rLy1NSkv9WM72mpCYpKUnDhg0Laxv9+/fnpLeIYxUcjpd1HCvrOFbWcaysi8WxClRC04GGwgAAwBYINQAAwBYINRakpaWpvLxcaWlpsd6VuMexCg7HyzqOlXUcK+s4VtYlwrHqNQ2FAQCAvVFSAwAAbIFQAwAAbIFQAwAAbIFQAwAAbIFQY8G6detUUFCg9PR0FRcXa+/evbHepZj7wx/+oH/6p39SXl6eHA6HfvOb33jMN8Zo6dKlGjp0qC666CKVlJTovffei83OxlhFRYXGjx+vfv36aciQIfrGN76hd99912OZ06dP65577lF2drYyMjJ00003qbGxMUZ7HDtPPPGERo8e7R7ca8KECfr973/vns9x8q2yslIOh0MLFixwT+N4uSxbtkwOh8PjMXLkSPd8jpOnuro63XbbbcrOztZFF12kL37xi3rjjTfc8+P5+51QE8Czzz6rsrIylZeXa9++fRozZoymTJmipqamWO9aTJ04cUJjxozRunXrvM5fuXKlfvKTn2j9+vXas2ePLr74Yk2ZMkWnT5/u4T2NvR07duiee+7R7t279eqrr+rs2bOaPHmyTpw44V5m4cKFeumll/TrX/9aO3bs0EcffaQZM2bEcK9jY9iwYaqsrFRtba3eeOMN/cM//IOmT5+uv/zlL5I4Tr788Y9/1H/8x39o9OjRHtM5XhdcddVVqq+vdz9ef/119zyO0wX/93//p6985Svq06ePfv/73+utt97Sv//7vysrK8u9TFx/vxv4VVRUZO655x7387a2NpOXl2cqKipiuFfxRZLZsmWL+3l7e7vJzc01jz76qHvap59+atLS0swzzzwTgz2ML01NTUaS2bFjhzHGdWz69Oljfv3rX7uXefvtt40kU1NTE6vdjBtZWVnmZz/7GcfJh+PHj5vLLrvMvPrqq+b66683999/vzGG86qz8vJyM2bMGK/zOE6eFi9ebK699lqf8+P9+52SGj/OnDmj2tpalZSUuKclJSWppKRENTU1Mdyz+Hbo0CE1NDR4HLfMzEwVFxdz3CQ1NzdLkgYOHChJqq2t1dmzZz2O18iRIzV8+PBefbza2tq0adMmnThxQhMmTOA4+XDPPffo61//usdxkTivunrvvfeUl5enz33uc7r11lt1+PBhSRynrn7729+qsLBQ3/rWtzRkyBBdffXV2rBhg3t+vH+/E2r8OHbsmNra2pSTk+MxPScnRw0NDTHaq/jXcWw4bt21t7drwYIF+spXvqIvfOELklzHKzU1VQMGDPBYtrcerz/96U/KyMhQWlqa7rrrLm3ZskVXXnklx8mLTZs2ad++faqoqOg2j+N1QXFxsX7xi19o69ateuKJJ3To0CFNnDhRx48f5zh18be//U1PPPGELrvsMr388su6++67dd999+mXv/ylpPj/fu81d+kG4sE999yjP//5zx71+fB0xRVXaP/+/Wpubtbzzz+v0tJS7dixI9a7FXeOHDmi+++/X6+++qrS09NjvTtx7atf/ar7/6NHj1ZxcbFGjBih5557ThdddFEM9yz+tLe3q7CwUD/60Y8kSVdffbX+/Oc/a/369SotLY3x3gVGSY0fgwYNUnJycrdW8I2NjcrNzY3RXsW/jmPDcfN077336ne/+52qqqo0bNgw9/Tc3FydOXNGn376qcfyvfV4paam6tJLL9W4ceNUUVGhMWPG6LHHHuM4dVFbW6umpiZdc801SklJUUpKinbs2KGf/OQnSklJUU5ODsfLhwEDBujyyy/X+++/z3nVxdChQ3XllVd6TBs1apS7ui7ev98JNX6kpqZq3Lhx2r59u3tae3u7tm/frgkTJsRwz+LbJZdcotzcXI/j1tLSoj179vTK42aM0b333qstW7botdde0yWXXOIxf9y4cerTp4/H8Xr33Xd1+PDhXnm8umpvb1drayvHqYtJkybpT3/6k/bv3+9+FBYW6tZbb3X/n+Pl3WeffaaDBw9q6NChnFddfOUrX+k25MRf//pXjRgxQlICfL/HuqVyvNu0aZNJS0szv/jFL8xbb71l7rjjDjNgwADT0NAQ612LqePHj5s333zTvPnmm0aSWb16tXnzzTfN3//+d2OMMZWVlWbAgAHmxRdfNAcOHDDTp083l1xyiTl16lSM97zn3X333SYzM9NUV1eb+vp69+PkyZPuZe666y4zfPhw89prr5k33njDTJgwwUyYMCGGex0bS5YsMTt27DCHDh0yBw4cMEuWLDEOh8O88sorxhiOUyCdez8Zw/Hq8P3vf99UV1ebQ4cOmf/5n/8xJSUlZtCgQaapqckYw3HqbO/evSYlJcU88sgj5r333jNPP/206du3r/mv//ov9zLx/P1OqLHg8ccfN8OHDzepqammqKjI7N69O9a7FHNVVVVGUrdHaWmpMcbV7e+HP/yhycnJMWlpaWbSpEnm3Xffje1Ox4i34yTJ/Od//qd7mVOnTpnvfe97Jisry/Tt29d885vfNPX19bHb6Rj5zne+Y0aMGGFSU1PN4MGDzaRJk9yBxhiOUyBdQw3Hy2XWrFlm6NChJjU11eTn55tZs2aZ999/3z2f4+TppZdeMl/4whdMWlqaGTlypHnyySc95sfz97vDGGNiU0YEAAAQObSpAQAAtkCoAQAAtkCoAQAAtkCoAQAAtkCoAQAAtkCoAQAAtkCoAQAAtkCoAQAAtkCoAQAAtkCoAQAAtkCoAQAAtkCoAQAAtvD/ATW7MQqTxrTlAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x,train_loss,c=\"blue\")\n",
    "plt.scatter(x,val_loss,c=\"orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f44fef1",
   "metadata": {},
   "source": [
    "I think setting eps to zero will still work, I do not see the possibility of a zero RV. Too tired today, will try tomorrow or something. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_3_11_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
