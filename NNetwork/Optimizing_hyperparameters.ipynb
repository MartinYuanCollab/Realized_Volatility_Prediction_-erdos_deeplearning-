{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06fe5209-9b7a-4bf1-a9e3-1d03e8d5bfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "import sys, importlib\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from proj_mod import training, data_processing, visualization\n",
    "importlib.reload(training);\n",
    "importlib.reload(data_processing);\n",
    "importlib.reload(visualization);\n",
    "\n",
    "device=(torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf1490c-2d0f-40b2-8533-024806d7777b",
   "metadata": {},
   "source": [
    "# Functions to optimize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539792c2-db63-4596-8515-c9f8ed060baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, device, eps=0):\n",
    "    model.train()\n",
    "    loss_fn = training.RMSPELoss(eps=eps) \n",
    "    total_data_count=len(dataloader.dataset)\n",
    "    sum_of_squares=0.0\n",
    "    for batch in dataloader:\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            sum_of_squares+=torch.sum(torch.square((pred-y)/(y+eps)))\n",
    "    with torch.no_grad():\n",
    "        rmspe=torch.sqrt(sum_of_squares/total_data_count)\n",
    "    return rmspe.item()\n",
    "\n",
    "def validate(model, dataloader, device, eps=0):\n",
    "    model.eval()\n",
    "    sum_of_squares = 0.0\n",
    "    total_data_count=len(dataloader.dataset)\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            sum_of_squares += torch.sum(torch.square((pred - y) / (y + eps)))\n",
    "        rmspe = torch.sqrt(sum_of_squares / total_data_count)\n",
    "    return rmspe.item()\n",
    "\n",
    "def objective(trial, define_model): \n",
    "\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "\n",
    "    model = define_model(trial).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler=optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,mode=\"min\",factor=0.5,patience=5,min_lr=1e-7)\n",
    "\n",
    "    eps = 1e-5\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "    max_epochs = 10\n",
    "\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device, eps)\n",
    "        val_loss = validate(model, test_loader, device, eps)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        trial.report(val_loss, epoch)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if val_loss < best_val_loss - 1e-4:  # added minimum delta threshold\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0          # reset counter on improvement\n",
    "        else:\n",
    "            patience_counter += 1         # increment if no improvement\n",
    "\n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                break  \n",
    "    \n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bdd271-35a6-478e-8868-dde35c850e12",
   "metadata": {},
   "source": [
    "# Usage examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e7f282-bf8e-425e-875a-8ca3dbc6b94c",
   "metadata": {},
   "source": [
    "## Needed by both models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323fa322-1387-49ad-870f-0b38cc6b91ee",
   "metadata": {},
   "source": [
    "### Data and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7c76a40-8952-418f-ac99-22e0aeb506c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In fold 0 :\n",
      "\n",
      "Train set end at 8117 .\n",
      "\n",
      "Test set start at 15516 end at 10890 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_time=np.load(\"../processed_data/recovered_time_id_order.npy\")\n",
    "df_RV_ts=pd.read_parquet(\"../processed_data/book_RV_ts_60_si.parquet\")\n",
    "\n",
    "df_target=pd.read_csv(\"../raw_data/kaggle_ORVP/train.csv\")\n",
    "df_target[\"row_id\"]=df_target[\"stock_id\"].astype(int).astype(str)+\"-\"+df_target[\"time_id\"].astype(int).astype(str)\n",
    "\n",
    "time_split_list=data_processing.time_cross_val_split(list_time=list_time,n_split=1,percent_val_size=10,list_output=True)\n",
    "train_time_id,test_time_id=time_split_list[0][0],time_split_list[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6d19a2-70f1-4e62-a3f9-5ecf25b41ccf",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55c2697f-5a42-4dc5-8787-67567bf6aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_emb_dim=32\n",
    "n_diff=2\n",
    "ts_dim=n_diff+1\n",
    "\n",
    "pos_embedder=training.pos_emb_cross_attn(length=60,ts_dim=ts_dim,emb_dim=ts_emb_dim,dropout=0.2,num_heads=4,keep_mag=True).to(device=device)\n",
    "\n",
    "ts_encoder_ff_layer=[\n",
    "    nn.Linear(in_features=ts_emb_dim,out_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=64,out_features=ts_emb_dim)\n",
    "]\n",
    "\n",
    "ts_decoder_ff_layer=[\n",
    "    nn.Linear(in_features=ts_emb_dim,out_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=64,out_features=ts_emb_dim)\n",
    "]\n",
    "\n",
    "output_ff=nn.Sequential(\n",
    "    nn.Linear(in_features=ts_emb_dim,out_features=1)\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e714224a-d919-4377-aa82-eea120e6b925",
   "metadata": {},
   "source": [
    "## Encoder-decoder model without id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c5072d-9dd3-46e7-82bb-2942055e38c5",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a26788f-db1a-4fbf-9501-679ffcc446ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=training.RVdataset(time_id_list=train_time_id,ts_features=[\"sub_int_RV\"],tab_features=[\"emb_id\"],df_ts_feat=df_RV_ts,df_target=df_target)\n",
    "test_dataset=training.RVdataset(time_id_list=test_time_id,ts_features=[\"sub_int_RV\"],tab_features=[\"emb_id\"],df_ts_feat=df_RV_ts,df_target=df_target)\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=512,shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=512,shuffle=True, num_workers =4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b262fac-e3ef-427a-86ec-8930434321d0",
   "metadata": {},
   "source": [
    "### Model definition and optimization of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b25956-adf3-4ae7-84cf-12af5f60b92b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def define_model_encdec(trial):\n",
    "    encoder_layers = trial.suggest_int(\"encoder_layer_num\", 2, 6)\n",
    "    decoder_layers = trial.suggest_int(\"decoder_layer_num\", 2, 6)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5, step=0.1)   \n",
    "\n",
    "    return training.encoder_decoder_teacherforcing(\n",
    "        pos_emb_model=pos_embedder,\n",
    "        output_feedforward=output_ff,\n",
    "        encoder_dropout=dropout,\n",
    "        decoder_dropout=dropout,\n",
    "        encoder_feedforward_list=ts_encoder_ff_layer,\n",
    "        decoder_feedforward_list=ts_decoder_ff_layer,\n",
    "        n_diff=n_diff,\n",
    "        encoder_layer_num=encoder_layers,\n",
    "        decoder_layer_num=decoder_layers,\n",
    "        input_scaler=10000,\n",
    "        ts_emb_dim=ts_emb_dim,\n",
    "        encoder_num_heads=4,\n",
    "        decoder_num_heads=4,\n",
    "        encoder_keep_mag=True,\n",
    "        decoder_keep_mag=True,\n",
    "        return_sum=True\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(lambda trial: objective(trial, define_model_encdec), n_trials=20)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30058ad-0851-4dfa-8c95-190c0219998c",
   "metadata": {},
   "source": [
    "I interrupted the execution, but these are the results for the first trials:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7723a6d-9242-4f6f-9616-47a71ea454ce",
   "metadata": {},
   "source": [
    "[I 2025-08-05 10:54:39,349] A new study created in memory with name: no-name-2029d593-a46b-4a1e-8988-a32d747af686\n",
    "[I 2025-08-05 10:57:38,321] Trial 0 finished with value: 0.23577575385570526 and parameters: {'lr': 0.00868445237867199, 'encoder_layer_num': 3, 'decoder_layer_num': 4, 'dropout': 0.1}. Best is trial 0 with value: 0.23577575385570526.\n",
    "[I 2025-08-05 11:01:05,833] Trial 1 finished with value: 0.22967329621315002 and parameters: {'lr': 0.0006277516484037366, 'encoder_layer_num': 2, 'decoder_layer_num': 6, 'dropout': 0.0}. Best is trial 1 with value: 0.22967329621315002.\n",
    "[I 2025-08-05 11:04:44,128] Trial 2 finished with value: 0.22987382113933563 and parameters: {'lr': 0.0009455489769112315, 'encoder_layer_num': 2, 'decoder_layer_num': 6, 'dropout': 0.30000000000000004}. Best is trial 1 with value: 0.22967329621315002.\n",
    "[I 2025-08-05 11:11:30,216] Trial 3 finished with value: 0.22987306118011475 and parameters: {'lr': 0.00015086803239011521, 'encoder_layer_num': 6, 'decoder_layer_num': 5, 'dropout': 0.0}. Best is trial 1 with value: 0.22967329621315002.\n",
    "[I 2025-08-05 11:15:14,044] Trial 4 finished with value: 0.22976943850517273 and parameters: {'lr': 0.00010708282792185345, 'encoder_layer_num': 4, 'decoder_layer_num': 3, 'dropout': 0.1}. Best is trial 1 with value: 0.22967329621315002.\n",
    "[W 2025-08-05 11:15:53,732] Trial 5 failed with parameters: {'lr': 9.694640858927792e-05, 'encoder_layer_num': 3, 'decoder_layer_num': 3, 'dropout': 0.0} because of the following error: KeyboardInterrupt()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb7b10-8567-4bd0-8d86-919d267e4a8f",
   "metadata": {},
   "source": [
    "## Encoder-decoder model with id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1503cc-5b51-40e1-923b-c3b8438d1062",
   "metadata": {},
   "source": [
    "### Data and dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdc674a4-90e7-447c-94a1-c70c940232a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RV_tab=pd.read_csv(\"../processed_data/RV_by_row_id.csv\")\n",
    "RV_tab[\"stock_id\"]=RV_tab[\"row_id\"].apply(lambda x: x.split(\"-\")[0])\n",
    "RV_tab[\"time_id\"]=RV_tab[\"row_id\"].apply(lambda x: x.split(\"-\")[1])\n",
    "\n",
    "# Creates tabular data, most specifically 'emb_id'\n",
    "unique_ids = sorted(RV_tab['stock_id'].unique())\n",
    "id_to_emb = {stock_id: i for i, stock_id in enumerate(unique_ids)}\n",
    "RV_tab['emb_id'] = RV_tab['stock_id'].map(id_to_emb)\n",
    "\n",
    "train_dataset=training.RVdataset(time_id_list=train_time_id,ts_features=[\"sub_int_RV\"],tab_features=[\"emb_id\"],df_ts_feat=df_RV_ts,df_tab_feat=RV_tab,df_target=df_target)\n",
    "test_dataset=training.RVdataset(time_id_list=test_time_id,ts_features=[\"sub_int_RV\"],tab_features=[\"emb_id\"],df_ts_feat=df_RV_ts,df_tab_feat=RV_tab,df_target=df_target)\n",
    "\n",
    "ts_place, id_place=train_dataset.featureplace[\"sub_int_RV\"], train_dataset.featureplace[\"emb_id\"]\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=512,shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=512,shuffle=True, num_workers =4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c2d058-add2-444a-aeab-7ceb2e27265d",
   "metadata": {},
   "source": [
    "### Model definition and optimization of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede89be4-7de1-46ef-b60a-9ecdfd0b44af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def define_model_encdec_id(trial):\n",
    "    encoder_layers = trial.suggest_int(\"encoder_layer_num\", 2, 6)\n",
    "    decoder_layers = trial.suggest_int(\"decoder_layer_num\", 2, 6)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5, step=0.1)   \n",
    "\n",
    "    base_model =training.encoder_decoder_teacherforcing(\n",
    "        pos_emb_model=pos_embedder,\n",
    "        output_feedforward=output_ff,\n",
    "        encoder_dropout=dropout,\n",
    "        decoder_dropout=dropout,\n",
    "        encoder_feedforward_list=ts_encoder_ff_layer,\n",
    "        decoder_feedforward_list=ts_decoder_ff_layer,\n",
    "        n_diff=n_diff,\n",
    "        encoder_layer_num=encoder_layers,\n",
    "        decoder_layer_num=decoder_layers, \n",
    "        input_scaler=10000,\n",
    "        ts_emb_dim=ts_emb_dim,\n",
    "        encoder_num_heads=4,\n",
    "        decoder_num_heads=4,\n",
    "        encoder_keep_mag=True,\n",
    "        decoder_keep_mag=True,\n",
    "        return_sum=True\n",
    "    )\n",
    "\n",
    "    id_emb_dim=8\n",
    "    id_hidden_dict=OrderedDict([(\"linear1\", nn.Linear(in_features=id_emb_dim, out_features=32),),\n",
    "                                (\"tanh1\", nn.Tanh()),\n",
    "                                (\"linear2\", nn.Linear(in_features=32, out_features=16)),\n",
    "                                (\"tanh2\", nn.Tanh()),\n",
    "                                (\"linear3\", nn.Linear(in_features=16, out_features=8)),\n",
    "                                (\"tanh3\", nn.Tanh()),\n",
    "                                (\"linear4\", nn.Linear(in_features=8,out_features=1))])\n",
    "    id_hidden_layers=nn.Sequential(id_hidden_dict).to(device=device)\n",
    "\n",
    "    return training.id_learned_embedding_adj_rnn_mtpl(ts_place=ts_place,\n",
    "                                             id_place=id_place, \n",
    "                                             rnn_model=base_model,\n",
    "                                             id_hidden_model=id_hidden_layers,\n",
    "                                             id_input_num=112,\n",
    "                                             emb_dim=id_emb_dim)\n",
    "\n",
    "study2 = optuna.create_study(direction=\"minimize\")\n",
    "study2.optimize(lambda trial: objective(trial, define_model_encdec_id), n_trials=20)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
